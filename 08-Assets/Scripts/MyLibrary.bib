@incollection{Atkeson2018,
  title = {Achieving Reliable Humanoid Robot Operations in the {{DARPA}} Robotics Challenge: {{Team WPI-CMU}}’s Approach},
  booktitle = {Springer {{Tracts}} in {{Advanced Robotics}}},
  author = {Atkeson, C.G. and Benzun, P.W.B. and Banerjee, N. and Berenson, D. and Bove, C.P. and Cui, X. and DeDonato, M. and Du, R. and Feng, S. and Franklin, P. and Gennert, M.A. and Graff, J.P. and He, P. and Jaeger, A. and Kim, J. and Knoedler, K. and Li, L. and Liu, C. and Long, X. and Polido, F. and Xinjilefu, X. and Padır, T.},
  date = {2018},
  volume = {121},
  pages = {271--307},
  doi = {10.1007/978-3-319-74666-1_8},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045395491&doi=10.1007%2f978-3-319-74666-1_8&partnerID=40&md5=9782e19078a4926bda1972fe0423cc96},
  abstract = {The DARPA Robotics Challenge (DRC) required participating human-robot teams to integrate mobility, manipulation, perception and operator interfaces to complete a simulated disaster mission. We describe our approach to the development of manipulation and locomotion capabilities for the humanoid robot atlas unplugged developed by Boston Dynamics. We focus on our approach, results and lessons learned from the DRC Finals to demonstrate our strategy including extensive operator practice, explicit monitoring for robot errors, adding additional sensing, and enabling the operator to control and monitor the robot at varying degrees of abstraction. Our safety-first strategy worked: we avoided falling and remote operators could safely recover from difficult situations. We were the only team in the DRC Finals that attempted all tasks, scored points (14/16), did not require physical human intervention (a reset), and did not fall in the two missions during the two days of tests. We also had the most consistent pair of runs. We ranked 3rd out of 23 teams when the scores from two official runs were averaged. © Springer International Publishing AG, part of Springer Nature 2018.},
  annotation = {rate: 3}
}

@inproceedings{Bajracharya2013,
  title = {High Fidelity Day/Night Stereo Mapping with Vegetation and Negative Obstacle Detection for Vision-in-the-Loop Walking},
  author = {Bajracharya, M. and Ma, J. and Malchano, M. and Perkins, A. and Rizzi, A.A. and Matthies, L.},
  date = {2013},
  pages = {3663--3670},
  doi = {10.1109/IROS.2013.6696879},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893724588&doi=10.1109%2fIROS.2013.6696879&partnerID=40&md5=7d7c06d39ff25836e25e9644032ef8b9},
  abstract = {This paper describes the stereo vision near-field terrain mapping system used by the Legged Squad Support System (LS3) quadruped vehicle to automatically adjust its gait in complex natural terrain. The mapping system achieves high robustness with a combination of stereo model-based outlier rejection and spatial and temporal filtering, enabled by a unique hybrid 2D/3D data structure. Classification of sparse structures allows the vehicle to traverse through vegetation. Inference of negative obstacles allows the vehicle to avoid steep drop-offs. A custom designed near-infrared illumination system enables operation at night. The mapping system has been tested extensively with controlled experiments and 72km of field testing in a wide variety of terrains and conditions. © 2013 IEEE.},
  eventtitle = {{{IEEE International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  annotation = {rate: 2}
}

@article{Barrau2015,
  title = {An {{EKF-SLAM}} Algorithm with Consistency Properties},
  author = {Barrau, Axel and Bonnabel, Silvere},
  date = {2015},
  abstract = {In this paper we address the inconsistency of the EKF-based SLAM algorithm that stems from non-observability of the origin and orientation of the global reference frame. We prove on the non-linear two-dimensional problem with point landmarks observed that this type of inconsistency is remedied using the Invariant EKF, a recently introduced variant of the EKF meant to account for the symmetries of the state space. Extensive Monte-Carlo runs illustrate the theoretical results.},
  langid = {english},
  annotation = {rate: 2},
  file = {C:\Users\15469\Zotero\storage\6J32KYNQ\Barrau 和 Bonnabel - An EKF-SLAM algorithm with consistency properties.pdf}
}

@article{Barrau2015a,
  title = {Non-Linear State Error Based Extended {{Kalman}} Filters with Applications to Navigation},
  author = {Barrau, Axel},
  date = {2015},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\2XA28VMY\Barrau - Non-linear state error based extended Kalman filte.pdf}
}

@inproceedings{Barrau2020,
  title = {A {{Mathematical Framework}} for {{IMU Error Propagation}} with {{Applications}} to {{Preintegration}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Barrau, Axel and Bonnabel, Silvere},
  date = {2020-05},
  pages = {5732--5738},
  publisher = {{IEEE}},
  location = {{Paris, France}},
  doi = {10.1109/ICRA40945.2020.9197492},
  url = {https://ieeexplore.ieee.org/document/9197492/},
  urldate = {2023-03-18},
  abstract = {To fuse information from inertial measurement units (IMU) with other sensors one needs an accurate model for IMU error propagation in terms of position, velocity and orientation, a triplet we call extended pose. In this paper we leverage a nontrivial result, namely log-linearity of inertial navigation equations based on the recently introduced Lie group SE2(3), to transpose the recent methodology of Barfoot and Furgale for associating uncertainty with poses (position, orientation) of SE(3) when using noisy wheel speeds, to the case of extended poses (position, velocity, orientation) of SE2(3) when using noisy IMUs. Besides, our approach to extended poses combined with log-linearity property allows revisiting the theory of preintegration on manifolds and reaching a further theoretic level in this field. We show exact preintegration formulas that account for rotating Earth, that is, centrifugal force and Coriolis effect, may be derived as a byproduct.},
  eventtitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72817-395-5},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\G9V6QL5G\Barrau 和 Bonnabel - 2020 - A Mathematical Framework for IMU Error Propagation.pdf}
}

@article{Barrau2023,
  title = {The {{Geometry}} of {{Navigation Problems}}},
  author = {Barrau, Axel and Bonnabel, Silvere},
  date = {2023-02},
  journaltitle = {IEEE Transactions on Automatic Control},
  shortjournal = {IEEE Trans. Automat. Contr.},
  volume = {68},
  number = {2},
  pages = {689--704},
  issn = {0018-9286, 1558-2523, 2334-3303},
  doi = {10.1109/TAC.2022.3144328},
  url = {https://ieeexplore.ieee.org/document/9689974/},
  urldate = {2023-03-15},
  abstract = {While many works exploiting an existing Lie group structure have been proposed for state estimation, in particular the invariant extended Kalman filter (IEKF), few papers address the construction of a group structure that allows casting a given system into the framework of invariant filtering. In this article, we introduce a large class of systems encompassing most problems involving a navigating vehicle encountered in practice. For those systems we introduce a novel methodology that systematically provides a group structure for the state space, including vectors of the body frame such as biases. We use it to derive observers having properties akin to those of linear observers or filters. The proposed unifying and versatile framework encompasses all systems, where IEKF has proved successful, improves state-of-the art “imperfect” IEKF for inertial navigation with sensor biases, and allows addressing novel examples, like GNSS antenna lever arm estimation.},
  langid = {english},
  annotation = {2 citations (Crossref) [2023-04-24]},
  file = {C:\Users\15469\Zotero\storage\CUPJQSTI\Barrau 和 Bonnabel - 2023 - The Geometry of Navigation Problems.pdf}
}

@article{barrauInvariantExtendedKalman2017,
  title = {The {{Invariant Extended Kalman Filter}} as a {{Stable Observer}}},
  author = {Barrau, Axel and Bonnabel, Silvere},
  date = {2017-04},
  journaltitle = {IEEE Transactions on Automatic Control},
  shortjournal = {IEEE Trans. Automat. Contr.},
  volume = {62},
  number = {4},
  pages = {1797--1812},
  issn = {0018-9286, 1558-2523},
  doi = {10.1109/TAC.2016.2594085},
  url = {http://ieeexplore.ieee.org/document/7523335/},
  urldate = {2023-02-02},
  abstract = {We analyze the convergence aspects of the invariant extended Kalman filter (IEKF), when the latter is used as a deterministic non-linear observer on Lie groups, for continuoustime systems with discrete observations. One of the main features of invariant observers for left-invariant systems on Lie groups is that the estimation error is autonomous. In this paper we first generalize this result by characterizing the (much broader) class of systems for which this property holds. For those systems, the Lie logarithm of the error turns out to obey a linear differential equation. Then, we leverage this “log-linear” property of the error evolution, to prove for those systems the local stability of the IEKF around any trajectory, under the standard conditions of the linear case. One mobile robotics example and one inertial navigation example illustrate the interest of the approach. Simulations evidence the fact that the EKF is capable of diverging in some challenging situations, where the IEKF with identical tuning keeps converging.},
  issue = {4},
  langid = {english},
  annotation = {rate: 2},
  file = {C:\Users\15469\Zotero\storage\HR5ZKMFG\Barrau 和 Bonnabel - 2017 - The Invariant Extended Kalman Filter as a Stable O.pdf}
}

@article{barrauInvariantKalmanFiltering2018,
  title = {Invariant {{Kalman Filtering}}},
  author = {Barrau, Axel and Bonnabel, Silvère},
  date = {2018-05-28},
  journaltitle = {Annual Review of Control, Robotics, and Autonomous Systems},
  shortjournal = {Annu. Rev. Control Robot. Auton. Syst.},
  volume = {1},
  number = {1},
  pages = {237--257},
  issn = {2573-5144, 2573-5144},
  doi = {10.1146/annurev-control-060117-105010},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-control-060117-105010},
  urldate = {2023-02-02},
  abstract = {The Kalman filter—or, more precisely, the extended Kalman filter (EKF)—is a fundamental engineering tool that is pervasively used in control and robotics and for various estimation tasks in autonomous systems. The recently developed field of invariant extended Kalman filtering uses the geometric structure of the state space and the dynamics to improve the EKF, notably in terms of mathematical guarantees. The methodology essentially applies in the fields of localization, navigation, and simultaneous localization and mapping (SLAM). Although it was created only recently, its remarkable robustness properties have already motivated a real industrial implementation in the aerospace field. This review aims to provide an accessible introduction to the methodology of invariant Kalman filtering and to allow readers to gain insight into the relevance of the method as well as its important differences with the conventional EKF. This should be of interest to readers intrigued by the practical application of mathematical theories and those interested in finding robust, simple-to-implement filters for localization, navigation, and SLAM, notably for autonomous vehicle guidance.},
  issue = {1},
  langid = {english},
  annotation = {rate: 2},
  file = {C:\Users\15469\Zotero\storage\L4F92GA4\Barrau 和 Bonnabel - 2018 - Invariant Kalman Filtering.pdf}
}

@inproceedings{Bazeille2013,
  title = {Vision Enhanced Reactive Locomotion Control for Trotting on Rough Terrain},
  author = {Bazeille, S. and Barasuol, V. and Focchi, M. and Havoutis, I. and Frigerio, M. and Buchli, J. and Semini, C. and Caldwell, D.G.},
  date = {2013},
  doi = {10.1109/TePRA.2013.6556366},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883324343&doi=10.1109%2fTePRA.2013.6556366&partnerID=40&md5=bb923d640d9a720f2c59c35352ea0314},
  abstract = {Legged robots have the potential to navigate in more challenging terrain than wheeled robots do. Unfortunately, their control is more difficult because they have to deal with the traditional mapping and path planning problems, as well as foothold computation, leg trajectories and posture control in order to achieve successful navigation. Many parameters need to be adjusted in real time to keep the robot stable and safe while it is moving. In this paper, we will present a new framework for a quadruped robot, which performs goal-oriented navigation on unknown rough terrain by using inertial measurement data and stereo vision. This framework includes perception and control, and allows the robot to navigate in a straight line forward to a visual goal in a difficult environment. The developed rough terrain locomotion system does not need any mapping or path planning: the stereo camera is used to visually guide the robot and evaluate the terrain roughness and an inertial measurement unit (IMU) is used for posture control. This new framework is an important step forward to achieve fully autonomous navigation because in the case of problems in the SLAM mapping, a reactive locomotion controller is always active. This ensures stable locomotion in rough terrain, by combining direct visual feedback and inertial measurements. By implementing this controller, an autonomous navigation system has been developed, which is goal-oriented and overcomes disturbances from the ground, the robot weight, or external forces. Indoor and outdoor experiments with our quadruped robot show the effectiveness and the robustness of this framework. © 2013 IEEE.},
  eventtitle = {{{IEEE Conference}} on {{Technologies}} for {{Practical Robot Applications}}, {{TePRA}}},
  annotation = {rate: 3}
}

@online{Bazzana2023,
  title = {How-to {{Augmented Lagrangian}} on {{Factor Graphs}}},
  author = {Bazzana, Barbara and Andreasson, Henrik and Grisetti, Giorgio},
  date = {2023-08-10},
  eprint = {2308.05444},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2308.05444},
  urldate = {2023-08-17},
  abstract = {Factor graphs are a very powerful graphical representation, used to model many problems in robotics. They are widely spread in the areas of Simultaneous Localization and Mapping (SLAM), computer vision, and localization. In this paper we describe an approach to fill the gap with other areas, such as optimal control, by presenting an extension of Factor Graph Solvers to constrained optimization. The core idea of our method is to encapsulate the Augmented Lagrangian (AL) method in factors of the graph that can be integrated straightforwardly in existing factor graph solvers.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\N6LV78WF\Bazzana 等 - 2023 - How-to Augmented Lagrangian on Factor Graphs.pdf}
}

@inproceedings{Bednarek2019,
  title = {What Am i Touching? {{Learning}} to Classify Terrain via Haptic Sensing},
  author = {Bednarek, J. and Bednarek, M. and Wellhausen, L. and Hutter, M. and Walas, K.},
  date = {2019},
  volume = {2019-May},
  pages = {7187--7193},
  doi = {10.1109/ICRA.2019.8794478},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067115398&doi=10.1109%2fICRA.2019.8794478&partnerID=40&md5=34b7eb8a5de41ab078c168cf2b3b0863},
  abstract = {Mobile robots are becoming very popular in real-world outdoors applications, where there are many challenges in robot control and perception. One of the most critical problems is to characterise the terrain traversed by the robot. This knowledge is indispensable for optimal terrain negotiation. Currently, most approaches are performing terrain classification from vision, but there is not enough research on terrain identification from a direct interaction of the robot with the environment. In our work, we proposed new methods for classification of force/torque data from an interaction of the legged robot foot with the ground, gathered during the walking process. We provided machine learning methods for terrain classification from raw force/torque signals for which we achieved 93\% accuracy on a challenging dataset with 160 minutes of recorded fixed-length steps. We also worked on a dataset where the assumption of a fixed-length step is not valid. In this case, the final result is around 80\% of accuracy. The most important fact is that the data in both cases was recorded while the robot was walking, no particular movements or controlled environment were needed. Additionally, we also proposed a clustering method which allows us to learn about the class membership based on the recorded data only, without any human supervision. © 2019 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 2}
}

@article{Belter2016,
  title = {{{RGB-D}} Terrain Perception and Dense Mapping for Legged Robots},
  author = {Belter, D. and Łabecki, P. and Fankhauser, P. and Siegwart, R.},
  date = {2016},
  journaltitle = {International Journal of Applied Mathematics and Computer Science},
  volume = {26},
  number = {1},
  pages = {81--97},
  doi = {10.1515/amcs-2016-0006},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964345042&doi=10.1515%2famcs-2016-0006&partnerID=40&md5=31223563006f86594078fea77f7817b2},
  abstract = {This paper addresses the issues of unstructured terrain modeling for the purpose of navigation with legged robots. We present an improved elevation grid concept adopted to the specific requirements of a small legged robot with limited perceptual capabilities. We propose an extension of the elevation grid update mechanism by incorporating a formal treatment of the spatial uncertainty. Moreover, this paper presents uncertainty models for a structured light RGB-D sensor and a stereo vision camera used to produce a dense depth map. The model for the uncertainty of the stereo vision camera is based on uncertainty propagation from calibration, through undistortion and rectification algorithms, allowing calculation of the uncertainty of measured 3D point coordinates. The proposed uncertainty models were used for the construction of a terrain elevation map using the Videre Design STOC stereo vision camera and Kinect-like range sensors. We provide experimental verification of the proposed mapping method, and a comparison with another recently published terrain mapping method for walking robots. © 2016 Dominik Belter et al., published by De Gruyter Open 2016.},
  annotation = {rate: 3}
}

@article{Benallegue2022,
  title = {Velocity-Aided {{IMU-based Tilt}} and {{Attitude Estimation}}},
  author = {Benallegue, Mehdi and Benallegue, Abdelaziz and Cisneros, Rafael and Chitour, Yacine},
  date = {2022},
  journaltitle = {IEEE Transactions on Automatic Control},
  shortjournal = {IEEE Trans. Automat. Contr.},
  pages = {1--14},
  issn = {0018-9286, 1558-2523, 2334-3303},
  doi = {10.1109/TAC.2022.3225758},
  url = {https://ieeexplore.ieee.org/document/9966736/},
  urldate = {2022-12-06},
  abstract = {This paper addresses the problem of estimating the tilt and more generally the attitude of a rigid body that is subject to high accelerations and equipped with inertial measurement units (IMU) and a sensor providing the body velocity (expressed in the reference frame attached to the body). In the absence of a magnetometer, tilt estimation is proposed through a two-step observer: a global-exponentially stable pre-estimation given to a manifold-constrained complementary filter. In the presence of a magnetometer, the presented observer allows to reconstruct the full attitude and tune how much the estimation of the tilt is influenced by the magnetometer, depending on the level of confidence given to the measurements of the magnetometer. All state estimators are proposed with proofs of almost global asymptotic stability and local exponential convergence. Finally, these estimators are compared with state-of-the-art solutions in clean and noisy simulations, allowing recommended solutions to be drawn for each case.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\5QTR5DHC\Benallegue 等 - 2022 - Velocity-aided IMU-based Tilt and Attitude Estimat.pdf}
}

@inproceedings{Bjelonic2023,
  title = {A {{Survey}} of~{{Wheeled-Legged Robots}}},
  author = {Bjelonic, M. and Klemm, V. and Lee, J. and Hutter, M.},
  date = {2023},
  volume = {530 LNNS},
  pages = {83--94},
  doi = {10.1007/978-3-031-15226-9_11},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138002653&doi=10.1007%2f978-3-031-15226-9_11&partnerID=40&md5=dccd7d3429a5c7bd619411d4a84178ce},
  abstract = {The community in legged robotics focuses on bio-inspired robots, although there are some human inventions that nature could not recreate. One of the most significant examples is the wheel which has made our transportation system more efficient and faster. Inspired by this human-made evolution, we present a survey of wheeled-legged robots, allowing robotic systems to be efficient on flat as well as versatile on challenging terrain. This enhancement, however, comes at the cost of increased complexity due to additional degrees of freedom at the end-effector, which empowers motions along the rolling direction. The missing examples in nature make designing templates that capture the underlying locomotion principles cumbersome, making hybrid locomotion challenging. This paper reviews some of the novel locomotion frameworks overcoming these challenges. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
  eventtitle = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
  annotation = {rate: 1}
}

@inproceedings{Bloechliger2016,
  title = {Foot-Eye Calibration of Legged Robot Kinematics},
  author = {Bloechliger, F. and Bloesch, M. and Fankhauser, P. and Hutter, M. and Siegwart, R.},
  date = {2016},
  pages = {420--427},
  doi = {10.1142/9789813149137_0050},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999816143&doi=10.1142%2f9789813149137_0050&partnerID=40&md5=060cb9ccdc95f10f87f12c1a62da77c9},
  abstract = {Legged robots rely on an accurate calibration of the system's kinematics for reliable motion tracking of dynamic gaits and for precise foot placement when moving in rough terrain. In our automatic foot-eye calibration approach, a monocular camera is attached to the robot and observes the robot's moving feet, which are equipped with Augmented Reality (AR) markers. The measurements are used to formulate a non-linear least squares problem over a fixed time window in order to estimate the 33 unknown parameters. This is efficiently solved with the Levenberg-Marquardt algorithm and we get estimates for both the kinematic and the camera parameters. The approach is successfully evaluated on a real quadruped robot. © 2016, World Scientific Publishing Co. Pte Ltd. All rights reserved.},
  eventtitle = {Advances in {{Cooperative Robotics}}: {{Proceedings}} of the 19th {{International Conference}} on {{Climbing}} and {{Walking Robots}} and the {{Support Technologies}} for {{Mobile Machines}}, {{CLAWAR}} 2016},
  annotation = {rate: 2}
}

@inproceedings{Bloesch2013,
  title = {State Estimation for Legged Robots on Unstable and Slippery Terrain},
  author = {Bloesch, M. and Gehring, C. and Fankhauser, P. and Hutter, M. and Hoepflinger, M.A. and Siegwart, R.},
  date = {2013},
  pages = {6058--6064},
  doi = {10.1109/IROS.2013.6697236},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893752051&doi=10.1109%2fIROS.2013.6697236&partnerID=40&md5=3b1b291cf2057cc322ce55eb8befe924},
  abstract = {This paper presents a state estimation approach for legged robots based on stochastic filtering. The key idea is to extract information from the kinematic constraints given through the intermittent contacts with the ground and to fuse this information with inertial measurements. To this end, we design an unscented Kalman filter based on a consistent formulation of the underlying stochastic model. To increase the robustness of the filter, an outliers rejection methodology is included into the update step. Furthermore, we present the nonlinear observability analysis of the system, where, by considering the special nature of 3D rotations, we obtain a relatively simple form of the corresponding observability matrix. This yields, that, except for the global position and the yaw angle, all states are in general observable. This also holds if only one foot is in contact with the ground. The presented filter is evaluated on a real quadruped robot trotting over an uneven and slippery terrain. © 2013 IEEE.},
  eventtitle = {{{IEEE International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  annotation = {rate: 3},
  file = {C:\Users\15469\Zotero\storage\M4KFT3HZ\Bloesch 等 - 2013 - State estimation for legged robots on unstable and.pdf}
}

@inproceedings{Bloesch2013a,
  title = {Kinematic Batch Calibration for Legged Robots},
  author = {Bloesch, M. and Hutter, M. and Gehring, C. and Hoepflinger, M.A. and Siegwart, R.},
  date = {2013},
  pages = {2542--2547},
  doi = {10.1109/ICRA.2013.6630924},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887292432&doi=10.1109%2fICRA.2013.6630924&partnerID=40&md5=ee6929955c9ffbce1e7576eebafba288},
  abstract = {This paper introduces a novel batch optimization based calibration framework for legged robots. Given a non-degenerate calibration dataset and considering the stochastic models of the sensors, the task is formulated as a maximum likelihood problem. In order to facilitate the derivation of consistent measurement equations, the trajectory of the robot and other auxiliary variables are included into the optimization problem. This formulation can be transformed into a nonlinear least squares problem which can be readily solved. Applied to our legged robot StarIETH, the framework estimates kinematic parameters (segment lengths, body dimensions, angular offsets), accelerometer and gyroscope biases, as well as full inter-sensor calibrations. The generic structure easily allows the inclusion of additional sensor modalities. Based on datasets obtained on the real robot the consistency and performance of the presented approach are successfully evaluated. © 2013 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 4}
}

@article{Bloesch2016,
  title = {A {{Primer}} on the {{Differential Calculus}} of {{3D Orientations}}},
  author = {Bloesch, Michael and Sommer, Hannes and Laidlow, Tristan and Burri, Michael and Nuetzi, Gabriel and Fankhauser, Peter and Bellicoso, Dario and Gehring, Christian and Leutenegger, Stefan and Hutter, Marco and Siegwart, Roland},
  date = {2016},
  abstract = {The proper handling of 3D orientations is a central element in many optimization problems in engineering. Unfortunately many researchers and engineers struggle with the formulation of such problems and often fall back to suboptimal solutions. The existence of many different conventions further complicates this issue, especially when interfacing multiple differing implementations. This document discusses an alternative approach which makes use of a more abstract notion of 3D orientations. The relative orientation between two coordinate systems is primarily identified by the coordinate mapping it induces. This is combined with the standard exponential map in order to introduce representation-independent and minimal differentials, which are very convenient in optimization based methods.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\A7LQ3L4Z\Bloesch 等 - A Primer on the Differential Calculus of 3D Orient.pdf}
}

@thesis{Bloesch2017,
  title = {State {{Estimation}} for {{Legged Robots}} - {{Kinematics}}, {{Inertial Sensing}}, and {{Computer Vision}}},
  author = {Bloesch, Michael},
  editora = {{Davison, Andrew} and {Siegwart, Roland}},
  editoratype = {collaborator},
  date = {2017},
  pages = {195 p.},
  institution = {{ETH Zurich}},
  doi = {10.3929/ETHZ-A-010875968},
  url = {http://hdl.handle.net/20.500.11850/129873},
  urldate = {2023-01-13},
  langid = {english},
  annotation = {rate: 4},
  file = {C:\Users\15469\Zotero\storage\IXLS7W3Q\Bloesch - 2017 - State Estimation for Legged Robots - Kinematics, I.pdf}
}

@incollection{Bloesch2018,
  title = {Technical {{Implementations}} of the {{Sense}} of {{Balance}}},
  booktitle = {Humanoid {{Robotics}}: {{A Reference}}},
  author = {Bloesch, Michael and Hutter, Marco},
  editor = {Goswami, Ambarish and Vadakkepat, Prahlad},
  date = {2018},
  pages = {1--29},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-007-7194-9_69-2},
  url = {http://link.springer.com/10.1007/978-94-007-7194-9_69-2},
  urldate = {2023-01-16},
  isbn = {978-94-007-7194-9},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\ZEQG85UR\Bloesch 和 Hutter - 2018 - Technical Implementations of the Sense of Balance.pdf}
}

@inproceedings{Bonnabel2007,
  title = {Left-Invariant Extended {{Kalman}} Filter and Attitude Estimation},
  author = {Bonnabel, Silvère},
  date = {2007},
  annotation = {rate: 2},
  file = {C:\Users\15469\Zotero\storage\WJD8T8TI\Left-invariant_extended_Kalman_filter_and_attitude_estimation.pdf}
}

@article{Brandao2020,
  title = {Placing and Scheduling Many Depth Sensors for Wide Coverage and Efficient Mapping in Versatile Legged Robots},
  author = {Brandão, M. and Figueiredo, R. and Takagi, K. and Bernardino, A. and Hashimoto, K. and Takanishi, A.},
  date = {2020},
  journaltitle = {International Journal of Robotics Research},
  volume = {39},
  number = {4},
  pages = {431--460},
  doi = {10.1177/0278364919891776},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077361456&doi=10.1177%2f0278364919891776&partnerID=40&md5=9b8cfe65dcf25a88d301160d3e4a6f43},
  abstract = {This article tackles the problem of designing 3D perception systems for robots with high visual requirements, such as versatile legged robots capable of different locomotion styles. In order to guarantee high visual coverage in varied conditions (e.g., biped walking, quadruped walking, ladder climbing), such robots need to be equipped with a large number of sensors, while at the same time managing the computational requirements that arise from such a system. We tackle this problem at both levels: sensor placement (how many sensors to install on the robot and where) and run-time acquisition scheduling under computational constraints (not all sensors can be acquired and processed at the same time). Our first contribution is a methodology for designing perception systems with a large number of depth sensors scattered throughout the links of a robot, using multi-objective optimization for optimal trade-offs between visual coverage and the number of sensors. We estimate the Pareto front of these objectives through evolutionary optimization, and implement a solution on a real legged robot. Our formulation includes constraints on task-specific coverage and design symmetry, which lead to reliable coverage and fast convergence of the optimization problem. Our second contribution is an algorithm for lowering the computational burden of mapping with such a high number of sensors, formulated as an information-maximization problem with several sampling techniques for speed. Our final system uses 20 depth sensors scattered throughout the robot, which can either be acquired simultaneously or optimally scheduled for low CPU usage while maximizing mapping quality. We show that, when compared with state-of-the-art robotic platforms, our system has higher coverage across a higher number of tasks, thus being suitable for challenging environments and versatile robots. We also demonstrate that our scheduling algorithm allows higher mapping performance to be obtained than with naïve and state-of-the-art methods by leveraging on measures of information gain and self-occlusion at low computational costs. © The Author(s) 2019.},
  annotation = {rate: 3}
}

@article{Brossard2020,
  title = {{{AI-IMU Dead-Reckoning}}},
  author = {Brossard, Martin and Barrau, Axel and Bonnabel, Silvere},
  date = {2020-12},
  journaltitle = {IEEE Transactions on Intelligent Vehicles},
  shortjournal = {IEEE Trans. Intell. Veh.},
  volume = {5},
  number = {4},
  pages = {585--595},
  issn = {2379-8904, 2379-8858},
  doi = {10.1109/TIV.2020.2980758},
  url = {https://ieeexplore.ieee.org/document/9035481/},
  urldate = {2023-02-16},
  abstract = {In this paper, we propose a novel accurate method for dead-reckoning of wheeled vehicles based only on an Inertial Measurement Unit (IMU). In the context of intelligent vehicles, robust and accurate dead-reckoning based on the IMU may prove useful to correlate feeds from imaging sensors, to safely navigate through obstructions, or for safe emergency stops in the extreme case of exteroceptive sensors failure. The key components of the method are the Kalman filter and the use of deep neural networks to dynamically adapt the noise parameters of the filter. The method is tested on the KITTI odometry dataset, and our dead-reckoning inertial method based only on the IMU accurately estimates 3D position, velocity, orientation of the vehicle and self-calibrates the IMU biases. We achieve on average a 1.10\% translational error and the algorithm competes with top-ranked methods which, by contrast, use LiDAR or stereo vision.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\7HQNB5VC\Brossard 等 - 2020 - AI-IMU Dead-Reckoning.pdf}
}

@online{Buchanan2021,
  title = {Learning {{Inertial Odometry}} for {{Dynamic Legged Robot State Estimation}}},
  author = {Buchanan, Russell and Camurri, Marco and Dellaert, Frank and Fallon, Maurice},
  date = {2021-11-01},
  eprint = {2111.00789},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.00789},
  urldate = {2023-05-01},
  abstract = {This paper introduces a novel proprioceptive state estimator for legged robots based on a learned displacement measurement from IMU data. Recent research in pedestrian tracking has shown that motion can be inferred from inertial data using convolutional neural networks. A learned inertial displacement measurement can improve state estimation in challenging scenarios where leg odometry is unreliable, such as slipping and compressible terrains. Our work learns to estimate a displacement measurement from IMU data which is then fused with traditional leg odometry. Our approach greatly reduces the drift of proprioceptive state estimation, which is critical for legged robots deployed in vision and lidar denied environments such as foggy sewers or dusty mines. We compared results from an EKF and an incremental fixed-lag factor graph estimator using data from several real robot experiments crossing challenging terrains. Our results show a reduction of relative pose error by 37 \% in challenging scenarios when compared to a traditional kinematic-inertial estimator without learned measurement. We also demonstrate a 22 \% reduction in error when used with vision systems in visually degraded environments such as an underground mine.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\6X82U2KH\Buchanan 等 - 2021 - Learning Inertial Odometry for Dynamic Legged Robo.pdf}
}

@article{Buchanan2023,
  title = {Deep {{IMU Bias Inference}} for {{Robust Visual-Inertial Odometry With Factor Graphs}}},
  author = {Buchanan, R. and Agrawal, V. and Camurri, M. and Dellaert, F. and Fallon, M.},
  date = {2023},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {8},
  number = {1},
  pages = {41--48},
  doi = {10.1109/LRA.2022.3222956},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142795452&doi=10.1109%2fLRA.2022.3222956&partnerID=40&md5=ee93f35700a4fc1eb00c99916a42fc0a},
  abstract = {Visual Inertial Odometry (VIO) is one of the most established state estimation methods for mobile platforms. However, when visual tracking fails, VIO algorithms quickly diverge due to rapid error accumulation during inertial data integration. This error is typically modeled as a combination of additive Gaussian noise and a slowly changing bias which evolves as a random walk. In this work, we propose to train a neural network to learn the true bias evolution. We implement and compare two common sequential deep learning architectures: LSTMs and Transformers. Our approach follows from recent learning-based inertial estimators, but, instead of learning a motion model, we target IMU bias explicitly, which allows us to generalize to locomotion patterns unseen in training. We show that our proposed method improves state estimation in visually challenging situations across a wide range of motions by quadrupedal robots, walking humans, and drones. Our experiments show an average 15\% reduction in drift rate, with much larger reductions when there is total vision failure. Importantly, we also demonstrate that models trained with one locomotion pattern (human walking) can be applied to another (quadruped robot trotting) without retraining. © 2016 IEEE.},
  annotation = {rate: 3}
}

@article{Camurri2017,
  title = {Probabilistic {{Contact Estimation}} and {{Impact Detection}} for {{State Estimation}} of {{Quadruped Robots}}},
  author = {Camurri, Marco and Fallon, Maurice and Bazeille, Stephane and Radulescu, Andreea and Barasuol, Victor and Caldwell, Darwin G. and Semini, Claudio},
  date = {2017-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {2},
  number = {2},
  pages = {1023--1030},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2017.2652491},
  url = {http://ieeexplore.ieee.org/document/7815333/},
  urldate = {2023-07-17},
  abstract = {Reliable state estimation is crucial for stable planning and control of legged locomotion. A fundamental component of a state estimator in legged platforms is Leg Odometry, which only requires information about kinematics and contacts. Many legged robots use dedicated sensors on each foot to detect ground contacts. However, this choice is impractical for many agile legged robots in field operations, as these sensors often degrade and break. Instead, this paper focuses on the development of a robust Leg Odometry module, which does not require contact sensors. The module estimates the probability of reliable contact and detects foot impacts using internal force sensing. This knowledge is then used to improve the kinematics-inertial state estimate of the robot’s base. We show how our approach can reach comparable performance to systems with foot sensors. Extensive experimental results lasting over 1 h are presented on our 85 kg quadrupedal robot HyQ carrying out a variety of gaits.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\YHWE4JT4\Camurri 等 - 2017 - Probabilistic Contact Estimation and Impact Detect.pdf}
}

@article{Camurri2020,
  title = {Pronto: {{A Multi-Sensor State Estimator}} for {{Legged Robots}} in {{Real-World Scenarios}}},
  author = {Camurri, M. and Ramezani, M. and Nobili, S. and Fallon, M.},
  date = {2020},
  journaltitle = {Frontiers in Robotics and AI},
  volume = {7},
  doi = {10.3389/frobt.2020.00068},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086788166&doi=10.3389%2ffrobt.2020.00068&partnerID=40&md5=9227e59cb9a4e71f1c07420a622a758c},
  abstract = {In this paper, we present a modular and flexible state estimation framework for legged robots operating in real-world scenarios, where environmental conditions, such as occlusions, low light, rough terrain, and dynamic obstacles can severely impair estimation performance. At the core of the proposed estimation system, called Pronto, is an Extended Kalman Filter (EKF) that fuses IMU and Leg Odometry sensing for pose and velocity estimation. We also show how Pronto can integrate pose corrections from visual and LIDAR and odometry to correct pose drift in a loosely coupled manner. This allows it to have a real-time proprioceptive estimation thread running at high frequency (250–1,000 Hz) for use in the control loop while taking advantage of occasional (and often delayed) low frequency (1–15 Hz) updates from exteroceptive sources, such as cameras and LIDARs. To demonstrate the robustness and versatility of the approach, we have tested it on a variety of legged platforms, including two humanoid robots (the Boston Dynamics Atlas and NASA Valkyrie) and two dynamic quadruped robots (IIT HyQ and ANYbotics ANYmal) for more than 2 h of total runtime and 1.37 km of distance traveled. The tests were conducted in a number of different field scenarios under the conditions described above. The algorithms presented in this paper are made available to the research community as open-source ROS packages. © Copyright © 2020 Camurri, Ramezani, Nobili and Fallon.},
  annotation = {rate: 5},
  file = {C:\Users\15469\Zotero\storage\7LP4VJNN\Camurri 等 - 2020 - Pronto A Multi-Sensor State Estimator for Legged .pdf}
}

@article{Chen2017,
  title = {Bioinspired {{Control}} of {{Walking}} with {{Toe-Off}}, {{Heel-Strike}}, and {{Disturbance Rejection}} for a {{Biped Robot}}},
  author = {Chen, X. and Yu, Z. and Zhang, W. and Zheng, Y. and Huang, Q. and Ming, A.},
  date = {2017},
  journaltitle = {IEEE Transactions on Industrial Electronics},
  volume = {64},
  number = {10},
  pages = {7962--7971},
  doi = {10.1109/TIE.2017.2698361},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029939182&doi=10.1109%2fTIE.2017.2698361&partnerID=40&md5=a15adeaaa3674465e3eb3068bc0ec20b},
  abstract = {Human-like features, like toe-off, heel-strike, and disturbance rejection, can enhance the performance of bipedal robots. However, the required control strategies for these motions influence each other, and few studies have considered them simultaneously. Humans can walk stably with toe-off and heel-strike even after experiencing disturbances. Thus, we can study human control strategies, and then, apply them to a bipedal robot. This paper proposes a bioinspired control method to realize stable walking with toe-off and heel-strike for a bipedal robot even after disturbances. First, we analyze human walking and obtain some control strategies. Then, we propose a pattern generator and a walking controller to mimic these strategies. The pattern generator can predefine the zero-moment-point to plan the center of mass trajectory and determine appropriate foot placement. The controller adjusts torso acceleration to make the support leg compliant with the external disturbances. The controller also achieves toe-off and heel-strike in cooperation with the pattern generator. Finally, the validity of the proposed method is confirmed through simulations and experiments. © 1982-2012 IEEE.},
  annotation = {rate: 3}
}

@article{Chen2023,
  title = {Monocular {{Visual-Inertial Odometry}} with {{Planar Regularities}}},
  author = {Chen, Chuchu and Geneva, Patrick and Peng, Yuxiang and Lee, Woosik and Huang, Guoquan},
  date = {2023},
  abstract = {State-of-the-art monocular visual-inertial odometry (VIO) approaches rely on sparse point features in part due to their efficiency, robustness, and prevalence, while ignoring high-level structural regularities such as planes that are common to man-made environments and can be exploited to further constrain motion. Generally, planes can be observed by a camera for significant periods of time due to their large spatial presence and thus, are amenable for long-term navigation. Therefore, in this paper, we design a novel realtime monocular VIO system that is fully regularized by planar features within a lightweight multi-state constraint Kalman filter (MSCKF). At the core of our method is an efficient robust monocular-based plane detection algorithm, which does not require additional sensing modalities such as a stereo or depth camera as commonly seen in the literature, while enabling realtime regularization of point features to environmental planes. Specifically, in the proposed MSCKF, long-lived planes are maintained in the state vector, while shorter ones are marginalized after use for efficiency. Planar regularities are applied to both in-state SLAM features and out-of-state MSCKF features, thus fully exploiting the environmental plane information to improve VIO performance. The proposed approach is evaluated with extensive Monte-Carlo simulations and different realworld experiments including an author-collected AR scenario, and shown to outperform the point-based VIO in structured environments.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\M8EG8FYA\Chen 等 - Monocular Visual-Inertial Odometry with Planar Reg.pdf}
}

@inproceedings{Chen2023a,
  title = {Direct {{LiDAR-Inertial Odometry}}: {{Lightweight LIO}} with {{Continuous-Time Motion Correction}}},
  shorttitle = {Direct {{LiDAR-Inertial Odometry}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Chen, Kenny and Nemiroff, Ryan and Lopez, Brett T.},
  date = {2023-05-29},
  pages = {3983--3989},
  publisher = {{IEEE}},
  location = {{London, United Kingdom}},
  doi = {10.1109/ICRA48891.2023.10160508},
  url = {https://ieeexplore.ieee.org/document/10160508/},
  urldate = {2023-08-01},
  abstract = {Aggressive motions from agile flights or traversing irregular terrain induce motion distortion in LiDAR scans that can degrade state estimation and mapping. Some methods exist to mitigate this effect, but they are still too simplistic or computationally costly for resource-constrained mobile robots. To this end, this paper presents Direct LiDAR-Inertial Odometry (DLIO), a lightweight LiDAR-inertial odometry algorithm with a new coarse-to-fine approach in constructing continuoustime trajectories for precise motion correction. The key to our method lies in the construction of a set of analytical equations which are parameterized solely by time, enabling fast and parallelizable point-wise deskewing. This method is feasible only because of the strong convergence properties in our nonlinear geometric observer, which provides provably correct state estimates for initializing the sensitive IMU integration step. Moreover, by simultaneously performing motion correction and prior generation, and by directly registering each scan to the map and bypassing scan-to-scan, DLIO’s condensed architecture is nearly 20\% more computationally efficient than the current state-of-the-art with a 12\% increase in accuracy. We demonstrate DLIO’s superior localization accuracy, map quality, and lower computational overhead as compared to four state-of-the-art algorithms through extensive tests using multiple public benchmark and self-collected datasets.},
  eventtitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {9798350323658},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\NGFMM4PR\Chen 等 - 2023 - Direct LiDAR-Inertial Odometry Lightweight LIO wi.pdf}
}

@online{Chen2023b,
  title = {Deep {{Learning}} for {{Visual Localization}} and {{Mapping}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Visual Localization}} and {{Mapping}}},
  author = {Chen, Changhao and Wang, Bing and Lu, Chris Xiaoxuan and Trigoni, Niki and Markham, Andrew},
  date = {2023-08-27},
  eprint = {2308.14039},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2308.14039},
  urldate = {2023-09-15},
  abstract = {Deep learning based localization and mapping approaches have recently emerged as a new research direction and receive significant attentions from both industry and academia. Instead of creating hand-designed algorithms based on physical models or geometric theories, deep learning solutions provide an alternative to solve the problem in a data-driven way. Benefiting from the ever-increasing volumes of data and computational power on devices, these learning methods are fast evolving into a new area that shows potentials to track self-motion and estimate environmental model accurately and robustly for mobile agents. In this work, we provide a comprehensive survey, and propose a taxonomy for the localization and mapping methods using deep learning. This survey aims to discuss two basic questions: whether deep learning is promising to localization and mapping; how deep learning should be applied to solve this problem. To this end, a series of localization and mapping topics are investigated, from the learning based visual odometry, global relocalization, to mapping, and simultaneous localization and mapping (SLAM). It is our hope that this survey organically weaves together the recent works in this vein from robotics, computer vision and machine learning communities, and serves as a guideline for future researchers to apply deep learning to tackle the problem of visual localization and mapping.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\YAX5NVQN\Chen 等 - 2023 - Deep Learning for Visual Localization and Mapping.pdf}
}

@inproceedings{Chilian2011,
  title = {Multisensor Data Fusion for Robust Pose Estimation of a Six-Legged Walking Robot},
  booktitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Chilian, A. and Hirschmuller, H. and Gorner, M.},
  date = {2011-09},
  pages = {2497--2504},
  publisher = {{IEEE}},
  location = {{San Francisco, CA}},
  doi = {10.1109/IROS.2011.6094484},
  url = {http://ieeexplore.ieee.org/document/6094484/},
  urldate = {2022-12-11},
  abstract = {For autonomous navigation tasks it is important that the robot always has a good estimate of its current pose with respect to its starting position and – in terms of orientation – with respect to the gravity vector. For this, the robot should make use of all available information and be robust against the failure of single sensors. In this paper a multisensor data fusion algorithm for the six-legged walking robot DLR Crawler is presented. The algorithm is based on an indirect feedback information filter that fuses measurements from an inertial measurement unit (IMU) with relative 3D leg odometry measurements and relative 3D visual odometry measurements from a stereo camera. Errors of the visual odometry are computed and considered in the filtering process in order to achieve accurate pose estimates which are robust against visual odometry failure. The algorithm was successfully tested and results are presented.},
  eventtitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2011)},
  isbn = {978-1-61284-456-5 978-1-61284-454-1 978-1-61284-455-8},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\8ABYZQZZ\Chilian 等 - 2011 - Multisensor data fusion for robust pose estimation.pdf}
}

@inproceedings{Cofield2019,
  title = {A {{Humanoid Robot Object Perception Approach Using Depth Images}}},
  author = {Cofield, A. and El-Shair, Z. and Rawashdeh, S.A.},
  date = {2019},
  volume = {2019-July},
  pages = {437--442},
  doi = {10.1109/NAECON46414.2019.9057808},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083318885&doi=10.1109%2fNAECON46414.2019.9057808&partnerID=40&md5=ac4da0b88508f1e0b655f272b626647e},
  abstract = {Humanoid robots have had significant research interest in the past two decades. Their classification as mobile manipulators allows them to work in unstructured environments creating new possibilities for human-robot interaction. Object grasping and manipulation are essential and enabling capabilities for mobile humanoid robots that require reliable perception. This paper presents a perception approach using depth images from an RGB-D camera to estimate the work plane and estimate object positions relative to the robot. Results from experiments with a set of object shapes and scenarios are presented. © 2019 IEEE.},
  eventtitle = {Proceedings of the {{IEEE National Aerospace Electronics Conference}}, {{NAECON}}},
  annotation = {rate: 2}
}

@article{Dang2020,
  title = {Graph-Based Subterranean Exploration Path Planning Using Aerial and Legged Robots},
  author = {Dang, T. and Tranzatto, M. and Khattak, S. and Mascarich, F. and Alexis, K. and Hutter, M.},
  date = {2020},
  journaltitle = {Journal of Field Robotics},
  volume = {37},
  number = {8},
  pages = {1363--1388},
  doi = {10.1002/rob.21993},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094637912&doi=10.1002%2frob.21993&partnerID=40&md5=076ac134a247a5a00d0fa6952cd7758a},
  abstract = {Autonomous exploration of subterranean environments remains a major challenge for robotic systems. In response, this paper contributes a novel graph-based subterranean exploration path planning method that is attuned to key topological properties of subterranean settings, such as large-scale tunnel-like networks and complex multibranched topologies. Designed both for aerial and legged robots, the proposed method is structured around a bifurcated local- and global-planner architecture. The local planner utilizes a rapidly exploring random graph to reliably and efficiently identify paths that optimize an exploration gain within a local subspace, while simultaneously avoiding obstacles, respecting applicable traversability constraints and honoring dynamic limitations of the robots. Reflecting the fact that multibranched and tunnel-like networks of underground environments can often lead to dead-ends and accounting for the robot endurance, the global planning layer works in conjunction with the local planner to incrementally build a sparse global graph and is engaged when the system must be repositioned to a previously identified frontier of the exploration space, or commanded to return-to-home. The designed planner is detailed with respect to its computational complexity and compared against state-of-the-art approaches. Emphasizing field experimentation, the method is evaluated within multiple real-life deployments using aerial robots and the ANYmal legged system inside both long-wall and room-and-pillar underground mines in the United States~and in Switzerland, as well as inside an underground bunker. The presented results further include missions conducted within the Defense Advanced Research Projects Agency (DARPA) Subterranean Challenge, a relevant competition on underground exploration. © 2020 Wiley Periodicals LLC},
  annotation = {rate: 2}
}

@article{DeDonato2017,
  title = {Team {{WPI-CMU}}: {{Achieving Reliable Humanoid Behavior}} in the {{DARPA Robotics Challenge}}},
  author = {DeDonato, M. and Polido, F. and Knoedler, K. and Babu, B.P.W. and Banerjee, N. and Bove, C.P. and Cui, X. and Du, R. and Franklin, P. and Graff, J.P. and He, P. and Jaeger, A. and Li, L. and Berenson, D. and Gennert, M.A. and Feng, S. and Liu, C. and Xinjilefu, X. and Kim, J. and Atkeson, C.G. and Long, X. and Padır, T.},
  date = {2017},
  journaltitle = {Journal of Field Robotics},
  volume = {34},
  number = {2},
  pages = {381--399},
  doi = {10.1002/rob.21685},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013096189&doi=10.1002%2frob.21685&partnerID=40&md5=04cc73e0242664f63c80d26c8d744b84},
  abstract = {In the DARPA Robotics Challenge (DRC), participating human-robot teams were required to integrate mobility, manipulation, perception, and operator interfaces to complete a simulated disaster mission. We describe our approach using the humanoid robot Atlas Unplugged developed by Boston Dynamics. We focus on our approach, results, and lessons learned from the DRC Finals to demonstrate our strategy, including extensive operator practice, explicit monitoring for robot errors, adding additional sensing, and enabling the operator to control and monitor the robot at varying degrees of abstraction. Our safety-first strategy worked: we avoided falling, and remote operators could safely recover from difficult situations. We were the only team in the DRC Finals that attempted all tasks, scored points (14/16), did not require physical human intervention (a reset), and did not fall in the two missions during the two days of tests. We also had the most consistent pair of runs. © 2017 Wiley Periodicals, Inc.},
  annotation = {rate: 3}
}

@inproceedings{Dey2022,
  title = {{{PrePARE}}: {{Predictive Proprioception}} for {{Agile Failure Event Detection}} in {{Robotic Exploration}} of {{Extreme Terrains}}},
  author = {Dey, S. and Fan, D. and Schmid, R. and Dixit, A. and Otsu, K. and Touma, T. and Schilling, A.F. and Agha-Mohammadi, A.-A.},
  date = {2022},
  volume = {2022-October},
  pages = {4338--4343},
  doi = {10.1109/IROS47612.2022.9981660},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146309494&doi=10.1109%2fIROS47612.2022.9981660&partnerID=40&md5=fe22794fb568cf06da1468731fe40ccf},
  abstract = {Legged robots can traverse a wide variety of terrains, some of which may be challenging for wheeled robots, such as stairs or highly uneven surfaces. However, quadruped robots face stability challenges on slippery surfaces. This can be resolved by adjusting the robot's locomotion by switching to more conservative and stable locomotion modes, such as crawl mode (where three feet are in contact with the ground always) or amble mode (where one foot touches down at a time) to prevent potential falls. To tackle these challenges, we propose an approach to learn a model from past robot experience for predictive detection of potential failures. Accordingly, we trigger gait switching merely based on proprioceptive sensory information. To learn this predictive model, we propose a semi-supervised process for detecting and annotating ground truth slip events in two stages: We first detect abnormal occurrences in the time series sequences of the gait data using an unsupervised anomaly detector, and then, the anomalies are verified with expert human knowledge in a replay simulation to assert the event of a slip. These annotated slip events are then used as ground truth examples to train an ensemble decision learner for predicting slip probabilities across terrains for traversability. We analyze our model on data recorded by a legged robot on multiple sites with slippery terrain. We demonstrate that a potential slip event can be predicted up to 720 ms ahead of a potential fall with an average precision greater than 0.95 and an average F-score of 0.82. Finally, we validate our approach in real-time by deploying it on a legged robot and switching its gait mode based on slip event detection. © 2022 IEEE.},
  eventtitle = {{{IEEE International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  annotation = {rate: 2}
}

@article{Fahmi2021,
  title = {On {{State Estimation}} for {{Legged Locomotion}} over {{Soft Terrain}}},
  author = {Fahmi, S. and Fink, G. and Semini, C.},
  date = {2021},
  journaltitle = {IEEE Sensors Letters},
  volume = {5},
  number = {1},
  pages = {1--4},
  doi = {10.1109/LSENS.2021.3049954},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099544482&doi=10.1109%2fLSENS.2021.3049954&partnerID=40&md5=1dd2f8f80d087bedb11be7e290707469},
  abstract = {Locomotion over soft terrain remains a challenging problem for legged robots. Most of the work done on state estimation for legged robots is designed for rigid contacts and does not take into account the physical parameters of the terrain. That said, this letter answers the following questions: How and why does soft terrain affect state estimation for legged robots? To do so, we utilized a state estimator that fuses IMU measurements with leg odometry that is designed with rigid contact assumptions. We experimentally validated the state estimator with the HyQ robot trotting over both soft and rigid terrain. We demonstrate that soft terrain negatively affects state estimation for legged robots and that the state estimates have a noticeable drift over soft terrain compared to rigid terrain. © 2017 IEEE.},
  issue = {1},
  annotation = {10 citations (Crossref) [2023-04-24]},
  file = {C:\Users\15469\Zotero\storage\MQM7ZN3W\Fahmi 等 - 2021 - On State Estimation for Legged Locomotion over Sof.pdf}
}

@inproceedings{Fallon2014,
  title = {Drift-Free Humanoid State Estimation Fusing Kinematic, Inertial and {{LIDAR}} Sensing},
  booktitle = {2014 {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  author = {Fallon, Maurice F. and Antone, Matthew and Roy, Nicholas and Teller, Seth},
  date = {2014-11},
  pages = {112--119},
  publisher = {{IEEE}},
  location = {{Madrid, Spain}},
  doi = {10.1109/HUMANOIDS.2014.7041346},
  url = {http://ieeexplore.ieee.org/document/7041346/},
  urldate = {2023-04-25},
  abstract = {This paper describes an algorithm for the probabilistic fusion of sensor data from a variety of modalities (inertial, kinematic and LIDAR) to produce a single consistent position estimate for a walking humanoid. Of specific interest is our approach for continuous LIDAR-based localization which maintains reliable drift-free alignment to a prior map using a Gaussian Particle Filter. This module can be bootstrapped by constructing the map on-the-fly and performs robustly in a variety of challenging field situations. We also discuss a two-tier estimation hierarchy which preserves registration to this map and other objects in the robot’s vicinity while also contributing to direct low-level control of a Boston Dynamics Atlas robot. Extensive experimental demonstrations illustrate how the approach can enable the humanoid to walk over uneven terrain without stopping (for tens of minutes), which would otherwise not be possible. We characterize the performance of the estimator for each sensor modality and discuss the computational requirements.},
  eventtitle = {2014 {{IEEE-RAS}} 14th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}} 2014)},
  isbn = {978-1-4799-7174-9},
  langid = {english},
  annotation = {rate: 5},
  file = {C:\Users\15469\Zotero\storage\RNZFNVC8\Fallon 等 - 2014 - Drift-free humanoid state estimation fusing kinema.pdf}
}

@article{Fallon2018,
  title = {Accurate and Robust Localization for Walking Robots Fusing Kinematics, Inertial, Vision and {{LIDAR}}},
  author = {Fallon, M.},
  date = {2018},
  journaltitle = {Interface Focus},
  volume = {8},
  number = {4},
  doi = {10.1098/rsfs.2018.0015},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048733509&doi=10.1098%2frsfs.2018.0015&partnerID=40&md5=5fceca83fabafd783ce3184d999cc50d},
  abstract = {In this article, we review methods for localization and situational awareness of biped and quadruped robotics. This type of robot is modelled as a freefloating mechanical system subject to external forces and constrained by whole-body distributed rigid contacts. Measurements of the state of the robot can be made using a variety of sensor information-such as kinematics (the sensing of the joint angles of the robot), contact force (pressure sensors in the robot's feet), accelerometers and gyroscopes as well as external sensors such as vision and LIDAR. This high-frequency state estimate is then passed to the control system of the robot to allow it to traverse terrain or manipulate its environment. In this article, we describe the development of an estimator for the Boston Dynamics Atlas humanoid robot. It was later adapted to the HyQ2 quadruped, developed by the Istituto Italiano di Tecnologia. Some discussion is given as to future trends while also considering briefly the relationship with biological systems. © 2018 The Author(s) Published by the Royal Society. All rights reserved.},
  annotation = {rate: 3}
}

@thesis{Feng2010,
  title = {中县干部},
  author = {冯, 军旗},
  date = {2010},
  annotation = {titleTranslation: 中县干部},
  file = {C:\Users\15469\Zotero\storage\NH9LXS92\中县干部 冯军旗.pdf}
}

@inproceedings{Forster2015,
  title = {{{IMU Preintegration}} on {{Manifold}} for {{Efficient Visual-Inertial Maximum-a-Posteriori Estimation}}},
  booktitle = {Robotics: {{Science}} and {{Systems XI}}},
  author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
  date = {2015-07-13},
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2015.XI.006},
  url = {http://www.roboticsproceedings.org/rss11/p06.pdf},
  urldate = {2023-01-10},
  eventtitle = {Robotics: {{Science}} and {{Systems}} 2015},
  isbn = {978-0-9923747-1-6},
  annotation = {rate: 4},
  file = {C:\Users\15469\Zotero\storage\MVEL6K6U\Forster 等 - 2015 - IMU Preintegration on Manifold for Efficient Visua.pdf}
}

@article{Forster2017,
  title = {On-{{Manifold Preintegration}} for {{Real-Time Visual}}–{{Inertial Odometry}}},
  author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
  date = {2017},
  journaltitle = {IEEE TRANSACTIONS ON ROBOTICS},
  volume = {33},
  number = {1},
  abstract = {Current approaches for visual–inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time; this problem is further emphasized by the fact that inertial measurements come at high rate, hence, leading to the fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a preintegration theory that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the maximum a posteriori state estimator. Our theoretical development enables the computation of all necessary Jacobians for the optimization and a posteriori bias correction in analytic form. The second contribution is to show that the preintegrated inertial measurement unit model can be seamlessly integrated into a visual–inertial pipeline under the unifying framework of factor graphs. This enables the application of incremental-smoothing algorithms and the use of a structureless model for visual measurements, which avoids optimizing over the 3-D points, further accelerating the computation. We perform an extensive evaluation of our monocular VIO pipeline on real and simulated datasets. The results confirm that our modeling effort leads to an accurate state estimation in real time, outperforming state-of-the-art approaches.},
  langid = {english},
  annotation = {rate: 5},
  file = {C:\Users\15469\Zotero\storage\RV6CRVAN\On-Manifold_Preintegration_for_Real-Time_Visual--Inertial_Odometry.pdf}
}

@inproceedings{Fourmy2021,
  title = {Contact {{Forces Preintegration}} for {{Estimation}} in {{Legged Robotics}} Using {{Factor Graphs}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Fourmy, Mederic and Flayols, Thomas and Leziart, Pierre-Alexandre and Mansard, Nicolas and Sola, Joan},
  date = {2021-05-30},
  pages = {1372--1378},
  publisher = {{IEEE}},
  location = {{Xi'an, China}},
  doi = {10.1109/ICRA48506.2021.9561037},
  url = {https://ieeexplore.ieee.org/document/9561037/},
  urldate = {2023-07-11},
  abstract = {State estimation, in particular estimation of the base position, orientation and velocity, plays a big role in the efficiency of legged robot stabilization. The estimation of the base state is particularly important because of its strong correlation with the underactuated dynamics, i.e. the evolution of center of mass and angular momentum. Yet this estimation is typically done in two phases, first estimating the base state, then reconstructing the center of mass from the robot model. The underactuated dynamics is indeed not properly observed, and any bias in the model would not be corrected from the sensors. While it has already been observed that force measurements make such a bias observable, these are often only used for a binary estimation of the contact state. In this paper, we propose to simultaneously estimate the base and the underactuation state by exploiting all measurements simultaneously. To this end, we propose several contributions to implement a complete state estimator using factor graphs. Contact forces altering the underactuated dynamics are preintegrated using a novel adaptation of the IMU pre-integration method, which constitutes the principal contribution. IMU preintegration is also used to estimate the positional motion of the base. Encoder measurements then participate to the estimation in two ways: by providing leg odometry displacements which contributes to the observability of IMU biases; and by relating the positional and centroidal states, thus connecting the whole graph and producing a tightly-coupled whole-body estimator. The validity of the approach is demonstrated on real data captured by the Solo12 quadruped robot.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72819-077-8},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\RA49KIJB\Fourmy 等 - 2021 - Contact Forces Preintegration for Estimation in Le.pdf}
}

@inproceedings{Fu2022,
  title = {Coupling {{Vision}} and {{Proprioception}} for {{Navigation}} of {{Legged Robots}}},
  author = {Fu, Z. and Kumar, A. and Agarwal, A. and Qi, H. and Malik, J. and Pathak, D.},
  date = {2022},
  volume = {2022-June},
  pages = {1894--1904},
  doi = {10.1109/CVPRW56347.2022.00206},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135437418&doi=10.1109%2fCVPRW56347.2022.00206&partnerID=40&md5=10db9a6940787d9ccac40ce927920ce4},
  abstract = {We exploit the complementary strengths of vision and proprioception to develop a point-goal navigation system for legged robots, called VP-Nav. Legged systems are capable of traversing more complex terrain than wheeled robots, but to fully utilize this capability, we need a high-level path planner in the navigation system to be aware of the walking capabilities of the low-level locomotion policy in varying environments. We achieve this by using proprioceptive feedback to ensure the safety of the planned path by sensing unexpected obstacles like glass walls, terrain properties like slipperiness or softness of the ground and robot properties like extra payload that are likely missed by vision. The navigation system uses onboard cameras to generate an occupancy map and a corresponding cost map to reach the goal. A fast marching planner then generates a target path. A velocity command generator takes this as input to generate the desired velocity for the walking policy. A safety advisor module adds sensed unexpected obstacles to the occupancy map and environment-determined speed limits to the velocity command generator. We show superior performance compared to wheeled robot baselines, and ablation studies which have disjoint high-level planning and low-level control. We also show the real-world deployment of VP-Nav on a quadruped robot with onboard sensors and computation. Videos at https://navigation-locomotion.github.io © 2022 IEEE.},
  eventtitle = {{{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  annotation = {rate: 4}
}

@article{Gama2021,
  title = {Goal-Directed Tactile Exploration for Body Model Learning through Self-Touch on a Humanoid Robot},
  author = {Gama, F. and Shcherban, M. and Rolf, M. and Hoffmann, M.},
  date = {2021},
  journaltitle = {IEEE Transactions on Cognitive and Developmental Systems},
  doi = {10.1109/TCDS.2021.3104881},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113271684&doi=10.1109%2fTCDS.2021.3104881&partnerID=40&md5=823e95dc279e5adce178f0e2bd039b75},
  abstract = {An early integration of tactile sensing into motor coordination is the norm in animals, but still a challenge for robots. Tactile exploration through touches on the body gives rise to first body models and bootstraps further development such as reaching competence. Reaching to one\&\#x2019;s own body requires connections of the tactile and motor space only. Still, the problems of high dimensionality and motor redundancy persist. Through an embodied computational model for the learning of self-touch on a simulated humanoid robot with artificial sensitive skin, we demonstrate that this task can be achieved (i) effectively and (ii) efficiently at scale by employing the computational frameworks for the learning of internal models for reaching: intrinsic motivation and goal babbling. We relate our results to infant studies on spontaneous body exploration as well as reaching to vibrotactile targets on the body. We analyze the reaching configurations of one infant followed weekly between 4 and 18 months of age and derive further requirements for the computational model: accounting for (iii) continuous rather than sporadic touch and (iv) consistent redundancy resolution. Results show the general success of the learning models in the touch domain, but also point out limitations in achieving fully continuous touch. CCBY},
  annotation = {rate: 2}
}

@article{Gan2018,
  title = {On the Dynamic Similarity between Bipeds and Quadrupeds: {{A}} Case Study on Bounding},
  author = {Gan, Z. and Jiao, Z. and Remy, C.D.},
  date = {2018},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {4},
  pages = {3614--3621},
  doi = {10.1109/LRA.2018.2854923},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063304887&doi=10.1109%2fLRA.2018.2854923&partnerID=40&md5=ddb15a1c2d230a749c81a44a4c422264},
  abstract = {In this letter, we identify passive dynamic bounding gaits of quadrupedal robots and investigate the dynamic similarities between these gaits and those of a bipedal model. For this purpose, we develop a passive dynamic bounding model of a quadruped with an extended main body. This model is based on the established SLIP model, yet has dynamic swing leg motions instead of a predefined angle of attack. We present a gait-identification framework that is based on a numerical continuation approach and that allows us to find all possible passive periodic motions that emerge from pronking and bounding in place. These gaits include forward pronking as well as forward bounding with gathered suspension, extended suspension, and with two suspensions. By conducting a parameter study on the body inertia, we can draw parallels between bipedal and quadrupedal gaits, and can also show how the structure of possible solutions changes with the main body inertia. In particular, we see sudden changes in the gait structure at certain critical values of the main body inertia. This letter shows connections between different types of quadrupedal gaits and may serve as a guideline in the design and control of gaits for energetically economical legged robots. It also sheds some light at the question why animals with different morphology use different gaits at the same Froude numbers. © 2016 IEEE.},
  annotation = {rate: 2},
  file = {C:\Users\15469\Zotero\storage\DZPL8XAJ\Gan 等 - 2018 - On the dynamic similarity between bipeds and quadr.pdf}
}

@inproceedings{Gao2021,
  title = {{{SVM-Based State Estimation}} of {{Biped Robot}}},
  booktitle = {2021 7th {{International Conference}} on {{Mechatronics}} and {{Robotics Engineering}} ({{ICMRE}})},
  author = {Gao, Chengzhi and Xie, Ye and Kong, Lingyu and Chen, XingYu and Xie, Anhuan and Zhang, Dan},
  date = {2021-02-03},
  pages = {41--47},
  publisher = {{IEEE}},
  location = {{Budapest, Hungary}},
  doi = {10.1109/ICMRE51691.2021.9384850},
  url = {https://ieeexplore.ieee.org/document/9384850/},
  urldate = {2023-01-25},
  abstract = {Currently, the research of biped robots is becoming more and more popular, the kinematics-based state estimation, as an important precondition for its dynamic walking, has also become the focus of many researchers. Owing to the hybrid dynamics of biped robot, ground contact of each foot has to be previously evaluated to complete state estimation when using kinematics method. The contribution of this paper is to introduce the soft SVM to detect the contact phase. This SVM-based method free biped from the use of force sensors, which are highly vulnerable under the impact between feet and contact surfaces. Moreover, accurate dynamic modelling is not requisited. These two advantages indicate the strong robustness of SVM -based contact detection .Based on this contact detection method, the paper studies the state fusion method by applying extended Kalman filter to combine kinematics estimation with IMU data. Finally, the SVM-based contact detection algorithm and the complete state fusion method are both verified on our biped robot, with several experiments. The accuracy of the SVM -based method is validated by the comparison with force sensor based method. In addition, in contrast to torque-based method, its accuracy does not highly rely on the selection of algorithm parameters, like torque threshold.},
  eventtitle = {2021 7th {{International Conference}} on {{Mechatronics}} and {{Robotics Engineering}} ({{ICMRE}})},
  isbn = {978-1-66541-489-0},
  langid = {english},
  annotation = {rate: 2},
  file = {C:\Users\15469\Zotero\storage\L3U3AAPE\Gao 等 - 2021 - SVM-Based State Estimation of Biped Robot.pdf}
}

@article{Gao2022,
  title = {Invariant {{Filtering}} for {{Legged Humanoid Locomotion}} on a {{Dynamic Rigid Surface}}},
  author = {Gao, Yuan and Yuan, Chengzhi and Gu, Yan},
  date = {2022-08},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  shortjournal = {IEEE/ASME Trans. Mechatron.},
  volume = {27},
  number = {4},
  pages = {1900--1909},
  issn = {1083-4435, 1941-014X},
  doi = {10.1109/TMECH.2022.3176015},
  url = {https://ieeexplore.ieee.org/document/9847283/},
  urldate = {2022-12-11},
  abstract = {State estimation for legged locomotion over a dynamic rigid surface (DRS), which is a rigid surface moving in the world frame (e.g., ships, aircraft, and trains), remained an underexplored problem. This article introduces an invariant extended Kalman filter that estimates the robot’s pose and velocity during DRS locomotion by using common sensors of legged robots [e.g., inertial measurement units (IMUs), joint encoders, and RDB-D camera]. A key feature of the filter lies in that it explicitly addresses the nonstationary surface–foot contact point and the hybrid robot behaviors. Another key feature is that in the absence of IMU biases, the filter satisfies the attractive group affine and invariant observation conditions, and is thus provably convergent for the deterministic continuous phases. The observability analysis is performed to reveal the effects of DRS movement on the state observability, and the convergence property of the hybrid deterministic filter system is examined for the observable state variables. Experiments of a Digit humanoid robot walking on a pitching treadmill validate the effectiveness of the proposed filter under large estimation errors and moderate DRS movement. The video of the experiments can be found at: https://youtu.be/ ScQIBFUSKzo.},
  issue = {4},
  langid = {english},
  annotation = {0 citations (Crossref) [2023-04-24] rate: 2},
  file = {C:\Users\15469\Zotero\storage\DJG45PH4\Gao 等 - 2022 - Invariant Filtering for Legged Humanoid Locomotion.pdf}
}

@inproceedings{George2015,
  title = {Visual and Inertial Odometry for a Disaster Recovery Humanoid},
  author = {George, M. and Tardif, J.-P. and Kelly, A.},
  date = {2015},
  volume = {105},
  pages = {501--514},
  doi = {10.1007/978-3-319-07488-7_34},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958524521&doi=10.1007%2f978-3-319-07488-7_34&partnerID=40&md5=252c8d1c1b16726203d6883b5a60b76e},
  abstract = {Disaster recovery robots must operate in unstructured environments where wheeled or tracked motion may not be feasible or where it may be subject to extreme slip. Many industrial disaster scenarios also preclude reliance on GNSS or other external signals as robots are deployed indoors or underground. Two of the candidates for precise positioning in these scenarios are visual odometry and inertial navigation. This paper presents some practical experience in the design and analysis of a combined visual and inertial odometry system for the Carnegie Mellon University Highly Intelligent Mobile Platform (CHIMP); a humanoid robot competing in the darpa Robotics Challenge. © Springer International Publishing Switzerland 2015.},
  eventtitle = {Springer {{Tracts}} in {{Advanced Robotics}}},
  annotation = {rate: 3}
}

@article{Goffin,
  title = {Invariant {{Kalman Filtering}} with {{Noise-Free Pseudo-Measurements}}},
  author = {Goffin, Sven and Bonnabel, Silvere and Bruls, Olivier and Sacre, Pierre},
  abstract = {In this paper, we focus on developing an Invariant Extended Kalman Filter (IEKF) for extended pose estimation for a noisy system with state equality constraints. We treat those constraints as noise-free pseudo-measurements. To this aim, we provide a formula for the Kalman gain in the limit of noisefree measurements and rank-deficient covariance matrix. We relate the constraints to group-theoretic properties and study the behavior of the IEKF in the presence of such noise-free measurements. We illustrate this perspective on the estimation of the motion of the load of a overhead crane, when a wireless inertial measurement unit is mounted on the hook.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\UXDBKDHA\Goffin 等 - Invariant Kalman Filtering with Noise-Free Pseudo-.pdf}
}

@inproceedings{Gong2019,
  title = {Feedback {{Control}} of a {{Cassie Bipedal Robot}}: {{Walking}}, {{Standing}}, and {{Riding}} a {{Segway}}},
  shorttitle = {Feedback {{Control}} of a {{Cassie Bipedal Robot}}},
  booktitle = {2019 {{American Control Conference}} ({{ACC}})},
  author = {Gong, Yukai and Hartley, Ross and Da, Xingye and Hereid, Ayonga and Harib, Omar and Huang, Jiunn-Kai and Grizzle, Jessy},
  date = {2019-07},
  pages = {4559--4566},
  publisher = {{IEEE}},
  location = {{Philadelphia, PA, USA}},
  doi = {10.23919/ACC.2019.8814833},
  url = {https://ieeexplore.ieee.org/document/8814833/},
  urldate = {2022-12-11},
  abstract = {The Cassie bipedal robot designed by Agility Robotics is providing academics with a common platform for sharing and comparing algorithms for locomotion, perception, and navigation. This paper focuses on feedback control for standing and walking using the methods of virtual constraints and gait libraries. The designed controller was implemented six weeks after the robot arrived at the University of Michigan and allowed it to stand in place as well as walk over sidewalks, grass, snow, sand, and burning brush. The controller for standing also enables the robot to ride a Segway. Software supporting the work in this paper is available on GitHub.},
  eventtitle = {2019 {{American Control Conference}} ({{ACC}})},
  isbn = {978-1-5386-7926-5},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\Q4HZVIWI\Gong 等 - 2019 - Feedback Control of a Cassie Bipedal Robot Walkin.pdf}
}

@inproceedings{Gong2021,
  title = {One-{{Step Ahead Prediction}} of {{Angular Momentum}} about the {{Contact Point}} for {{Control}} of {{Bipedal Locomotion}}: {{Validation}} in a {{LIP-inspired Controller}}},
  shorttitle = {One-{{Step Ahead Prediction}} of {{Angular Momentum}} about the {{Contact Point}} for {{Control}} of {{Bipedal Locomotion}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Gong, Yukai and Grizzle, Jessy},
  date = {2021-05-30},
  pages = {2832--2838},
  publisher = {{IEEE}},
  location = {{Xi'an, China}},
  doi = {10.1109/ICRA48506.2021.9560821},
  url = {https://ieeexplore.ieee.org/document/9560821/},
  urldate = {2023-02-18},
  abstract = {Ultimately, feedback control is about making adjustments using current state information in order to meet an objective in the future. In the control of bipedal locomotion, linear velocity of the center of mass has been widely accepted as the primary variable around which feedback control objectives are formulated. In this paper, we argue that it is easier to predict the one-step ahead evolution of angular momentum about the contact point than it is to make a similar prediction for linear velocity, and hence it provides a superior quantity for feedback control. So as not to confuse the benefits of predicting angular momentum with any other control design decisions, we reformulate the standard LIP model in terms of angular momentum and show how to regulate swing foot touchdown position at the end of the current step so as to meet an angular momentum objective at the end of the next step. We implement the resulting feedback controller on the 20 degreeof-freedom bipedal robot, Cassie Blue, where each leg accounts for nearly one-third of the robot’s total mass of 32 Kg. Under this controller, the robot achieves fast walking, rapid turning while walking, large disturbance rejection, and locomotion on rough terrain.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72819-077-8},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\95U2R644\Gong 和 Grizzle - 2021 - One-Step Ahead Prediction of Angular Momentum abou.pdf}
}

@article{Guadarrama-Olvera2022,
  title = {Preemptive {{Foot Compliance}} to {{Lower Impact During Biped Robot Walking Over Unknown Terrain}}},
  author = {Guadarrama-Olvera, J. Rogelio and Kajita, Shuuji and Cheng, Gordon},
  date = {2022-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {7},
  number = {3},
  pages = {8006--8011},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2022.3187253},
  url = {https://ieeexplore.ieee.org/document/9810342/},
  urldate = {2022-10-21},
  abstract = {In this work, we present a novel method for ankle/foot compliance for biped humanoid robots walking over uneven terrain. Based on distributed plantar proximity sensing, we developed the Preemptive Foot Compliance (PFC) control that generates a Preemptive Ground Reaction Wrench that modifies the foot orientation to maximize the largest contact area to reduce the impact force produced at landing. PFC can be easily included in any walking controller for flat ground and become capable of walking on uneven terrains. The PFC was tested on two full-size humanoid robots running two different walking controllers, originally designed only for flat-ground walking. Both robots increase their capability to walk over uneven terrain and the walking impacts in flat grown were reduced by approximately 80\%.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\YLSMM52L\Guadarrama-Olvera 等。 - 2022 - Preemptive Foot Compliance to Lower Impact During .pdf}
}

@inproceedings{Haldane2014,
  title = {Detection of Slippery Terrain with a Heterogeneous Team of Legged Robots},
  author = {Haldane, D.W. and Fankhauser, P. and Siegwart, R. and Fearing, R.S.},
  date = {2014},
  pages = {4576--4581},
  doi = {10.1109/ICRA.2014.6907527},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929223204&doi=10.1109%2fICRA.2014.6907527&partnerID=40&md5=8a552fda003eff7d94cb67bbc65383d7},
  abstract = {Legged robots come in a range of sizes and capabilities. By combining these robots into heterogeneous teams, joint locomotion and perception tasks can be achieved by utilizing the diversified features of each robot. In this work we present a framework for using a heterogeneous team of legged robots to detect slippery terrain. StarlETH, a large and highly capable quadruped uses the VelociRoACH as a novel remote probe to detect regions of slippery terrain. StarlETH localizes the team using internal state estimation. To classify slippage of the VelociRoACH, we develop several Support Vector Machines (SVM) based on data from both StarlETH and VelociRoACH. By combining the team's information about the motion of VelociRoACH, a classifier was built which could detect slippery spots with 92\% (125/135) accuracy using only four features. © 2014 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 2}
}

@article{Han2022,
  title = {Combining {{Lidar}} and {{Camera}} via {{Attention Mechanism}} for {{Simultaneous Localization}} and {{Mapping}} in {{Outdoor Environments}}},
  author = {Han, Lam Kha},
  date = {2022},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\BGCNR3SD\Han - Combining Lidar and Camera via Attention Mechanism.pdf}
}

@online{Han2023,
  title = {{{DAMS-LIO}}: {{A Degeneration-Aware}} and {{Modular Sensor-Fusion LiDAR-inertial Odometry}}},
  shorttitle = {{{DAMS-LIO}}},
  author = {Han, Fuzhang and Zheng, Han and Huang, Wenjun and Xiong, Rong and Wang, Yue and Jiao, Yanmei},
  date = {2023-03-23},
  eprint = {2302.01703},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.01703},
  urldate = {2023-05-24},
  abstract = {With robots being deployed in increasingly complex environments like underground mines and planetary surfaces, the multi-sensor fusion method has gained more and more attention which is a promising solution to state estimation in the such scene. The fusion scheme is a central component of these methods. In this paper, a light-weight iEKF-based LiDAR-inertial odometry system is presented, which utilizes a degeneration-aware and modular sensor-fusion pipeline that takes both LiDAR points and relative pose from another odometry as the measurement in the update process only when degeneration is detected. Both the Cramer-Rao Lower Bound (CRLB) theory and simulation test are used to demonstrate the higher accuracy of our method compared to methods using a single observation. Furthermore, the proposed system is evaluated in perceptually challenging datasets against various state-of-the-art sensor-fusion methods. The results show that the proposed system achieves real-time and high estimation accuracy performance despite the challenging environment and poor observations.},
  langid = {english},
  pubstate = {preprint},
  keywords = {IMU,卡尔曼,激光雷达,迭代卡尔曼},
  file = {C:\Users\15469\Zotero\storage\6B55QYLR\Han 等 - 2023 - DAMS-LIO A Degeneration-Aware and Modular Sensor-.pdf}
}

@inproceedings{Hartley2018,
  title = {Hybrid {{Contact Preintegration}} for {{Visual-Inertial-Contact State Estimation Using Factor Graphs}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Hartley, Ross and Jadidi, Maani Ghaffari and Gan, Lu and Huang, Jiunn-Kai and Grizzle, Jessy W. and Eustice, Ryan M.},
  date = {2018-10},
  pages = {3783--3790},
  publisher = {{IEEE}},
  location = {{Madrid}},
  doi = {10.1109/IROS.2018.8593801},
  url = {https://ieeexplore.ieee.org/document/8593801/},
  urldate = {2022-12-11},
  abstract = {The factor graph framework is a convenient modeling technique for robotic state estimation where states are represented as nodes, and measurements are modeled as factors. When designing a sensor fusion framework for legged robots, one often has access to visual, inertial, joint encoder, and contact sensors. While visual-inertial odometry has been studied extensively in this framework, the addition of a preintegrated contact factor for legged robots has been only recently proposed. This allowed for integration of encoder and contact measurements into existing factor graphs, however, new nodes had to be added to the graph every time contact was made or broken. In this work, to cope with the problem of switching contact frames, we propose a hybrid contact preintegration theory that allows contact information to be integrated through an arbitrary number of contact switches. The proposed hybrid modeling approach reduces the number of required variables in the nonlinear optimization problem by only requiring new states to be added alongside camera or selected keyframes. This method is evaluated using real experimental data collected from a Cassie-series robot where the trajectory of the robot produced by a motion capture system is used as a proxy for ground truth. The evaluation shows that inclusion of the proposed preintegrated hybrid contact factor alongside visual-inertial navigation systems improves estimation accuracy as well as robustness to vision failure, while its generalization makes it more accessible for legged platforms.},
  eventtitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  isbn = {978-1-5386-8094-0},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\KFR44HQA\Hartley 等 - 2018 - Hybrid Contact Preintegration for Visual-Inertial-.pdf}
}

@inproceedings{Hartley2018a,
  title = {Contact-{{Aided Invariant Extended Kalman Filtering}} for {{Legged Robot State Estimation}}},
  author = {Hartley, R. and Jadidi, M.G. and Grizzle, J.W. and Eustice, R.M.},
  date = {2018},
  doi = {10.15607/RSS.2018.XIV.050},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062981334&doi=10.15607%2fRSS.2018.XIV.050&partnerID=40&md5=8a21719fe871051fbeb22d4c0a2fc97b},
  abstract = {This paper derives a contact-aided inertial navigation observer for a 3D bipedal robot using the theory of invariant observer design. Aided inertial navigation is fundamentally a nonlinear observer design problem; thus, current solutions are based on approximations of the system dynamics, such as an Extended Kalman Filter (EKF), which uses a system’s Jacobian linearization along the current best estimate of its trajectory. On the basis of the theory of invariant observer design by Barrau and Bonnabel, and in particular, the Invariant EKF (InEKF), we show that the error dynamics of the point contact-inertial system follows a log-linear autonomous differential equation; hence, the observable state variables can be rendered convergent with a domain of attraction that is independent of the system’s trajectory. Due to the log-linear form of the error dynamics, it is not necessary to perform a nonlinear observability analysis to show that when using an Inertial Measurement Unit (IMU) and contact sensors, the absolute position of the robot and a rotation about the gravity vector (yaw) are unobservable. We further augment the state of the developed InEKF with IMU biases, as the online estimation of these parameters has a crucial impact on system performance. We evaluate the convergence of the proposed system with the commonly used quaternion-based EKF observer using a Monte-Carlo simulation. In addition, our experimental evaluation using a Cassie-series bipedal robot shows that the contact-aided InEKF provides better performance in comparison with the quaternion-based EKF as a result of exploiting symmetries present in the system dynamics. © 2018, MIT Press Journals. All rights reserved.},
  eventtitle = {Robotics: {{Science}} and {{Systems}}},
  annotation = {rate: 5},
  file = {C:\Users\15469\Zotero\storage\CKPVQNS6\Hartley 等 - 2018 - Contact-Aided Invariant Extended Kalman Filtering .pdf}
}

@inproceedings{Hartley2018b,
  title = {Legged {{Robot State-Estimation Through Combined Forward Kinematic}} and {{Preintegrated Contact Factors}}},
  author = {Hartley, R. and Mangelson, J. and Gan, L. and Jadidi, M.G. and Walls, J.M. and Eustice, R.M. and Grizzle, J.W.},
  date = {2018},
  pages = {4422--4429},
  doi = {10.1109/ICRA.2018.8460748},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062942620&doi=10.1109%2fICRA.2018.8460748&partnerID=40&md5=eeee11ba8a82542e4bf71a9217ac6073},
  abstract = {State-of-the-art robotic perception systems have achieved sufficiently good performance using Inertial Measurement Units (IMUs), cameras, and nonlinear optimization techniques, that they are now being deployed as technologies. However, many of these methods rely significantly on vision and often fail when visual tracking is lost due to lighting or scarcity of features. This paper presents a state-estimation technique for legged robots that takes into account the robot's kinematic model as well as its contact with the environment. We introduce forward kinematic factors and preintegrated contact factors into a factor graph framework that can be incrementally solved in real-time. The forward kinematic factor relates the robot's base pose to a contact frame through noisy encoder measurements. The preintegrated contact factor provides odometry measurements of this contact frame while accounting for possible foot slippage. Together, the two developed factors constrain the graph optimization problem allowing the robot's trajectory to be estimated. The paper evaluates the method using simulated and real sensory IMU and kinematic data from experiments with a Cassie-series robot designed by Agility Robotics. These preliminary experiments show that using the proposed method in addition to IMU decreases drift and improves localization accuracy, suggesting that its use can enable successful recovery from a loss of visual tracking. © 2018 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 5},
  file = {C:\Users\15469\Zotero\storage\S99HXZSH\Hartley 等 - 2018 - Legged Robot State-Estimation Through Combined For.pdf}
}

@thesis{Hartley2019,
  title = {Contact-{{Aided State Estimation}} on {{Lie Groups}} for {{Legged Robot Mapping}} and {{Control}}},
  author = {Hartley, Matthew Ross},
  date = {2019},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\7IYKTJ25\Hartley - Contact-Aided State Estimation on Lie Groups for L.pdf}
}

@article{Hartley2020,
  title = {Contact-Aided Invariant Extended {{Kalman}} Filtering for Robot State Estimation},
  author = {Hartley, R. and Ghaffari, M. and Eustice, R.M. and Grizzle, J.W.},
  date = {2020},
  journaltitle = {International Journal of Robotics Research},
  volume = {39},
  number = {4},
  pages = {402--430},
  doi = {10.1177/0278364919894385},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078316611&doi=10.1177%2f0278364919894385&partnerID=40&md5=de463635629978c6945a15726c5827ad},
  abstract = {Legged robots require knowledge of pose and velocity in order to maintain stability and execute walking paths. Current solutions either rely on vision data, which is susceptible to environmental and lighting conditions, or fusion of kinematic and contact data with measurements from an inertial measurement unit (IMU). In this work, we develop a contact-aided invariant extended Kalman filter (InEKF) using the theory of Lie groups and invariant observer design. This filter combines contact-inertial dynamics with forward kinematic corrections to estimate pose and velocity along with all current contact points. We show that the error dynamics follows a log-linear autonomous differential equation with several important consequences: (a) the observable state variables can be rendered convergent with a domain of attraction that is independent of the system’s trajectory; (b) unlike the standard EKF, neither the linearized error dynamics nor the linearized observation model depend on the current state estimate, which (c) leads to improved convergence properties and (d) a local observability matrix that is consistent with the underlying nonlinear system. Furthermore, we demonstrate how to include IMU biases, add/remove contacts, and formulate both world-centric and robo-centric versions. We compare the convergence of the proposed InEKF with the commonly used quaternion-based extended Kalman filter (EKF) through both simulations and experiments on a Cassie-series bipedal robot. Filter accuracy is analyzed using motion capture, while a LiDAR mapping experiment provides a practical use case. Overall, the developed contact-aided InEKF provides better performance in comparison with the quaternion-based EKF as a result of exploiting symmetries present in system. © The Author(s) 2020.},
  issue = {4},
  annotation = {112 citations (Semantic Scholar/DOI) [2023-04-24] rate: 5},
  file = {C\:\\Users\\15469\\Zotero\\storage\\394CWPQ4\\Hartley 等 - 2020 - Contact-aided invariant extended Kalman filtering .pdf;C\:\\Users\\15469\\Zotero\\storage\\9JZXCMTK\\RIEKF_ErrorDynamics.pdf}
}

@article{Hashemi2022,
  title = {Robust {{Slip-Aware Fusion}} for {{Mobile Robots State Estimation}}},
  author = {Hashemi, E. and He, X. and Johansson, K.H.},
  date = {2022},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  pages = {7896--7903},
  doi = {10.1109/LRA.2022.3184768},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133682208&doi=10.1109%2fLRA.2022.3184768&partnerID=40&md5=8d48a2d9441255a44c5a79920aa15d5b},
  abstract = {A novel robust and slip-aware speed estimation framework is developed and experimentally verified for mobile robot navigation by designing proprioceptive robust observers at each wheel. The observer for each corner is proved to be consistent, in the sense that it can provide an upper bound of the mean square estimation error (MSE) timely. Under proper conditions, the MSE is proved to be uniformly bounded. A covariance intersection fusion method is used to fuse the wheel-level estimates, such that the updated estimate remains consistent. The estimated slips at each wheel are then used for a robust consensus to improve the reliability of speed estimation in harsh and combined-slip scenarios. As confirmed by indoor and outdoor experiments under different surface conditions, the developed framework addresses state estimation challenges for mobile robots that experience uneven torque distribution or large slip. The novel proprioceptive observer can also be integrated with existing tightly-coupled visual-inertial navigation systems.  © 2016 IEEE.},
  issue = {3},
  annotation = {0 citations (Crossref) [2023-04-24]}
}

@article{He2023,
  title = {Symbolic {{Representation}} and {{Toolkit Development}} of {{Iterated Error-State Extended Kalman Filters}} on {{Manifolds}}},
  author = {He, Dongjiao and Xu, Wei and Zhang, Fu},
  date = {2023},
  journaltitle = {IEEE Transactions on Industrial Electronics},
  shortjournal = {IEEE Trans. Ind. Electron.},
  pages = {1--10},
  issn = {0278-0046, 1557-9948},
  doi = {10.1109/TIE.2023.3237872},
  url = {https://ieeexplore.ieee.org/document/10024988/},
  urldate = {2023-05-06},
  abstract = {Error-state extended Kalman filter (ESEKF) is one of extensively used filtering techniques in robot systems. There are many works that cast ESEKF on manifolds to improve consistency. However, most of these works are designed case by case, which makes it difficult to extend to new manifolds. In this paper, we propose a generic method to formulate the iterated error-state extended Kalman filter (IESEKF) on manifolds, which aims to facilitate the deployment of IESEKF for on-manifold systems (e.g., lidar-inertial and visual-inertial systems). Firstly, a canonical on-manifold representation of the robot system is proposed, based on which, an on-manifold IESEKF framework is formulated and solved by linearization at each estimation point. The proposed framework has two main advantages, one is that an equivalent error-state system is derived from linearization, which is minimally-parameterized without any singularities in practice. And the other is that in each step of IESEKF, the manifold constraints are decoupled from the system behaviors, ultimately leading to a generic and symbolic IESEKF framework that naturally evolving on manifolds. Based on the separation of manifold constraints from the system behaviors, the onmanifold IESEKF is implemented as a toolkit in C++ packages, with which the user needs only to provide the system-specific descriptions, and then call the respective filter steps (e.g., predict, update) without dealing with any manifold constraints. The existing implementation supports full iterated Kalman filtering for versatile systems on manifold M = Rm×SO(3)×· · ·×SO(3)× SEN (3)×· · ·×SEN (3)× S2 × · · · × S2 or any of its sub-manifolds, and is extendable to other types of manifold when necessary. The proposed symbolic IESEKF and the developed toolkit are verified by implementing two filter-based tightly-coupled lidar-inertial navigation systems. Results show that, while greatly facilitating the EKF deployment the developed toolkit leads to estimation performances and computation efficiency comparable to handengineered counterparts. Finally, the toolkit is open-sourced at https://github.com/hku-mars/IKFoM. The aimed application is the real-time state estimation of dynamic systems (e.g., robots) whose states are evolving on manifolds.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\QNP2RLFJ\He 等 - 2023 - Symbolic Representation and Toolkit Development of.pdf}
}

@inproceedings{Homberger2019,
  title = {Support Surface Estimation for Legged Robots},
  author = {Homberger, T. and Wellhausen, L. and Fankhauser, P. and Hutter, M.},
  date = {2019},
  volume = {2019-May},
  pages = {8470--8476},
  doi = {10.1109/ICRA.2019.8793646},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069757669&doi=10.1109%2fICRA.2019.8793646&partnerID=40&md5=938e2768b11c78087557ad3dd83346cb},
  abstract = {The high agility of legged systems allows them to operate in rugged outdoor environments. In these situations, knowledge about the terrain geometry is key for foothold planning to enable safe locomotion. However, on penetrable or highly compliant terrain (e.g. grass) the visibility of the supporting ground surface is obstructed, i.e. it cannot directly be perceived by depth sensors. We present a method to estimate the underlying terrain topography by fusing haptic information about foot contact closure locations with exteroceptive sensing. To obtain a dense support surface estimate from sparsely sampled footholds we apply Gaussian process regression. Exteroceptive information is integrated into the support surface estimation procedure by estimating the height of the penetrable surface layer from discrete penetration depth measurements at the footholds. The method is designed such that it provides a continuous support surface estimate even if there is only partial exteroceptive information available due to shadowing effects. Field experiments with the quadrupedal robot ANYmal show how the robot can smoothly and safely navigate in dense vegetation. © 2019 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 3}
}

@inproceedings{Hornung2010,
  title = {Humanoid Robot Localization in Complex Indoor Environments},
  booktitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Hornung, A and Wurm, K M and Bennewitz, M},
  date = {2010-10},
  pages = {1690--1695},
  publisher = {{IEEE}},
  location = {{Taipei}},
  doi = {10.1109/IROS.2010.5649751},
  url = {http://ieeexplore.ieee.org/document/5649751/},
  urldate = {2023-04-25},
  eventtitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2010)},
  isbn = {978-1-4244-6674-0}
}

@inproceedings{Hourdakis2021,
  title = {{{roboSLAM}}: {{Dense RGB-D SLAM}} for {{Humanoid Robots}}},
  shorttitle = {{{roboSLAM}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Hourdakis, Emmanouil and Piperakis, Stylianos and Trahanias, Panos},
  date = {2021-09-27},
  pages = {2224--2231},
  publisher = {{IEEE}},
  location = {{Prague, Czech Republic}},
  doi = {10.1109/IROS51168.2021.9636044},
  url = {https://ieeexplore.ieee.org/document/9636044/},
  urldate = {2023-04-27},
  abstract = {In the current paper we investigate the challenges of localizing walking humanoid robots using Visual SLAM (VSLAM). We propose a novel dense RGB-D SLAM framework that seamlessly integrates with the dynamic state of a humanoid, to provide real-time localization and dense mapping of its surroundings. Following the path of recent research in humanoid localization, in the current work we explore the integration between a VSLAM system and the humanoid state, by considering the gait cycle and the feet contacts. We analyze how these effects undermine the quality of data acquisition and association for VSLAM, by capturing the unilateral ground forces at the robot’s feet, and design a system that mitigates their impact.},
  eventtitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  isbn = {978-1-66541-714-3},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\QYWZACTB\Hourdakis 等 - 2021 - roboSLAM Dense RGB-D SLAM for Humanoid Robots.pdf}
}

@article{Hu2022,
  title = {Tightly-{{Coupled Visual-Inertial-Pressure Fusion Using Forward}} and {{Backward IMU Preintegration}}},
  author = {Hu, Chao and Zhu, Shiqiang and Liang, Yiming and Song, Wei},
  date = {2022-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {7},
  number = {3},
  pages = {6790--6797},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2022.3177847},
  url = {https://ieeexplore.ieee.org/document/9782141/},
  urldate = {2023-01-09},
  abstract = {In this work, we present a visual-inertial-pressure (VIP) fusion method for underwater robot localization. Specifically, this letter focuses on the tightly-coupled fusion of pressure measurements into a visual inertial odometry (VIO) based on sliding window optimization. Previous works used to associate partial pressure measurements with the nearest keyframes, which not only fail to utilize all the pressure measurement information but also introduce measurement errors that only lead to sub-optimal solutions. Inspired by the current tightly-coupled visual-inertial-GPS and visual-inertial-UWB fusion methods, this letter uses IMU preintegration algorithm to derive the pressure factors. Furthermore, we propose a backward IMU preintegration method and the pressure factors are derived using forward or backward IMU preintegration based on the time-offset between the pressure measurements and the adjacent keyframes. Quantitative and qualitative analyses through simulation and real-world datasets experiments demonstrate the effectiveness of the method with negligible time cost.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\ADZZUFX4\Hu 等 - 2022 - Tightly-Coupled Visual-Inertial-Pressure Fusion Us.pdf}
}

@online{Hua2023,
  title = {{{PIEKF-VIWO}}: {{Visual-Inertial-Wheel Odometry}} Using {{Partial Invariant Extended Kalman Filter}}},
  shorttitle = {{{PIEKF-VIWO}}},
  author = {Hua, Tong and Li, Tao and Pei, Ling},
  date = {2023-03-14},
  eprint = {2303.07668},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.07668},
  urldate = {2023-04-26},
  abstract = {Invariant Extended Kalman Filter (IEKF) has been successfully applied in Visual-inertial Odometry (VIO) as an advanced achievement of Kalman filter, showing great potential in sensor fusion. In this paper, we propose partial IEKF (PIEKF), which only incorporates rotation-velocity state into the Lie group structure and apply it for Visual-InertialWheel Odometry (VIWO) to improve positioning accuracy and consistency. Specifically, we derive the rotation-velocity measurement model, which combines wheel measurements with kinematic constraints. The model circumvents the wheel odometer’s 3D integration and covariance propagation, which is essential for filter consistency. And a plane constraint is also introduced to enhance the position accuracy. A dynamic outlier detection method is adopted, leveraging the velocity state output. Through the simulation and real-world test, we validate the effectiveness of our approach, which outperforms the standard Multi-State Constraint Kalman Filter (MSCKF) based VIWO in consistency and accuracy.},
  langid = {english},
  pubstate = {preprint},
  keywords = {IMU,不变卡尔曼,卡尔曼,视觉,轮里程计},
  file = {C:\Users\15469\Zotero\storage\BP4UW9CC\Hua 等 - 2023 - PIEKF-VIWO Visual-Inertial-Wheel Odometry using P.pdf}
}

@article{Huai2022,
  title = {Robocentric Visual–Inertial Odometry},
  author = {Huai, Zheng and Huang, Guoquan},
  date = {2022-06},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {41},
  number = {7},
  pages = {667--689},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364919853361},
  url = {http://journals.sagepub.com/doi/10.1177/0278364919853361},
  urldate = {2023-02-21},
  abstract = {In this paper, we propose a novel robocentric formulation of the visual–inertial navigation system (VINS) within a slidingwindow filtering framework and design an efficient, lightweight, robocentric visual–inertial odometry (R-VIO) algorithm for consistent motion tracking even in challenging environments using only a monocular camera and a six-axis inertial measurement unit (IMU). The key idea is to deliberately reformulate the VINS with respect to a moving local frame, rather than a fixed global frame of reference as in the standard world-centric VINS, in order to obtain relative motion estimates of higher accuracy for updating global pose. As an immediate advantage of this robocentric formulation, the proposed RVIO can start from an arbitrary pose, without the need to align the initial orientation with the global gravitational direction. More importantly, we analytically show that the linearized robocentric VINS does not undergo the observability mismatch issue as in the standard world-centric counterparts that has been identified in the literature as the main cause of estimation inconsistency. Furthermore, we investigate in depth the special motions that degrade the performance in the world-centric formulation and show that such degenerate cases can be easily compensated for by the proposed robocentric formulation, without resorting to additional sensors as in the world-centric formulation, thus leading to better robustness. The proposed R-VIO algorithm has been extensively validated through both Monte Carlo simulation and realworld experiments with different sensing platforms navigating in different environments, and shown to achieve better (or competitive at least) performance than the state-of-the-art VINS, in terms of consistency, accuracy, and efficiency.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\XIY5LI5T\Huai 和 Huang - 2022 - Robocentric visual–inertial odometry.pdf}
}

@online{Huai2023,
  title = {A {{Quick Guide}} for the {{Iterated Extended Kalman Filter}} on {{Manifolds}}},
  author = {Huai, Jianzhu and Gao, Xiang},
  date = {2023-07-18},
  eprint = {2307.09237},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2307.09237},
  urldate = {2023-07-24},
  abstract = {The extended Kalman filter (EKF) is a common state estimation method for discrete nonlinear systems. It recursively executes the propagation step as time goes by and the update step when a set of measurements arrives. In the update step, the EKF linearizes the measurement function only once. In contrast, the iterated EKF (IEKF) refines the state in the update step by iteratively solving a least squares problem. The IEKF has been extended to work with state variables on manifolds which have differentiable ⊞ and ⊟ operators, including Lie groups. However, existing descriptions are often long, deep, and even with errors. This note provides a quick reference for the IEKF on manifolds, using freshman-level matrix calculus. Besides the bare-bone equations, we highlight the key steps in deriving them.},
  langid = {english},
  pubstate = {preprint},
  keywords = {卡尔曼,教程,迭代卡尔曼},
  file = {C:\Users\15469\Zotero\storage\AVN2XXE7\Huai 和 Gao - 2023 - A Quick Guide for the Iterated Extended Kalman Fil.pdf}
}

@online{Imai2022,
  title = {Vision-{{Guided Quadrupedal Locomotion}} in the {{Wild}} with {{Multi-Modal Delay Randomization}}},
  author = {Imai, Chieko Sarah and Zhang, Minghao and Zhang, Yuchen and Kierebinski, Marcin and Yang, Ruihan and Qin, Yuzhe and Wang, Xiaolong},
  date = {2022-07-23},
  eprint = {2109.14549},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2109.14549},
  urldate = {2022-12-11},
  abstract = {Developing robust vision-guided controllers for quadrupedal robots in complex environments with various obstacles, dynamical surroundings and uneven terrains is very challenging. While Reinforcement Learning (RL) provides a promising paradigm for agile locomotion skills with vision inputs in simulation, it is still very challenging to deploy the vision-guided RL policy in the real world. Our key insight is that the asynchronous multi-modal observations, caused by different latencies in different components of the real robot, create a large sim2real gap for a RL policy. In this paper, we propose MultiModal Delay Randomization (MMDR) to address this issue when training in simulation. Specifically, we randomize the selections for both the proprioceptive states and the visual observations in time during training, aiming to simulate the asynchronous inputs when deploying to the real robot. With this technique, we are able to train a RL policy for end-to-end locomotion control in simulation, which can be directly deployed on the real A1 quadruped robot running in the wild. We evaluate our method in different outdoor environments with complex terrain and obstacles. We show that the robot can smoothly maneuver at a high speed while avoiding the obstacles, achieving significant improvement over the baselines. Our project page with videos is at https://mehooz.github.io/mmdr-wild/.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\MFNW7ME2\Imai 等 - 2022 - Vision-Guided Quadrupedal Locomotion in the Wild w.pdf}
}

@online{Jayaraman2023,
  title = {A {{Lie-Theoretic Approach}} to {{Propagating Uncertainty Jointly}} in {{Attitude}} and {{Angular Momentum}}},
  author = {Jayaraman, Amitesh S. and Ye, Jikai and Chirikjian, Gregory S.},
  date = {2023-09-06},
  eprint = {2309.03112},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2309.03112},
  urldate = {2023-09-15},
  abstract = {Dynamic state estimation, as opposed to kinematic state estimation, seeks to estimate not only the orientation of a rigid body but also its angular velocity, through Euler’s equations of rotational motion. This paper demonstrates that the dynamic state estimation problem can be reformulated as estimating a probability distribution on a Lie group defined on phase space (the product space of rotation and angular momentum). The propagation equations are derived nonparametrically for the mean and covariance of the distribution. It is also shown that the equations can be approximately solved by ignoring the third and higher moments of the probability distribution. Numerical experiments show that the distribution constructed from the propagated mean and covariance fits the sample data better than an extended Kalman filter.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\HBLPZINZ\Jayaraman 等 - 2023 - A Lie-Theoretic Approach to Propagating Uncertaint.pdf}
}

@article{Jenelten2020,
  title = {Perceptive {{Locomotion}} in {{Rough Terrain}} - {{Online Foothold Optimization}}},
  author = {Jenelten, F. and Miki, T. and Vijayan, A.E. and Bjelonic, M. and Hutter, M.},
  date = {2020},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {4},
  pages = {5370--5376},
  doi = {10.1109/LRA.2020.3007427},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088690438&doi=10.1109%2fLRA.2020.3007427&partnerID=40&md5=950633cfa6ab941918cf41048e5e39fe},
  abstract = {Compared to wheeled vehicles, legged systems have a vast potential to traverse challenging terrain. To exploit the full potential, it is crucial to tightly integrate terrain perception for foothold planning. We present a hierarchical locomotion planner together with a foothold optimizer that finds locally optimal footholds within an elevation map. The map is generated in real-time from on-board depth sensors. We further propose a terrain-aware contact schedule to deal with actuator velocity limits. We validate the combined locomotion pipeline on our quadrupedal robot ANYmal with a variety of simulated and real-world experiments. We show that our method can cope with stairs and obstacles of heights up to \textbackslash text\{33\}\textbackslash\% of the robot's leg length.  © 2016 IEEE.},
  annotation = {rate: 2}
}

@article{Ji2022,
  title = {Concurrent {{Training}} of a {{Control Policy}} and a {{State Estimator}} for {{Dynamic}} and {{Robust Legged Locomotion}}},
  author = {Ji, G. and Mun, J. and Kim, H. and Hwangbo, J.},
  date = {2022},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {4630--4637},
  doi = {10.1109/LRA.2022.3151396},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124815777&doi=10.1109%2fLRA.2022.3151396&partnerID=40&md5=6bc4d3b01717629f3bf5aad791a2aa1e},
  abstract = {In this letter, we propose a locomotion training framework where a control policy and a state estimator are trained concurrently. The framework consists of a policy network which outputs the desired joint positions and a state estimation network which outputs estimates of the robot's states such as the base linear velocity, foot height, and contact probability. We exploit a fast simulation environment to train the networks and the trained networks are transferred to the real robot. The trained policy and state estimator are capable of traversing diverse terrains such as a hill, slippery plate, and bumpy road. We also demonstrate that the learned policy can run at up to 3.75 m/s on normal flat ground and 3.54 m/s on a slippery plate with the coefficient of friction of 0.22. © 2016 IEEE.},
  issue = {2},
  annotation = {9 citations (Crossref) [2023-04-24]}
}

@article{Jin2023,
  title = {Learning-{{Aided Inertial Odometry With Nonlinear State Estimator}} on {{Manifold}}},
  author = {Jin, Yuqiang and Zhang, Wen-An and Sun, Hu and Yu, Li},
  date = {2023},
  journaltitle = {IEEE Transactions on Intelligent Transportation Systems},
  shortjournal = {IEEE Trans. Intell. Transport. Syst.},
  pages = {1--12},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2023.3273391},
  url = {https://ieeexplore.ieee.org/document/10127586/},
  urldate = {2023-05-20},
  abstract = {Relying only on inertial measurement units (IMUs) for robust state estimation is critical to vehicle safety when imaging sensors abruptly fail. In this paper, we propose to consider learning-based method as a complement to the kinematic model, and obtain ego-motion based on the nonlinear filter pipeline. To be specific, we first model the state of the IMU on the manifold such that the beliefs of prior model are propagated correctly. Then, we construct an uncertainty-aware network to simultaneously learn the integral terms in the kinematic equations, and recursively compute the rigid body position and velocity as pseudo-measurements. We additionally use a nonlinear estimator to properly fuse the model with the learned observations, whose prior information is endowed by model on the manifold, while the updating correction signals are provided by the network on pattern learning, and finally, the split covariance intersection (SCI) is utilized to reasonably handle the unknown correlated information in both. The performance of method is evaluated in terms of accuracy, robustness, extensibility and server aspects using both simulated and real-world dataset. Experimental results demonstrate a promising performance of the proposed method to traditional or learning-based ones.},
  langid = {english},
  keywords = {IMU,机器学习,里程计},
  file = {C:\Users\15469\Zotero\storage\743R4JZG\Jin 等 - 2023 - Learning-Aided Inertial Odometry With Nonlinear St.pdf}
}

@article{Johnson2015,
  title = {Team {{IHMC}}'s Lessons Learned from the {{DARPA}} Robotics Challenge Trials},
  author = {Johnson, M. and Shrewsbury, B. and Bertrand, S. and Wu, T. and Duran, D. and Floyd, M. and Abeles, P. and Stephen, D. and Mertins, N. and Lesman, A. and Carff, J. and Rifenburgh, W. and Kaveti, P. and Straatman, W. and Smith, J. and Griffioen, M. and Layton, B. and De Boer, T. and Koolen, T. and Neuhaus, P. and Pratt, J.},
  date = {2015},
  journaltitle = {Journal of Field Robotics},
  volume = {32},
  number = {2},
  pages = {192--208},
  doi = {10.1002/rob.21571},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922947757&doi=10.1002%2frob.21571&partnerID=40&md5=3fffd86d83de3d748eda49232f1c5f82},
  abstract = {This article is a summary of the experiences of the Florida Institute for Human \& Machine Cognition (IHMC) team during the DARPA Robotics Challenge (DRC) Trials. The primary goal of the DRC is to develop robots capable of assisting humans in responding to natural and manmade disasters. The robots are expected to use standard tools and equipment to accomplish the mission. The DRC Trials consisted of eight different challenges that tested robot mobility, manipulation, and control under degraded communications and time constraints. Team IHMC competed using the Atlas humanoid robot made by Boston Dynamics. We competed against 16 international teams and placed second in the competition. This article discusses the challenges we faced in transitioning from simulation to hardware. It also discusses the lessons learned both during the competition and in the months of preparation leading up to it. The lessons address the value of reliable hardware and solid software practices. They also cover effective approaches to bipedal walking and designing for human-robot teamwork. Lastly, the lessons present a philosophical discussion about choices related to designing robotic systems. © 2015 Wiley Periodicals, Inc.},
  annotation = {rate: 1}
}

@inproceedings{Kang2023,
  title = {External {{Force Estimation}} of {{Legged Robots}} via a {{Factor Graph Framework}} with a {{Disturbance Observer}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Kang, Jeonguk and Kim, Hyun-Bin and Choi, Keun Ha and Kim, Kyung-Soo},
  date = {2023-05-29},
  pages = {12120--12126},
  publisher = {{IEEE}},
  location = {{London, United Kingdom}},
  doi = {10.1109/ICRA48891.2023.10161525},
  url = {https://ieeexplore.ieee.org/document/10161525/},
  urldate = {2023-07-08},
  abstract = {Recently, legged robots have been used for various purposes, such as exploring unknown terrain or interacting with the world. For control and planning legged systems during interactive operations, it is essential to estimate and respond to external forces. However, in legged system, it becomes difficult to estimate forces due to highly dynamic situations. There are several studies that use a force sensor on the foot and end effector, but these approaches have disadvantages in terms of cost and sustainability. Therefore, in this paper, we propose an improved method for estimating external forces without a force sensor. First, each leg force was obtained using the system dynamics of the robot with a disturbance observer. Then, by preintegration, it was tightly coupled with other sensors to estimate the pose and external force simultaneously. Despite the impact and slip, we estimate external forces accurately in standing and walking motions. Moreover, we compared pose estimation performance with VINS-Mono [1], and there is no significant accuracy degradation in spite of highly dynamic force residual.},
  eventtitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {9798350323658},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\TSWNF8CW\Kang 等 - 2023 - External Force Estimation of Legged Robots via a F.pdf}
}

@article{Kang2023a,
  title = {{{RL}} + {{Model-Based Control}}: {{Using On-Demand Optimal Control}} to {{Learn Versatile Legged Locomotion}}},
  shorttitle = {{{RL}} + {{Model-Based Control}}},
  author = {Kang, Dongho and Cheng, Jin and Zamora, Miguel and Zargarbashi, Fatemeh and Coros, Stelian},
  date = {2023-10},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {8},
  number = {10},
  pages = {6619--6626},
  issn = {2377-3766},
  doi = {10.1109/LRA.2023.3307008},
  abstract = {This letter presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits. These reference motions serve as targets for the RL policy to imitate, leading to the development of robust control policies that can be learned with reliability. Furthermore, by utilizing realistic simulation data that captures whole-body dynamics, RL effectively overcomes the inherent limitations in reference motions imposed by modeling simplifications. We validate the robustness and controllability of the RL training process within our framework through a series of experiments. In these experiments, our method showcases its capability to generalize reference motions and effectively handle more complex locomotion tasks that may pose challenges for the simplified model, thanks to RL's flexibility. Additionally, our framework effortlessly supports the training of control policies for robots with diverse dimensions, eliminating the necessity for robot-specific adjustments in the reward function and hyperparameters.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  file = {C\:\\Users\\15469\\Zotero\\storage\\DJ5L2LES\\Kang 等 - 2023 - RL + Model-Based Control Using On-Demand Optimal .pdf;C\:\\Users\\15469\\Zotero\\storage\\EVUEL5VI\\10225268.html}
}

@article{Kim2021,
  title = {Legged {{Robot State Estimation With Dynamic Contact Event Information}}},
  author = {Kim, Joon-Ha and Hong, Seungwoo and Ji, Gwanghyeon and Jeon, Seunghun and Hwangbo, Jemin and Oh, Jun-Ho and Park, Hae-Won},
  date = {2021-10},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {4},
  pages = {6733--6740},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3093876},
  url = {https://ieeexplore.ieee.org/document/9468900/},
  urldate = {2022-10-16},
  abstract = {This letter presents a state estimation algorithm for the legged robot by defining the problem as a Maximum A Posteriori (MAP) estimation problem and solving the problem with the Gauss-Newton algorithm. Moreover, marginalization by the Schur Complement method is adopted to make a fixed size problem. Each component of the cost function and its Jacobian are derived utilizing the SO(3) manifold structure, while we reparameterize the state with nominal state and variation to make linear algebra and vector calculus applied properly. Furthermore, a slip rejection method is proposed to reduce the erroneous effect of fault modeling of kinematics models. The proposed algorithm is verified by comparison with the Invariant Extended Kalman Filter (IEKF) in real robot experiments on various environments.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\BPWRELZL\Kim 等。 - 2021 - Legged Robot State Estimation With Dynamic Contact.pdf}
}

@article{Kim2022,
  title = {{{STEP}}: {{State Estimator}} for {{Legged Robots Using}} a {{Preintegrated Foot Velocity Factor}}},
  author = {Kim, Yeeun and Yu, Byeongho and Lee, Eungchang Mason and Kim, Joon-ha and Park, Hae-won and Myung, Hyun},
  date = {2022},
  journaltitle = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  volume = {7},
  number = {2},
  pages = {8},
  abstract = {We propose a novel state estimator for legged robots, STEP, achieved through a novel preintegrated foot velocity factor. In the preintegrated foot velocity factor, the usual non-slip assumption is not adopted. Instead, the end effector velocity becomes observable by exploiting the body speed obtained from a stereo camera. In other words, the preintegrated end effector’s pose can be estimated. Another advantage of our approach is that it eliminates the necessity for a contact detection step, unlike the typical approaches. The proposed method has also been validated in harsh-environment simulations and real-world experiments containing uneven or slippery terrains.},
  langid = {english},
  keywords = {无需接触估计},
  file = {C:\Users\15469\Zotero\storage\GQQ9ADM6\Kim 等。 - 2022 - STEP State Estimator for Legged Robots Using a Pr.pdf}
}

@online{Koide2023,
  title = {Exact {{Point Cloud Downsampling}} for {{Fast}} and {{Accurate Global Trajectory Optimization}}},
  author = {Koide, Kenji and Oishi, Shuji and Yokozuka, Masashi and Banno, Atsuhiko},
  date = {2023-07-11},
  eprint = {2307.02948},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.02948},
  urldate = {2023-07-12},
  abstract = {This paper presents a point cloud downsampling algorithm for fast and accurate trajectory optimization based on global registration error minimization. The proposed algorithm selects a weighted subset of residuals of the input point cloud such that the subset yields exactly the same quadratic point cloud registration error function as that of the original point cloud at the evaluation point. This method accurately approximates the original registration error function with only a small subset of input points (29 residuals at a minimum). Experimental results using the KITTI dataset demonstrate that the proposed algorithm significantly reduces processing time (by 87\%) and memory consumption (by 99\%) for global registration error minimization while retaining accuracy.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\FISVAJI2\Koide 等 - 2023 - Exact Point Cloud Downsampling for Fast and Accura.pdf}
}

@article{Kuindersma2016,
  title = {Optimization-Based Locomotion Planning, Estimation, and Control Design for the Atlas Humanoid Robot},
  author = {Kuindersma, Scott and Deits, Robin and Fallon, Maurice and Valenzuela, Andrés and Dai, Hongkai and Permenter, Frank and Koolen, Twan and Marion, Pat and Tedrake, Russ},
  date = {2016-03},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  volume = {40},
  number = {3},
  pages = {429--455},
  issn = {0929-5593, 1573-7527},
  doi = {10.1007/s10514-015-9479-3},
  url = {http://link.springer.com/10.1007/s10514-015-9479-3},
  urldate = {2023-02-01},
  abstract = {This paper describes a collection of optimization algorithms for achieving dynamic planning, control, and state estimation for a bipedal robot designed to operate reliably in complex environments. To make challenging locomotion tasks tractable, we describe several novel applications of convex, mixed-integer, and sparse nonlinear optimization to problems ranging from footstep placement to whole-body planning and control. We also present a state estimator formulation that, when combined with our walking controller, permits highly precise execution of extended walking plans over non-flat terrain. We describe our complete system integration and experiments carried out on Atlas, a full-size hydraulic humanoid robot built by Boston Dynamics, Inc.},
  issue = {3},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\ALMR2AIN\Kuindersma 等 - 2016 - Optimization-based locomotion planning, estimation.pdf}
}

@inproceedings{Kumar2022,
  title = {Periodic {{SLAM}}: {{Using Cyclic Constraints}} to {{Improve}} the {{Performance}} of {{Visual-Inertial SLAM}} on {{Legged Robots}}},
  author = {Kumar, H. and Payne, J.J. and Travers, M. and Johnson, A.M. and Choset, H.},
  date = {2022},
  pages = {9477--9483},
  doi = {10.1109/ICRA46639.2022.9811634},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136334430&doi=10.1109%2fICRA46639.2022.9811634&partnerID=40&md5=ceb6319ce3088cc27ddd766c5a545373},
  abstract = {Methods for state estimation that rely on visual information are challenging on legged robots due to rapid changes in the viewing angle of onboard cameras. In this work, we show that by leveraging structure in the way that the robot locomotes, the accuracy of visual-inertial SLAM in these challenging scenarios can be increased. We present a method that takes advantage of the underlying periodic predictability often present in the motion of legged robots to improve the performance of the feature tracking module within a visual-inertial SLAM system. Our method performs multi-session SLAM on a single robot, where each session is responsible for mapping during a distinct portion of the robot's gait cycle. Our method produces lower absolute trajectory error than several state-of-the-art methods for visual-inertial SLAM in both a simulated environment and on data collected on a quadrupedal robot executing dynamic gaits. On real-world bounding gaits, our median trajectory error was less than 35\% of the error of the next best estimate provided by state-of-the-art methods. © 2022 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 4}
}

@inproceedings{Lasguignes2021,
  title = {{{ICP Localization}} and {{Walking Experiments}} on a {{TALOS Humanoid Robot}}},
  author = {Lasguignes, T. and Maroger, I. and Fallon, M. and Ramezani, M. and Marchionni, L. and Stasse, O. and Mansard, N. and Watier, B.},
  date = {2021},
  pages = {800--805},
  doi = {10.1109/ICAR53236.2021.9659474},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124691065&doi=10.1109%2fICAR53236.2021.9659474&partnerID=40&md5=2b9eca98a47a312367c6b3a9073a4963},
  abstract = {This system paper describes the integration and the evaluation of an ICP-based localization system on the TALOS humanoid robot. The new generation of flash LiDAR systems, here an Ouster OS1-64, have made it possible to obtain 3D clouds at 10 Hz. Coupled with an Intel RealSense T265 providing visual-inertial odometry it is possible to localize the robot and use this information to generate foot steps in real time to reach specific points. The approach is validated with a Qualisys motion capture system. It is also used to generate real-time walking motion on the TALOS humanoid robot. This paper is an integration paper showing that it is now feasible to accurately guide a humanoid robot in an environment in real time using a LiDAR system.  © 2021 IEEE.},
  eventtitle = {2021 20th {{International Conference}} on {{Advanced Robotics}}, {{ICAR}} 2021},
  annotation = {rate: 3}
}

@article{Lee,
  title = {Walking {{State Estimation}} for {{Biped Robot Using Foot Contact Information}}},
  author = {Lee, Haeseong and Kim, Myeong-Ju and Sung, Eunho and Park, Jaeheung},
  abstract = {Contact provides useful information in many applications. For bipedal walking, CoP-ZMP criterion, which is computed using the contact wrench between the foot and ground, is one of the important criteria to measure the stability of the biped robot. However, the CoP-ZMP criterion requires an accurate measure of contact point to define a support polygon, which is a difficult problem when the robot walks on uneven terrain. In this respect, this paper proposes an estimation framework, which is regardless of the support polygon, to recognize a current walking state (WS) using the contact information. The framework consists of two indicators and one discriminator. The indicators use the Gaussian Mixture Model to describe sub-states for each input. The discriminator determines the current WS by referencing rules with the outputs from the indicators. Finally, the framework is demonstrated with simulations and real robot experiments using a full-size humanoid robot, TOCABI.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\JU2FJLC8\Lee 等 - Walking State Estimation for Biped Robot Using Foo.pdf}
}

@article{Lee2017,
  title = {Camera-Laser Fusion Sensor System and Environmental Recognition for Humanoids in Disaster Scenarios},
  author = {Lee, I. and Oh, J. and Kim, I. and Oh, J.-H.},
  date = {2017},
  journaltitle = {Journal of Mechanical Science and Technology},
  volume = {31},
  number = {6},
  pages = {2997--3003},
  doi = {10.1007/s12206-017-0543-0},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025692480&doi=10.1007%2fs12206-017-0543-0&partnerID=40&md5=21cd1480a6358d67627464d6ab4bbf5a},
  abstract = {This research aims to develop a vision sensor system and a recognition algorithm to enable a humanoid to operate autonomously in a disaster environment. In disaster response scenarios, humanoid robots that perform manipulation and locomotion tasks must identify the objects in the environment from those challenged by the call by the United States’ Defense Advanced Research Projects Agency, e.g., doors, valves, drills, debris, uneven terrains, and stairs, among others. In order for a humanoid to undertake a number of tasks, we construct a camera–laser fusion system and develop an environmental recognition algorithm. Laser distance sensor and motor are used to obtain 3D cloud data. We project the 3D cloud data onto a 2D image according to the intrinsic parameters of the camera and the distortion model of the lens. In this manner, our fusion sensor system performs functions such as those performed by the RGB-D sensor generally used in segmentation research. Our recognition algorithm is based on super-pixel segmentation and random sampling. The proposed approach clusters the unorganized cloud data according to geometric characteristics, namely, proximity and co-planarity. To assess the feasibility of our system and algorithm, we utilize the humanoid robot, DRC-HUBO, and the results are demonstrated in the accompanying video. © 2017, The Korean Society of Mechanical Engineers and Springer-Verlag GmbH Germany.},
  annotation = {rate: 2}
}

@online{Li2023,
  title = {Errors {{Dynamics}} in {{Affine Group Systems}}},
  author = {Li, Xinghan and Chen, Jianqi and Zhang, Han and Wei, Jieqiang and Wu, Junfeng},
  date = {2023-07-31},
  eprint = {2307.16597},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2307.16597},
  urldate = {2023-08-07},
  abstract = {Errors dynamics captures the evolution of the state errors between two distinct trajectories, that are governed by the same system rule but initiated or perturbed differently. In particular, state observer error dynamics analysis in matrix Lie group is fundamental in practice. In this paper, we focus on the error dynamics analysis for an affine group system under external disturbances or random noises. To this end, we first discuss the connections between the notions of affine group systems and linear group systems. We provide two equivalent characterizations of a linear group system. Such characterizations are based on the homeomorphism of its transition flow and linearity of its Lie algebra counterpart, respectively. Next, we investigate the evolution of a linear group system and we assume it is diffused by a Brownian motion in tangent spaces. We further show that the dynamics projected in the Lie algebra is governed by a stochastic differential equation with a linear drift term. We apply these findings in analyzing the error dynamics. Under differentiable disturbance, we derive an ordinary differential equation characterizing the evolution of the projected errors in the Lie algebra. In addition, the counterpart with stochastic disturbances is derived for the projected errors in terms of a stochastic differential equation. Explicit and accurate derivation of error dynamics is provided for matrix group SEN (3), which plays a vital role especially in robotic applications.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\2LX62R7A\2307.16597.pdf}
}

@online{Li2023a,
  title = {Joint {{Intrinsic}} and {{Extrinsic LiDAR-Camera Calibration}} in {{Targetless Environments Using Plane-Constrained Bundle Adjustment}}},
  author = {Li, Liang and Li, Haotian and Liu, Xiyuan and He, Dongjiao and Miao, Ziliang and Kong, Fanze and Li, Rundong and Liu, Zheng and Zhang, Fu},
  date = {2023-08-24},
  eprint = {2308.12629},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2308.12629},
  urldate = {2023-09-15},
  abstract = {This paper introduces a novel targetless method for joint intrinsic and extrinsic calibration of LiDAR-camera systems using plane-constrained bundle adjustment (BA). Our method leverages LiDAR point cloud measurements from planes in the scene, alongside visual points derived from those planes. The core novelty of our method lies in the integration of visual BA with the registration between visual points and LiDAR point cloud planes, which is formulated as a unified optimization problem. This formulation achieves concurrent intrinsic and extrinsic calibration, while also imparting depth constraints to the visual points to enhance the accuracy of intrinsic calibration. Experiments are conducted on both public data sequences and self-collected dataset. The results showcase that our approach not only surpasses other state-of-the-art (SOTA) methods but also maintains remarkable calibration accuracy even within challenging environments. For the benefits of the robotics community, we have open sourced our codes2.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\7QH5F7BF\Li 等 - 2023 - Joint Intrinsic and Extrinsic LiDAR-Camera Calibra.pdf}
}

@inproceedings{Lin2019,
  title = {Efficient Humanoid Contact Planning Using Learned Centroidal Dynamics Prediction},
  author = {Lin, Y.-C. and Ponton, B. and Righetti, L. and Berenson, D.},
  date = {2019},
  volume = {2019-May},
  pages = {5280--5286},
  doi = {10.1109/ICRA.2019.8794032},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071494131&doi=10.1109%2fICRA.2019.8794032&partnerID=40&md5=d83c064e19bb9767fa388fb82fcce9bd},
  abstract = {Humanoid robots dynamically navigate an environment by interacting with it via contact wrenches exerted at intermittent contact poses. Therefore, it is important to consider dynamics when planning a contact sequence. Traditional contact planning approaches assume a quasi-static balance criterion to reduce the computational challenges of selecting a contact sequence over a rough terrain. This however limits the applicability of the approach when dynamic motions are required, such as when walking down a steep slope or crossing a wide gap. Recent methods overcome this limitation with the help of efficient mixed integer convex programming solvers capable of synthesizing dynamic contact sequences. Nevertheless, its exponential-time complexity limits its applicability to short time horizon contact sequences within small environments. In this paper, we go beyond current approaches by learning a prediction of the dynamic evolution of the robot centroidal momenta, which can then be used for quickly generating dynamically robust contact sequences for robots with arms and legs using a search-based contact planner. We demonstrate the efficiency and quality of the results of the proposed approach in a set of dynamically challenging scenarios. © 2019 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 2}
}

@article{Lin2021,
  title = {Long-Horizon Humanoid Navigation Planning Using Traversability Estimates and Previous Experience},
  author = {Lin, Y.-C. and Berenson, D.},
  date = {2021},
  journaltitle = {Autonomous Robots},
  volume = {45},
  number = {6},
  pages = {937--956},
  doi = {10.1007/s10514-021-09996-3},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115227751&doi=10.1007%2fs10514-021-09996-3&partnerID=40&md5=a1e860dc1f344ebc80f9edc1c29a25cf},
  abstract = {Humanoids’ abilities to navigate stairs and uneven terrain make them well-suited for disaster response efforts. However, humanoid navigation in such environments is currently limited by the capabilities of navigation planners. Such planners typically consider only footstep locations, but planning with palm contacts may be necessary to cross a gap, avoid an obstacle, or maintain balance. However, considering palm contacts greatly increases the branching factor of the search, leading to impractical planning times for large environments. Planning a contact transition sequence in a large environment is important because it verifies that the robot will be able to reach a given goal. In previous work we explored using library-based methods to address difficult navigation planning problems requiring palm contacts, but such methods are not efficient when navigating an easy-to-traverse part of the environment. To maximize planning efficiency, we would like to use discrete planners when an area is easy to traverse and switch to the library-based method only when traversal becomes difficult. Thus, in this paper we present a method that (1) Plans a torso guiding path which accounts for the difficulty of traversing the environment as predicted by learned regressors; and (2) Decomposes the guiding path into a set of segments, each of which is assigned a motion mode (i.e. a set of feet and hands to use) and a planning method. Easily-traversable segments are assigned a discrete-search planner, while other segments are assigned a library-based method that fits existing motion plans to the environment near the given segment. Our results suggest that the proposed approach greatly outperforms standard discrete planning in success rate and planning time. We also show an application of the method to a real robot in a mock disaster scenario. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  annotation = {rate: 2}
}

@online{Lin2022,
  title = {R\$\^3\${{LIVE}}++: {{A Robust}}, {{Real-time}}, {{Radiance}} Reconstruction Package with a Tightly-Coupled {{LiDAR-Inertial-Visual}} State {{Estimator}}},
  shorttitle = {R\$\^3\${{LIVE}}++},
  author = {Lin, Jiarong and Zhang, Fu},
  date = {2022-09-08},
  eprint = {2209.03666},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2209.03666},
  urldate = {2023-07-04},
  abstract = {This work proposed a LiDAR-inertial-visual fusion framework termed R3LIVE++ to achieve robust and accurate state estimation while simultaneously reconstructing the radiance map on the fly. R3LIVE++ consists of a LiDAR-inertial odometry (LIO) and a visual-inertial odometry (VIO), both running in real-time. The LIO subsystem utilizes the measurements from a LiDAR for reconstructing the geometric structure, while the VIO subsystem simultaneously recovers the radiance information of the geometric structure from the input images. R3LIVE++ is developed based on R3LIVE and further improves the accuracy in localization and mapping by accounting for the camera photometric calibration and the online estimation of camera exposure time. We conduct more extensive experiments on public and private datasets to compare our proposed system against other state-of-the-art SLAM systems. Quantitative and qualitative results show that R3LIVE++ has significant improvements over others in both accuracy and robustness. Moreover, to demonstrate the extendability of R3LIVE++, we developed several applications based on our reconstructed maps, such as high dynamic range (HDR) imaging, virtual environment exploration, and 3D video gaming. Lastly, to share our findings and make contributions to the community, we release our codes, hardware design, and dataset on our Github: github.com/hku-mars/r3live.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\NJ49FKV4\Lin 和 Zhang - 2022 - R$^3$LIVE++ A Robust, Real-time, Radiance reconst.pdf}
}

@misc{Lin2022a,
  title = {基于多传感器的融合定位建图系统},
  author = {Lin, Jiarong},
  date = {2022},
  annotation = {abstractTranslation: a},
  file = {C:\Users\15469\Zotero\storage\R8VPP3J2\2023117105211-26cjsq.pdf}
}

@online{Lopez2023,
  title = {A {{Contracting Hierarchical Observer}} for {{Pose-Inertial Fusion}}},
  author = {Lopez, Brett T.},
  date = {2023-03-05},
  eprint = {2303.02777},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2303.02777},
  urldate = {2023-08-04},
  abstract = {This work presents a contracting hierarchical observer that fuses position and orientation measurements with an IMU to generate smooth position, linear velocity, orientation, and IMU bias estimates that are guaranteed to converge to their true values. The proposed approach is composed of two contracting observers. The first is a quaternion-based orientation observer that also estimates gyroscope bias. The output of the orientation observer serves as an input for another contracting observer that estimates position, linear velocity, and accelerometer bias thus forming a hierarchy. We show that the proposed observer guarantees all state estimates converge to their true values. Simulation results confirm the theoretical performance guarantees.},
  langid = {english},
  pubstate = {preprint},
  keywords = {IMU,observer},
  file = {C:\Users\15469\Zotero\storage\IARJFDR4\Lopez - 2023 - A Contracting Hierarchical Observer for Pose-Inert.pdf}
}

@book{Lynch2017,
  title = {Modern Robotics: Mechanics, Planning, and Control},
  shorttitle = {Modern Robotics},
  author = {Lynch, Kevin M. and Park, Frank C.},
  date = {2017},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, UK}},
  isbn = {978-1-107-15630-2 978-1-316-60984-2},
  langid = {english},
  pagetotal = {528},
  annotation = {OCLC: ocn983881868},
  file = {C:\Users\15469\Zotero\storage\HELSNARP\Lynch 和 Park - 2017 - Modern robotics mechanics, planning, and control.pdf}
}

@inproceedings{Lysakowski2022,
  title = {Unsupervised {{Learning}} of {{Terrain Representations}} for {{Haptic Monte Carlo Localization}}},
  author = {Lysakowski, M. and Nowicki, M.R. and Buchanan, R. and Camurri, M. and Fallon, M. and Walas, K.},
  date = {2022},
  pages = {4642--4648},
  doi = {10.1109/ICRA46639.2022.9812296},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136326770&doi=10.1109%2fICRA46639.2022.9812296&partnerID=40&md5=4372a54ebc6d276fcd4837101af17d3f},
  abstract = {Haptic sensing has recently been used effectively for legged robot localization in extreme scenarios where cam-eras and LiDAR might fail, such as dusty mines and foggy sewers. However, existing haptic sensing mainly relies on supervised classification, with training and evaluation executed over explicit terrain classes. Defining classes is a significant limitation to real-world applications, where prior labelling and handcrafted classes are often impractical. This paper proposes a novel haptic localization system based on a fully unsupervised terrain representation learned solely from the force/torque sensors located in the quadruped robot's feet. Instead of using the detected terrain class for localization, we propose an improved autoencoder architecture to generate a sparse map of encodings on the first run and to localize against this sparse map during subsequent runs. We compare our approach to a haptic localization system based on supervised terrain classification, showing that the unsupervised method has comparable or better performance than the supervised one for the same trajectories while clearly outperforming the proprioceptive odometry estimator available on the robot. Therefore, the proposed approach is well-suited for a routine maintenance application, increasing the platform's robustness. © 2022 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 1}
}

@article{Ma2016,
  title = {Real-Time Pose Estimation of a Dynamic Quadruped in {{GPS-denied}} Environments for 24-Hour Operation},
  author = {Ma, J. and Bajracharya, M. and Susca, S. and Matthies, L. and Malchano, M.},
  date = {2016},
  journaltitle = {International Journal of Robotics Research},
  volume = {35},
  number = {6},
  pages = {631--653},
  doi = {10.1177/0278364915587333},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971590297&doi=10.1177%2f0278364915587333&partnerID=40&md5=c86fea6459851eb6e274a17eabbf8d26},
  abstract = {We present a real-time system that enables a highly capable dynamic quadruped robot to maintain an accurate six-degree-of-freedom pose estimate (within a 1.0\% error of distance traveled) over long distances traversed through complex, dynamic outdoor terrain, during day and night, in the presence of camera occlusion and saturation, and occasional large external disturbances, such as slips or falls. The system fuses a stereo-camera sensor, inertial measurement unit, leg odometry, and optional intermittent GPS position updates with an extended Kalman filter to ensure robust, low-latency performance. To maintain a six-degree-of-freedom local positioning accuracy alongside the global positioning knowledge, two reference frames are used; a local reference frame and a global reference frame, with the former benefiting obstacle detection and mapping and the latter for operator-specified and autonomous way-point following. Extensive experimental results obtained from multiple field tests are presented to illustrate the performance and robustness of the system over hours of continuous runs and hundreds of kilometers of distance traveled in a wide variety of terrains and conditions. © SAGE Publications.},
  annotation = {rate: 4},
  file = {C:\Users\15469\Zotero\storage\P6LQU8SE\Ma 等 - 2016 - Real-time pose estimation of a dynamic quadruped i.pdf}
}

@book{Ma2020,
  title = {Kalman {{Filtering}} and {{Information Fusion}}},
  author = {Ma, Hongbin and Yan, Liping and Xia, Yuanqing and Fu, Mengyin},
  date = {2020},
  publisher = {{Springer Singapore}},
  location = {{Singapore}},
  doi = {10.1007/978-981-15-0806-6},
  url = {http://link.springer.com/10.1007/978-981-15-0806-6},
  urldate = {2023-07-24},
  isbn = {9789811508059 9789811508066},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\KJH46VI7\Ma 等 - 2020 - Kalman Filtering and Information Fusion.pdf}
}

@article{Majumdar2023,
  title = {Fundamental Limits for Sensor-Based Robot Control},
  author = {Majumdar, Anirudha and Mei, Zhiting and Pacelli, Vincent},
  date = {2023-08-24},
  journaltitle = {The International Journal of Robotics Research},
  pages = {02783649231190947},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/02783649231190947},
  url = {https://doi.org/10.1177/02783649231190947},
  urldate = {2023-09-16},
  abstract = {Our goal is to develop theory and algorithms for establishing fundamental limits on performance imposed by a robot’s sensors for a given task. In order to achieve this, we define a quantity that captures the amount of task-relevant information provided by a sensor. Using a novel version of the generalized Fano's inequality from information theory, we demonstrate that this quantity provides an upper bound on the highest achievable expected reward for one-step decision-making tasks. We then extend this bound to multi-step problems via a dynamic programming approach. We present algorithms for numerically computing the resulting bounds, and demonstrate our approach on three examples: (i) the lava problem from the literature on partially observable Markov decision processes, (ii) an example with continuous state and observation spaces corresponding to a robot catching a freely-falling object, and (iii) obstacle avoidance using a depth sensor with non-Gaussian noise. We demonstrate the ability of our approach to establish strong limits on achievable performance for these problems by comparing our upper bounds with achievable lower bounds (computed by synthesizing or learning concrete control policies).},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\I4VR4TH8\Majumdar 等 - 2023 - Fundamental limits for sensor-based robot control.pdf}
}

@inproceedings{Manchester2011,
  title = {Stable Dynamic Walking over Rough Terrain: {{Theory}} and Experiment},
  author = {Manchester, I.R. and Mettin, U. and Iida, F. and Tedrake, R.},
  date = {2011},
  volume = {70},
  pages = {123--138},
  doi = {10.1007/978-3-642-19457-3_8},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957998421&doi=10.1007%2f978-3-642-19457-3_8&partnerID=40&md5=924191841be6c2d8f59239fdbee714d3},
  abstract = {We propose a constructive control design for stabilization of non-periodic trajectories of underactuated mechanical systems. An important example of such a system is an underactuated "dynamic walking" biped robot walking over rough terrain. The proposed technique is to compute a transverse linearization about the desired motion: a linear impulsive system which locally represents dynamics about a target trajectory. This system is then exponentially stabilized using a modified receding-horizon control design. The proposed method is experimentally verified using a compass-gait walker: a two-degree-of-freedom biped with hip actuation but pointed stilt-like feet. The technique is, however, very general and can be applied to higher degree-of-freedom robots over arbitrary terrain and other impulsive mechanical systems. © 2011 Springer-Verlag.},
  eventtitle = {Springer {{Tracts}} in {{Advanced Robotics}}},
  issue = {STAR},
  annotation = {rate: 2}
}

@inproceedings{Manchester2011a,
  title = {Stable Dynamic Walking over Uneven Terrain},
  author = {Manchester, I.R. and Mettin, U. and Iida, F. and Tedrake, R.},
  date = {2011},
  volume = {30},
  number = {3},
  pages = {265--279},
  doi = {10.1177/0278364910395339},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952175443&doi=10.1177%2f0278364910395339&partnerID=40&md5=8c2437f20fb721e39120b663d751aa65},
  abstract = {We propose a constructive control design for stabilization of non-periodic trajectories of underactuated robots. An important example of such a system is an underactuated "dynamic walking" biped robot traversing rough or uneven terrain. The stabilization problem is inherently challenging due to the nonlinearity, open-loop instability, hybrid (impact) dynamics, and target motions which are not known in advance. The proposed technique is to compute a transverse linearization about the desired motion: a linear impulsive system which locally represents "transversal" dynamics about a target trajectory. This system is then exponentially stabilized using a modified receding-horizon control design, providing exponential orbital stability of the target trajectory of the original nonlinear system. The proposed method is experimentally verified using a compass-gait walker: a two-degree-of-freedom biped with hip actuation but pointed stilt-like feet. The technique is, however, very general and can be applied to a wide variety of hybrid nonlinear systems. © The Author(s) 2011.},
  eventtitle = {International {{Journal}} of {{Robotics Research}}},
  annotation = {rate: 2}
}

@article{Maravgakis2023,
  title = {Probabilistic {{Contact State Estimation}} for {{Legged Robots}} Using {{Inertial Information}}},
  author = {Maravgakis, Michael and Argiropoulos, Despina-Ekaterini and Piperakis, Stylianos and Trahanias, Panos},
  date = {2023},
  abstract = {Legged robot navigation in unstructured and slippery terrains depends heavily on the ability to accurately identify the quality of contact between the robot’s feet and the ground. Contact state estimation is regarded as a challenging problem and is typically addressed by exploiting force measurements, joint encoders and/or robot kinematics and dynamics. In contrast to most state of the art approaches, the current work introduces a novel probabilistic method for estimating the contact state based solely on proprioceptive sensing, as it is readily available by Inertial Measurement Units (IMUs) mounted on the robot’s end effectors. Capitalizing on the uncertainty of IMU measurements, our method estimates the probability of stable contact. This is accomplished by approximating the multimodal probability density function over a batch of data points for each axis of the IMU with Kernel Density Estimation. The proposed method has been extensively assessed against both real and simulated scenarios on bipedal and quadrupedal robotic platforms such as ATLAS, TALOS and Unitree’s GO1.},
  langid = {english},
  keywords = {IMU,接触估计,状态估计},
  file = {C:\Users\15469\Zotero\storage\XA8RN32R\Maravgakis 等 - Probabilistic Contact State Estimation for Legged .pdf}
}

@article{Masuya2020,
  title = {A Review of State Estimation of Humanoid Robot Targeting the Center of Mass, Base Kinematics, and External Wrench},
  author = {Masuya, K. and Ayusawa, K.},
  date = {2020},
  journaltitle = {Advanced Robotics},
  volume = {34},
  number = {21-22},
  pages = {1380--1389},
  doi = {10.1080/01691864.2020.1835532},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094911905&doi=10.1080%2f01691864.2020.1835532&partnerID=40&md5=32fb450c81c120bb402ff8411df3b715},
  abstract = {The state estimation plays an essential role in controlling humanoid robots. Among the state estimation for humanoid robots, this paper targets the center of mass, the base kinematics, and the external wrench. We summarize the basic ideas to estimate them and briefly review the recent state estimators. © 2020 Informa UK Limited, trading as Taylor \& Francis Group and The Robotics Society of Japan.},
  issue = {21-22},
  annotation = {3 citations (Crossref) [2023-04-24]},
  file = {C:\Users\15469\Zotero\storage\UD4I5V9J\Masuya 和 Ayusawa - 2020 - A review of state estimation of humanoid robot tar.pdf}
}

@inproceedings{Mattamala2021,
  title = {Learning {{Camera Performance Models}} for {{Active Multi-Camera Visual Teach}} and {{Repeat}}},
  author = {Mattamala, M. and Ramezani, M. and Camurri, M. and Fallon, M.},
  date = {2021},
  volume = {2021-May},
  pages = {14346--14352},
  doi = {10.1109/ICRA48506.2021.9561650},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124112287&doi=10.1109%2fICRA48506.2021.9561650&partnerID=40&md5=456b24261aa637dac9a4835b70cd8455},
  abstract = {In dynamic and cramped industrial environments, achieving reliable Visual Teach and Repeat (VT\&R) with a single-camera is challenging. In this work, we develop a robust method for non-synchronized multi-camera VT\&R. Our contribution are expected Camera Performance Models (CPM) which evaluate the camera streams from the teach step to determine the most informative one for localization during the repeat step. By actively selecting the most suitable camera for localization, we are able to successfully complete missions when one of the cameras is occluded, faces into feature poor locations or if the environment has changed. Furthermore, we explore the specific challenges of achieving VT\&R on a dynamic quadruped robot, ANYmal. The camera does not follow a linear path (due to the walking gait and holonomicity) such that precise path-following cannot be achieved. Our experiments feature forward and backward facing stereo cameras showing VT\&R performance in cluttered indoor and outdoor scenarios. We compared the trajectories the robot executed during the repeat steps demonstrating typical tracking precision of less than 10 cm on average. With a view towards omni-directional localization, we show how the approach generalizes to four cameras in simulation. © 2021 IEEE},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 2}
}

@article{Miki2022,
  title = {Learning Robust Perceptive Locomotion for Quadrupedal Robots in the Wild},
  author = {Miki, T. and Lee, J. and Hwangbo, J. and Wellhausen, L. and Koltun, V. and Hutter, M.},
  date = {2022},
  journaltitle = {Science Robotics},
  volume = {7},
  number = {62},
  doi = {10.1126/scirobotics.abk2822},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123560399&doi=10.1126%2fscirobotics.abk2822&partnerID=40&md5=d275cedd5634e954737ba0530ff46934},
  abstract = {Legged robots that can operate autonomously in remote and hazardous environments will greatly increase opportunities for exploration into underexplored areas. Exteroceptive perception is crucial for fast and energy-efficient locomotion: Perceiving the terrain before making contact with it enables planning and adaptation of the gait ahead of time to maintain speed and stability. However, using exteroceptive perception robustly for locomotion has remained a grand challenge in robotics. Snow, vegetation, and water visually appear as obstacles on which the robot cannot step or are missing altogether due to high reflectance. In addition, depth perception can degrade due to difficult lighting, dust, fog, reflective or transparent surfaces, sensor occlusion, and more. For this reason, the most robust and general solutions to legged locomotion to date rely solely on proprioception. This severely limits locomotion speed because the robot has to physically feel out the terrain before adapting its gait accordingly. Here, we present a robust and general solution to integrating exteroceptive and proprioceptive perception for legged locomotion. We leverage an attention-based recurrent encoder that integrates proprioceptive and exteroceptive input. The encoder is trained end to end and learns to seamlessly combine the different perception modalities without resorting to heuristics. The result is a legged locomotion controller with high robustness and speed. The controller was tested in a variety of challenging natural and urban environments over multiple seasons and completed an hour-long hike in the Alps in the time recommended for human hikers. © 2022 American Association for the Advancement of Science. All rights reserved.},
  annotation = {rate: 1}
}

@inproceedings{Mourikis2007,
  title = {A {{Multi-State Constraint Kalman Filter}} for {{Vision-aided Inertial Navigation}}},
  booktitle = {Proceedings 2007 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Mourikis, Anastasios I. and Roumeliotis, Stergios I.},
  date = {2007-04},
  pages = {3565--3572},
  publisher = {{IEEE}},
  location = {{Rome, Italy}},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2007.364024},
  url = {http://ieeexplore.ieee.org/document/4209642/},
  urldate = {2023-05-07},
  abstract = {In this paper, we present an Extended Kalman Filter (EKF)-based algorithm for real-time vision-aided inertial navigation. The primary contribution of this work is the derivation of a measurement model that is able to express the geometric constraints that arise when a static feature is observed from multiple camera poses. This measurement model does not require including the 3D feature position in the state vector of the EKF and is optimal, up to linearization errors. The vision-aided inertial navigation algorithm we propose has computational complexity only linear in the number of features, and is capable of high-precision pose estimation in large-scale real-world environments. The performance of the algorithm is demonstrated in extensive experimental results, involving a camera/IMU system localizing within an urban area.},
  eventtitle = {2007 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  isbn = {978-1-4244-0602-9 978-1-4244-0601-2},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\8HLSWQVI\Mourikis 和 Roumeliotis - 2007 - A Multi-State Constraint Kalman Filter for Vision-.pdf}
}

@incollection{Nelson2018,
  title = {The {{PETMAN}} and {{Atlas Robots}} at {{Boston Dynamics}}},
  booktitle = {Humanoid {{Robotics}}: {{A Reference}}},
  author = {Nelson, G. and Saunders, A. and Playter, R.},
  date = {2018},
  pages = {169--186},
  doi = {10.1007/978-94-007-6046-2_15},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138947766&doi=10.1007%2f978-94-007-6046-2_15&partnerID=40&md5=fafc30138be6e85df6ea0ccf69875320},
  abstract = {In 2008, Boston Dynamics began the development of a series of humanoid robots. The goal was to build bipeds with advanced mobility and dexterity. Our first bipeds, the PETMAN robots, later evolved into three different versions of Atlas. Collectively we programmed these robots to walk at human speeds, cross rock fields, climb stairs and inclines, and negotiate outdoor terrain. We also experimented with augmenting balance using angular momentum. Here we recount the development of these machines and the locomotion behaviors we were able to demonstrate with them. © Springer Nature B.V. 2019.},
  annotation = {rate: 2}
}

@report{Neves2022,
  type = {preprint},
  title = {Decoding {{Reinforcement Learning}} for Newcomers},
  author = {Neves, Francisco and F. Reis, Matheus and Andrade, Gustavo and Aguiar, A. Pedro and Pinto, Andry Maykol},
  date = {2022-11-23},
  doi = {10.36227/techrxiv.21583893.v1},
  url = {https://www.techrxiv.org/articles/preprint/Decoding_Reinforcement_Learning_for_newcomers/21583893/1},
  urldate = {2023-02-25},
  abstract = {Contribution: An intelligible step-by-step Reinforcement Learning (RL) problem formulation and the availability of an easy-to-use demonstrative toolbox for students at various levels (e.g., undergraduate, bachelor, master, doctorate), researchers and educators. This tool facilitates the familiarization with the key concepts of RL, its problem formulation and implementation. The results demonstrated in this paper are produced by a Python program that is released open-source, along with other lecture materials to reduce the learning barriers in such innovative research topic in robotics.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\KHJUFZW7\Neves 等 - 2022 - Decoding Reinforcement Learning for newcomers.pdf}
}

@inproceedings{Nishiwaki2017,
  title = {Autonomous Navigation of a Humanoid Robot over Unknown Rough Terrain},
  author = {Nishiwaki, K. and Chestnutt, J. and Kagami, S.},
  date = {2017},
  volume = {100},
  pages = {619--634},
  doi = {10.1007/978-3-319-29363-9_35},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984829440&doi=10.1007%2f978-3-319-29363-9_35&partnerID=40&md5=946e2097907a0429f6b596968e93381f},
  abstract = {The present paper describes the integration of laser-based perception, footstep planning, and walking control of a humanoid robot for navigation over previously unknown rough terrain. A perception system that obtains the shape of the surrounding environment to an accuracy of a few centimeters is realized based on input obtained using a scanning laser range sensor. A footstep planner decides the sequence of stepping positions using the obtained terrain shape. A walking controller that can cope with a few centimeters error in terrain shape measurement is achieved by combining 40 ms cycle online walking pattern generation and a sensor feedback ground reaction force controller. An operation interface that was developed to send commands to the robot is also presented. A mixed-reality display is adopted in order to realize intuitive interfaces. The navigation system is implemented on the HRP-2, a full-size humanoid robot. The performance of the proposed system for navigation over unknown rough terrain is investigated through several experiments. © Springer International Publishing Switzerland 2017.},
  eventtitle = {Springer {{Tracts}} in {{Advanced Robotics}}},
  annotation = {rate: 2}
}

@inproceedings{Nobili2017,
  title = {Heterogeneous Sensor Fusion for Accurate State Estimation of Dynamic Legged Robots},
  author = {Nobili, S. and Camurri, M. and Barasuol, V. and Focchi, M. and Caldwell, D.G. and Semini, C. and Fallon, M.},
  date = {2017},
  volume = {13},
  doi = {10.15607/rss.2017.xiii.007},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048730738&doi=10.15607%2frss.2017.xiii.007&partnerID=40&md5=703ac4b3f14b6b1ba70d350b2ce7b496},
  abstract = {In this paper we present a system for the state estimation of a dynamically walking and trotting quadruped. The approach fuses four heterogeneous sensor sources (inertial, kinematic, stereo vision and LIDAR) to maintain an accurate and consistent estimate of the robot's base link velocity and position in the presence of disturbances such as slips and missteps. We demonstrate the performance of our system, which is robust to changes in the structure and lighting of the environment, as well as the terrain over which the robot crosses. Our approach builds upon a modular inertial-driven Extended Kalman Filter which incorporates a rugged, probabilistic leg odometry component with additional inputs from stereo visual odometry and LIDAR registration. The simultaneous use of both stereo vision and LIDAR helps combat operational issues which occur in real applications. To the best of our knowledge, this paper is the first to discuss the complexity of consistent estimation of pose and velocity states, as well as the fusion of multiple exteroceptive signal sources at largely different frequencies and latencies, in a manner which is acceptable for a quadruped's feedback controller. A substantial experimental evaluation demonstrates the robustness and accuracy of our system, achieving continuously accurate localization and drift per distance traveled below 1cm/m. © 2017 MIT Press Journals. All rights reserved.},
  eventtitle = {Robotics: {{Science}} and {{Systems}}},
  annotation = {rate: 4}
}

@inproceedings{Nubert2021,
  title = {Self-Supervised {{Learning}} of {{LiDAR Odometry}} for {{Robotic Applications}}},
  author = {Nubert, J. and Khattak, S. and Hutter, M.},
  date = {2021},
  volume = {2021-May},
  pages = {9601--9607},
  doi = {10.1109/ICRA48506.2021.9561063},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125492186&doi=10.1109%2fICRA48506.2021.9561063&partnerID=40&md5=84828d330dfc3487f9cca3a3506b3b6d},
  abstract = {Reliable robot pose estimation is a key building block of many robot autonomy pipelines, with LiDAR localization being an active research domain. In this work, a versatile self-supervised LiDAR odometry estimation method is presented, in order to enable the efficient utilization of all available LiDAR data while maintaining real-time performance. The proposed approach selectively applies geometric losses during training, being cognizant of the amount of information that can be extracted from scan points. In addition, no labeled or ground-truth data is required, hence making the presented approach suitable for pose estimation in applications where accurate ground-truth is difficult to obtain. Furthermore, the presented network architecture is applicable to a wide range of environments and sensor modalities without requiring any network or loss function adjustments. The proposed approach is thoroughly tested for both indoor and outdoor real-world applications through a variety of experiments using legged, tracked and wheeled robots, demonstrating the suitability of learning-based LiDAR odometry for complex robotic applications. © 2021 IEEE},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 2}
}

@misc{Park,
  title = {Manifolds, {{Geometry}},  and {{Robotics}}},
  author = {Park, Frank C.},
  annotation = {abstractTranslation: Park},
  file = {C:\Users\15469\Zotero\storage\TPPDRYN7\Park - Manifolds, Geometry,  and Robotics.pdf}
}

@inproceedings{Patel2022,
  title = {{{LiDAR-guided}} Object Search and Detection in {{Subterranean Environments}}},
  author = {Patel, M. and Waibel, G. and Khattak, S. and Hutter, M.},
  date = {2022},
  pages = {41--46},
  doi = {10.1109/SSRR56537.2022.10018684},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147546878&doi=10.1109%2fSSRR56537.2022.10018684&partnerID=40&md5=5fc28f39df82186ca530b0dc451f19eb},
  abstract = {Detecting objects of interest, such as human survivors, safety equipment, and structure access points, is critical to any search-and-rescue operation. Robots deployed for such time-sensitive efforts rely on their onboard sensors to perform their designated tasks. However, as disaster response operations are predominantly conducted under perceptually degraded conditions, commonly utilized sensors such as visual cameras and LiDARs suffer in terms of performance degradation. In response, this work presents a method that utilizes the complementary nature of vision and depth sensors to leverage multi-modal information to aid object detection at longer distances. In particular, depth and intensity values from sparse LiDAR returns are used to generate proposals for objects present in the environment. These proposals are then utilized by a Pan-Tilt-Zoom (PTZ) camera system to perform a directed search by adjusting its pose and zoom level for performing object detection and classification in difficult environments. The proposed work has been thoroughly verified using an ANYmal quadruped robot in underground settings and on datasets collected during the DARPA Subterranean Challenge finals. © 2022 IEEE.},
  eventtitle = {{{SSRR}} 2022 - {{IEEE International Symposium}} on {{Safety}}, {{Security}}, and {{Rescue Robotics}}},
  annotation = {rate: 2}
}

@article{Popescu2022,
  title = {Experimental {{Investigations}} into {{Using Motion Capture State Feedback}} for {{Real-Time Control}} of a {{Humanoid Robot}}},
  author = {Popescu, Mihaela and Mronga, Dennis and Bergonzani, Ivan and Kumar, Shivesh and Kirchner, Frank},
  date = {2022-12-15},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {22},
  number = {24},
  pages = {9853},
  issn = {1424-8220},
  doi = {10.3390/s22249853},
  url = {https://www.mdpi.com/1424-8220/22/24/9853},
  urldate = {2023-01-09},
  abstract = {Regardless of recent advances, humanoid robots still face significant difficulties in performing locomotion tasks. Among the key challenges that must be addressed to achieve robust bipedal locomotion are dynamically consistent motion planning, feedback control, and state estimation of such complex systems. In this paper, we investigate the use of an external motion capture system to provide state feedback to an online whole-body controller. We present experimental results with the humanoid robot RH5 performing two different whole-body motions: squatting with both feet in contact with the ground and balancing on one leg. We compare the execution of these motions using state feedback from (i) an external motion tracking system and (ii) an internal state estimator based on inertial measurement unit (IMU), forward kinematics, and contact sensing. It is shown that stateof-the-art motion capture systems can be successfully used in the high-frequency feedback control loop of humanoid robots, providing an alternative in cases where state estimation is not reliable.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\2WLBJ8DS\Popescu 等 - 2022 - Experimental Investigations into Using Motion Capt.pdf}
}

@article{Qin2018,
  title = {{{VINS-Mono}}: {{A Robust}} and {{Versatile Monocular Visual-Inertial State Estimator}}},
  shorttitle = {{{VINS-Mono}}},
  author = {Qin, Tong and Li, Peiliang and Shen, Shaojie},
  date = {2018-08},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {34},
  number = {4},
  pages = {1004--1020},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2018.2853729},
  url = {https://ieeexplore.ieee.org/document/8421746/},
  urldate = {2023-04-28},
  abstract = {One camera and one low-cost inertial measurement unit (IMU) form a monocular visual-inertial system (VINS), which is the minimum sensor suite (in size, weight, and power) for the metric six degrees-of-freedom (DOF) state estimation. In this paper, we present VINS-Mono: a robust and versatile monocular visual-inertial state estimator. Our approach starts with a robust procedure for estimator initialization. A tightly coupled, nonlinear optimization-based method is used to obtain highly accurate visual-inertial odometry by fusing preintegrated IMU measurements and feature observations. A loop detection module, in combination with our tightly coupled formulation, enables relocalization with minimum computation. We additionally perform 4DOF pose graph optimization to enforce the global consistency. Furthermore, the proposed system can reuse a map by saving and loading it in an efficient way. The current and previous maps can be merged together by the global pose graph optimization. We validate the performance of our system on public datasets and real-world experiments and compare against other state-of-the-art algorithms. We also perform an onboard closedloop autonomous flight on the microaerial-vehicle platform and port the algorithm to an iOS-based demonstration. We highlight that the proposed work is a reliable, complete, and versatile system that is applicable for different applications that require high accuracy in localization. We open source our implementations for both PCs (https://github.com/HKUST-Aerial-Robotics/VINSMono) and iOS mobile devices (https://github.com/HKUST-AerialRobotics/VINS-Mobile).},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\EXZI5ZCG\Qin 等 - 2018 - VINS-Mono A Robust and Versatile Monocular Visual.pdf}
}

@inproceedings{Qin2020,
  title = {A {{Novel Foot Contact Probability Estimator}} for {{Biped Robot State Estimation}}},
  author = {Qin, M. and Yu, Z. and Chen, X. and Huang, Q. and Fu, C. and Ming, A. and Tao, C.},
  date = {2020},
  pages = {1901--1906},
  doi = {10.1109/ICMA49215.2020.9233715},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096520494&doi=10.1109%2fICMA49215.2020.9233715&partnerID=40&md5=88665be4ed80131600134f8e81ae1340},
  abstract = {State estimation is an important part of biped robot control, but unreliable foot contact estimation would lead to inaccurate state estimation result. In this paper, to reduce the state estimation error caused by the inaccurate contact estimation, we propose a novel simplified contact probability estimator based on the force/torque sensor mounted on the foot. The contact probability is used to tune the covariance matrices of the extended Kalman filter, and the parameters of the probability estimator are optimized iteratively through minimizing the error between state estimation result and ground truth measurement. The experimental result on BHR-6P biped robot shows that the proposed method can effectively reduce the state estimation error.  © 2020 IEEE.},
  eventtitle = {2020 {{IEEE International Conference}} on {{Mechatronics}} and {{Automation}}, {{ICMA}} 2020},
  file = {C:\Users\15469\Zotero\storage\NSTL4U5W\Qin 等 - 2020 - A Novel Foot Contact Probability Estimator for Bip.pdf}
}

@thesis{Qin2020a,
  title = {双足机器人躯干和质心运动状态估计},
  author = {秦, 鸣悦},
  date = {2020},
  institution = {{北京理工大学}},
  annotation = {rate: 5},
  file = {C:\Users\15469\Zotero\storage\8A6J3BPK\2020.06-秦鸣悦-硕士学位论文-双足机器人躯干和质心运动状态估计.pdf}
}

@misc{Qiu2020,
  title = {预积分总结与公式推导},
  author = {邱, 笑晨},
  date = {2020},
  annotation = {titleTranslation: 预积分总结与公式推导},
  file = {C:\Users\15469\Zotero\storage\D3SYZ3HE\预计分总结与公式推导.pdf}
}

@article{Radosavovic2023,
  title = {Learning {{Humanoid Locomotion}} with {{Transformers}}},
  author = {Radosavovic, Ilija and Xiao, Tete and Zhang, Bike and Darrell, Trevor and Malik, Jitendra and Sreenath, Koushil},
  date = {2023},
  abstract = {We present a sim-to-real learning-based approach for real-world humanoid locomotion. Our controller is a causal Transformer trained by autoregressive prediction of future actions from the history of observations and actions. We hypothesize that the observation-action history contains useful information about the world that a powerful Transformer model can use to adapt its behavior in-context, without updating its weights. We do not use state estimation, dynamics models, trajectory optimization, reference trajectories, or pre-computed gait libraries. Our controller is trained with large-scale model-free reinforcement learning on an ensemble of randomized environments in simulation and deployed to the real world in a zero-shot fashion. We evaluate our approach in high-fidelity simulation and successfully deploy it to the real robot as well. To the best of our knowledge, this is the first demonstration of a fully learning-based method for real-world full-sized humanoid locomotion.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\WA2JNUSB\Radosavovic 等 - Learning Humanoid Locomotion with Transformers.pdf}
}

@inproceedings{Rai2018,
  title = {Bayesian {{Optimization Using Domain Knowledge}} on the {{ATRIAS Biped}}},
  author = {Rai, A. and Antonova, R. and Song, S. and Martin, W. and Geyer, H. and Atkeson, C.},
  date = {2018},
  pages = {1771--1778},
  doi = {10.1109/ICRA.2018.8461237},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063147098&doi=10.1109%2fICRA.2018.8461237&partnerID=40&md5=64dda0b0384372cfb0adaa5135e1c78e},
  abstract = {Robotics controllers often consist of expert-designed heuristics, which can be hard to tune in higher dimensions. Simulation can aid in optimizing these controllers if parameters learned in simulation transfer to hardware. Unfortunately, this is often not the case in legged locomotion, necessitating learning directly on hardware. This motivates using data-efficient learning techniques like Bayesian Optimization (BO) to minimize collecting expensive data samples. BO is a black-box data-efficient optimization scheme, though its performance typically degrades in higher dimensions. We aim to overcome this problem by incorporating domain knowledge, with a focus on bipedal locomotion. In our previous work, we proposed a feature transformation that projected a 16-dimensional locomotion controller to a 1-dimensional space using knowledge of human walking. When optimizing a human-inspired neuromuscular controller in simulation, this feature transformation enhanced sample efficiency of BO over traditional BO with a Squared Exponential kernel. In this paper, we present a generalized feature transform applicable to non-humanoid robot morphologies and evaluate it on the ATRIAS bipedal robot, in both simulation and hardware. We present three different walking controllers and two are evaluated on the real robot. Our results show that this feature transform captures important aspects of walking and accelerates learning on hardware and simulation, as compared to traditional BO. © 2018 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 3}
}

@inproceedings{Ramezani2020a,
  title = {Online {{LiDAR-SLAM}} for {{Legged Robots}} with {{Robust Registration}} and {{Deep-Learned Loop Closure}}},
  author = {Ramezani, M. and Tinchev, G. and Iuganov, E. and Fallon, M.},
  date = {2020},
  pages = {4158--4164},
  doi = {10.1109/ICRA40945.2020.9196769},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086239970&doi=10.1109%2fICRA40945.2020.9196769&partnerID=40&md5=27211bee5ec696b38d83812c51e0ac0b},
  abstract = {In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map. © 2020 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 5},
  file = {C:\Users\15469\Zotero\storage\WPL79JT6\Ramezani 等 - 2020 - Online LiDAR-SLAM for Legged Robots with Robust Re.pdf}
}

@inproceedings{Reinke2019,
  title = {A {{Factor Graph Approach}} to {{Multi-camera Extrinsic Calibration}} on {{Legged Robots}}},
  author = {Reinke, A. and Camurri, M. and Semini, C.},
  date = {2019},
  pages = {391--394},
  doi = {10.1109/IRC.2019.00071},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064116633&doi=10.1109%2fIRC.2019.00071&partnerID=40&md5=3a051a79cfbb3b5b83d0082ca9f0e432},
  abstract = {Legged robots are becoming popular not only in research, but also in industry, where they can demonstrate their superiority over wheeled machines in a variety of applications. Either when acting as mobile manipulators or just as all-terrain ground vehicles, these machines need to precisely track the desired base and end-effector trajectories, perform Simultaneous Localization and Mapping (SLAM), and move in challenging environments, all while keeping balance. A crucial aspect for these tasks is that all onboard sensors must be properly calibrated and synchronized to provide consistent signals for all the software modules they feed. In this paper, we focus on the problem of calibrating the relative pose between a set of cameras and the base link of a quadruped robot. This pose is fundamental to successfully perform sensor fusion, state estimation, mapping, and any other task requiring visual feedback. To solve this problem, we propose an approach based on factor graphs that jointly optimizes the mutual position of the cameras and the robot base using kinematics and fiducial markers. We also quantitatively compare its performance with other state-of-the-art methods on the hydraulic quadruped robot HyQ. The proposed approach is simple, modular, and independent from external devices other than the fiducial marker. © 2019 IEEE.},
  eventtitle = {Proceedings - 3rd {{IEEE International Conference}} on {{Robotic Computing}}, {{IRC}} 2019},
  annotation = {rate: 4}
}

@inproceedings{Reinstein2011,
  title = {Dead Reckoning in a Dynamic Quadruped Robot: {{Inertial}} Navigation System Aided by a Legged Odometer},
  shorttitle = {Dead Reckoning in a Dynamic Quadruped Robot},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Reinstein, Michal and Hoffmann, Matej},
  date = {2011-05},
  pages = {617--624},
  publisher = {{IEEE}},
  location = {{Shanghai, China}},
  doi = {10.1109/ICRA.2011.5979609},
  url = {http://ieeexplore.ieee.org/document/5979609/},
  urldate = {2022-12-11},
  abstract = {It is an important ability for any mobile robot to be able to estimate its posture and to gauge the distance it travelled. The information can be obtained from various sources. In this work, we have addressed this problem in a dynamic quadruped robot. We have designed and implemented a navigation algorithm for full body state (position, velocity, and attitude) estimation that does not use any external reference (such as GPS, or visual landmarks). Extended Kalman Filter was used to provide error estimation and data fusion from two independent sources of information: Inertial Navigation System mechanization algorithm processing raw inertial data, and legged odometry, which provided velocity aiding. We present a novel data-driven architecture for legged odometry that relies on a combination of joint sensor signals and pressure sensors. Our navigation system ensures precise tracking of a running robot's posture (roll and pitch), and satisfactory tracking of its position over medium time intervals. We have shown our method to work for two different dynamic turning gaits and on two terrains with significantly different friction. We have also successfully demonstrated how our method generalizes to different velocities.},
  eventtitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-61284-386-5},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\XEXMVQNW\Reinstein 和 Hoffmann - 2011 - Dead reckoning in a dynamic quadruped robot Inert.pdf}
}

@article{Reinstein2013,
  title = {Dead {{Reckoning}} in a {{Dynamic Quadruped Robot Based}} on {{Multimodal Proprioceptive Sensory Information}}},
  author = {Reinstein, Michal and Hoffmann, Matej},
  date = {2013-04},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {29},
  number = {2},
  pages = {563--571},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2012.2228309},
  url = {http://ieeexplore.ieee.org/document/6374692/},
  urldate = {2022-12-11},
  abstract = {It is an important ability for any mobile robot to be able to estimate its posture and to gauge the distance it traveled. In this paper, we have addressed this problem in a dynamic quadruped robot by combining traditional state estimation methods with machine learning. We have designed and implemented a navigation algorithm for full body state (position, velocity, and attitude) estimation that uses no external reference but relies on multimodal proprioceptive sensory information only. The extended Kalman filter (EKF) was used to provide error estimation and data fusion from two independent sources of information: 1) strapdown mechanization algorithm processing raw inertial data and 2) legged odometry.},
  issue = {2},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\KR2WL5SY\Reinstein 和 Hoffmann - 2013 - Dead Reckoning in a Dynamic Quadruped Robot Based .pdf}
}

@article{Remy2017,
  title = {Ambiguous Collision Outcomes and Sliding with Infinite Friction in Models of Legged Systems},
  author = {Remy, C.D.},
  date = {2017},
  journaltitle = {International Journal of Robotics Research},
  volume = {36},
  number = {12},
  pages = {1252--1267},
  doi = {10.1177/0278364917731820},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032436327&doi=10.1177%2f0278364917731820&partnerID=40&md5=7eb8feeef22921e6d0d6b385ae540e86},
  abstract = {This work explores the collision process at foot contact in models of legged robots. In particular, we highlight that for legged systems the widely used assumption of no sliding of the contact points can yield inconsistent outcomes. For certain contact configurations and system parameters, neither the assumption of lift-off of the trailing foot nor the assumption that it stays fixed on the ground yield valid solutions. The foot will slide, even if an infinite coefficient of friction is assumed. In a related effect, we present configurations in which the solution of the collision process is ambiguous. These behaviors are examined for the models of a minimalistic biped, a passive dynamic walker, and the five-link bipedal robot RAMone. The paper provides background for these non-intuitive behaviors and investigates the influence of system parameters onto the collision behavior of a passive dynamic walker. By studying bipedal models that range from minimalistic to physically accurate, this paper establishes a bridge between the theoretical study of unilateral contact in multibody dynamic simulations and the application of these simulation methods in the modeling of ground contact in actual legged systems. © SAGE Publications.},
  annotation = {rate: 5}
}

@inproceedings{Rotella2014,
  title = {State Estimation for a Humanoid Robot},
  author = {Rotella, N. and Bloesch, M. and Righetti, L. and Schaal, S.},
  date = {2014},
  pages = {952--958},
  doi = {10.1109/IROS.2014.6942674},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911490429&doi=10.1109%2fIROS.2014.6942674&partnerID=40&md5=fd41a058b458a049bab521a84d36c2be},
  abstract = {This paper introduces a framework for state estimation on a humanoid robot platform using only common proprioceptive sensors and knowledge of leg kinematics. The presented approach extends that detailed in prior work on a point-foot quadruped platform by adding the rotational constraints imposed by the humanoid's flat feet. As in previous work, the proposed Extended Kalman Filter accommodates contact switching and makes no assumptions about gait or terrain, making it applicable on any humanoid platform for use in any task. A nonlinear observability analysis is performed on both the point-foot and flat-foot filters and it is concluded that the addition of rotational constraints significantly simplifies singular cases and improves the observability characteristics of the system. Results on a simulated walking dataset demonstrate the performance gain of the flat-foot filter as well as confirm the results of the presented observability analysis. © 2014 IEEE.},
  eventtitle = {{{IEEE International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  annotation = {rate: 4}
}

@inproceedings{Rudin2022,
  title = {Advanced {{Skills}} by {{Learning Locomotion}} and {{Local Navigation End-to-End}}},
  author = {Rudin, N. and Hoeller, D. and Bjelonic, M. and Hutter, M.},
  date = {2022},
  volume = {2022-October},
  pages = {2497--2503},
  doi = {10.1109/IROS47612.2022.9981198},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146310202&doi=10.1109%2fIROS47612.2022.9981198&partnerID=40&md5=f948fc01f365b5606341ea333fe23620},
  abstract = {The common approach for local navigation on challenging environments with legged robots requires path planning, path following and locomotion, which usually requires a locomotion control policy that accurately tracks a commanded velocity. However, by breaking down the navigation problem into these sub-tasks, we limit the robot's capabilities since the individual tasks do not consider the full solution space. In this work, we propose to solve the complete problem by training an end-to-end policy with deep reinforcement learning. Instead of continuously tracking a precomputed path, the robot needs to reach a target position within a provided time. The task's success is only evaluated at the end of an episode, meaning that the policy does not need to reach the target as fast as possible. It is free to select its path and the locomotion gait. Training a policy in this way opens up a larger set of possible solutions, which allows the robot to learn more complex behaviors. We compare our approach to velocity tracking and additionally show that the time dependence of the task reward is critical to successfully learn these new behaviors. Finally, we demonstrate the successful deployment of policies on a real quadrupedal robot. The robot is able to cross challenging terrains, which were not possible previously, while using a more energy-efficient gait and achieving a higher success rate. Supplementary videos can be found on the project website: https://sites.google.com/leggedrobotics.com/end-to-end-loco-navigation © 2022 IEEE.},
  eventtitle = {{{IEEE International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  annotation = {rate: 2}
}

@inproceedings{Scona2017,
  title = {Direct Visual {{SLAM}} Fusing Proprioception for a Humanoid Robot},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Scona, Raluca and Nobili, Simona and Petillot, Yvan R. and Fallon, Maurice},
  date = {2017-09},
  pages = {1419--1426},
  publisher = {{IEEE}},
  location = {{Vancouver, BC}},
  doi = {10.1109/IROS.2017.8205943},
  url = {http://ieeexplore.ieee.org/document/8205943/},
  urldate = {2023-04-27},
  abstract = {In this paper we investigate the application of semi-dense visual Simultaneous Localisation and Mapping (SLAM) to the humanoid robotics domain. Challenges of visual SLAM applied to humanoids include the type of dynamic motion executed by the robot, a lack of features in man-made environments and the presence of dynamics in the scene. Previous research on humanoid SLAM focused mostly on feature-based methods which result in sparse environment reconstructions. Instead, we investigate the application of a modern direct method to obtain a semi-dense visually interpretable map which can be used for collision free motion planning. We tackle the challenge of using direct visual SLAM on a humanoid by proposing a more robust pose tracking method. This is formulated as an optimisation problem over a cost function which combines information from the stereo camera and a lowdrift kinematic-inertial motion prior. Extensive experimental demonstrations characterise the performance of our method using the NASA Valkyrie humanoid robot in a laboratory environment equipped with a Vicon motion capture system. Our experiments demonstrate pose tracking robustness to challenges such as sudden view change, motion blur in the image, change in illumination and tracking through sequences of featureless areas in the environment. Finally, we provide a qualitative evaluation of our stereo reconstruction against a LIDAR map.},
  eventtitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  isbn = {978-1-5386-2682-5},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\MBEA3H59\Scona 等 - 2017 - Direct visual SLAM fusing proprioception for a hum.pdf}
}

@inproceedings{Shen2015,
  title = {Tightly-Coupled Monocular Visual-Inertial Fusion for Autonomous Flight of Rotorcraft {{MAVs}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Shen, Shaojie and Michael, Nathan and Kumar, Vijay},
  date = {2015-05},
  pages = {5303--5310},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/ICRA.2015.7139939},
  url = {http://ieeexplore.ieee.org/document/7139939/},
  urldate = {2023-01-10},
  eventtitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-4799-6923-4},
  file = {C:\Users\15469\Zotero\storage\NYJJMJDE\Shen 等 - 2015 - Tightly-coupled monocular visual-inertial fusion f.pdf}
}

@inproceedings{Shiguematsu2019,
  title = {Effects of {{Biped Humanoid Robot Walking Gaits}} on {{Sparse Visual Odometry Algorithms}}},
  author = {Shiguematsu, Y.M. and Brandao, M. and Hashimoto, K. and Takanishi, A.},
  date = {2019},
  volume = {2018-November},
  pages = {160--165},
  doi = {10.1109/HUMANOIDS.2018.8625015},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062301835&doi=10.1109%2fHUMANOIDS.2018.8625015&partnerID=40&md5=565d5a2c5ca81a72240639a67b91ff16},
  abstract = {Motivated by experiments showing that humans regulate their walking speed in order to improve localization performance, in this paper we explore the effects of walking gait on biped humanoid localization. We focus on step length as a proxy for speed and because of its ready applicability to current footstep planners, and we compare the performance of three different sparse visual odometry (VO) algorithms as a function of step length: a direct, a semi-direct and an indirect algorithm. The direct algorithm's performance decreased the longer the step lengths, which along with the analysis of inertial and force/torque data, point to a decrease in performance due to an increase of mechanical vibrations. The indirect algorithm's performance decreased in an opposite way, i.e., showing more errors with shorter step lengths, which we show to be due to the effects of drift over time. The semi-direct algorithm showed a performance in-between the previous two. These observations show that footstep planning could be used to improve the performance of VO algorithms in the future. © 2018 IEEE.},
  eventtitle = {{{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  annotation = {rate: 2}
}

@article{Sleiman2023,
  title = {Versatile {{Multi-Contact Planning}} and {{Control}} for {{Legged Loco-Manipulation}}},
  author = {Sleiman, Jean-Pierre and Farshidian, Farbod and Hutter, Marco},
  date = {2023-08-16},
  journaltitle = {Science Robotics},
  shortjournal = {Sci. Robot.},
  volume = {8},
  number = {81},
  eprint = {2308.09179},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {eadg5014},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.adg5014},
  url = {http://arxiv.org/abs/2308.09179},
  urldate = {2023-09-12},
  abstract = {Loco-manipulation planning skills are pivotal for expanding the utility of robots in everyday environments. These skills can be assessed based on a system's ability to coordinate complex holistic movements and multiple contact interactions when solving different tasks. However, existing approaches have been merely able to shape such behaviors with hand-crafted state machines, densely engineered rewards, or pre-recorded expert demonstrations. Here, we propose a minimally-guided framework that automatically discovers whole-body trajectories jointly with contact schedules for solving general loco-manipulation tasks in pre-modeled environments. The key insight is that multi-modal problems of this nature can be formulated and treated within the context of integrated Task and Motion Planning (TAMP). An effective bilevel search strategy is achieved by incorporating domain-specific rules and adequately combining the strengths of different planning techniques: trajectory optimization and informed graph search coupled with sampling-based planning. We showcase emergent behaviors for a quadrupedal mobile manipulator exploiting both prehensile and non-prehensile interactions to perform real-world tasks such as opening/closing heavy dishwashers and traversing spring-loaded doors. These behaviors are also deployed on the real system using a two-layer whole-body tracking controller.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\HZAJ7W6J\Sleiman 等 - 2023 - Versatile Multi-Contact Planning and Control for L.pdf}
}

@online{Sola2021,
  title = {A Micro {{Lie}} Theory for State Estimation in Robotics},
  author = {Solà, Joan and Deray, Jeremie and Atchuthan, Dinesh},
  date = {2021-12-08},
  eprint = {1812.01537},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.01537},
  urldate = {2022-10-26},
  abstract = {A Lie group is an old mathematical abstract object dating back to the XIX century, when mathematician Sophus Lie laid the foundations of the theory of continuous transformation groups. Its influence has spread over diverse areas of science and technology many years later. In robotics, we are recently experiencing an important trend in its usage, at least in the fields of estimation, and particularly in motion estimation for navigation. Yet for a vast majority of roboticians, Lie groups are highly abstract constructions and therefore difficult to understand and to use.},
  langid = {english},
  pubstate = {preprint},
  annotation = {rate: 5},
  file = {C:\Users\15469\Zotero\storage\A3M7LA77\Solà 等。 - 2021 - A micro Lie theory for state estimation in robotic.pdf}
}

@inproceedings{Stephens2011,
  title = {State Estimation for Force-Controlled Humanoid Balance Using Simple Models in the Presence of Modeling Error},
  author = {Stephens, B.J.},
  date = {2011},
  pages = {3994--3999},
  doi = {10.1109/ICRA.2011.5980358},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871681135&doi=10.1109%2fICRA.2011.5980358&partnerID=40&md5=08f75dce12b3e36000de0adf6852fb5c},
  abstract = {This paper considers the design of state estimators for dynamic balancing systems using a Linear Inverted Pendulum model with unknown modeling errors such as a center of mass measurement offset or an external force. A variety of process and output models are constructed and compared. For a system containing modeling error, it is shown that a naive estimator (one that doesn't account for this error) will result in inaccurate state estimates. These state estimators are evaluated on a force-controlled humanoid robot for a sinusoidal swaying task and a forward push recovery task. © 2011 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 4}
}

@article{Stolzle2022,
  title = {Reconstructing {{Occluded Elevation Information}} in {{Terrain Maps}} with {{Self-Supervised Learning}}},
  author = {Stolzle, M. and Miki, T. and Gerdes, L. and Azkarate, M. and Hutter, M.},
  date = {2022},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {1697--1704},
  doi = {10.1109/LRA.2022.3141662},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123317672&doi=10.1109%2fLRA.2022.3141662&partnerID=40&md5=cc9275fa46a2eacc79cdb7c7dd0e53ad},
  abstract = {Accurate and complete terrain maps enhance the awareness of autonomous robots and enable safe and optimal path planning. Rocks and topography often create occlusions and lead to missing elevation information in the Digital Elevation Map (DEM). Currently, these occluded areas are either fully avoided during motion planning or the missing values in the elevation map are filled-in using traditional interpolation, diffusion or patch-matching techniques. These methods cannot leverage the high-level terrain characteristics and the geometric constraints of line of sight we humans use intuitively to predict occluded areas. We introduce a self-supervised learning approach capable of training on real-world data without a need for ground-truth information to reconstruct the occluded areas in the DEMs. We accomplish this by adding artificial occlusion to the incomplete elevation maps constructed on a real robot by performing ray casting. We first evaluate a supervised learning approach on synthetic data for which we have the full ground-truth available and subsequently move to several real-world datasets. These real-world datasets were recorded during exploration of both structured and unstructured terrain with a legged robot, and additionally in a planetary scenario on Lunar analogue terrain. We state a significant improvement compared to the baseline methods both on synthetic terrain and for the real-world datasets. Our neural network is able to run in real-time on both CPU and GPU with suitable sampling rates for autonomous ground robots. We motivate the applicability of reconstructing occlusion in elevation maps with preliminary motion planning experiments.  © 2016 IEEE.},
  annotation = {rate: 2}
}

@article{Sun2023,
  title = {{{TransFusionOdom}}: {{Transformer-based LiDAR-Inertial Fusion Odometry Estimation}}},
  shorttitle = {{{TransFusionOdom}}},
  author = {Sun, Leyuan and Ding, Guanqun and Qiu, Yue and Yoshiyasu, Yusuke and Kanehiro, Fumio},
  date = {2023},
  journaltitle = {IEEE Sensors Journal},
  shortjournal = {IEEE Sensors J.},
  pages = {1--1},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2023.3302401},
  url = {https://ieeexplore.ieee.org/document/10214516/},
  urldate = {2023-08-17},
  abstract = {Multi-modal fusion of sensors is a commonly used approach to enhance the performance of odometry estimation, which is also a fundamental module for mobile robots. Recently, learning-based approaches garner the attention in this field, due to their robust non-handcrafted designs. However, the question of How to perform fusion among different modalities in a supervised sensor fusion odometry estimation task? is still one of challenging issues remains. Some simple operations, such as element-wise summation and concatenation, are not capable of assigning adaptive attentional weights to incorporate different modalities efficiently, which make it difficult to achieve competitive odometry results. Besides, the Transformer architecture has shown potential for multi-modal fusion tasks, particularly in the domains of vision with language. In this work, we propose an end-to-end supervised Transformer-based LiDAR-Inertial fusion framework (namely TransFusionOdom) for odometry estimation. The multi-attention fusion module demonstrates different fusion approaches for homogeneous and heterogeneous modalities to address the overfitting problem that can arise from blindly increasing the complexity of the model. Additionally, to interpret the learning process of the Transformer-based multi-modal interactions, a general visualization approach is introduced to illustrate the interactions between modalities. Moreover, exhaustive ablation studies evaluate different multi-modal fusion strategies to verify the performance of proposed fusion strategy. A synthetic multi-modal dataset is made public to validate the generalization ability of the proposed fusion strategy, which also works for other combinations of different modalities. The quantitative and qualitative odometry evaluations on the KITTI dataset verify the proposed TransFusionOdom can achieve superior performance compared with other learning-based related works.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\T75C644E\Sun 等 - 2023 - TransFusionOdom Transformer-based LiDAR-Inertial .pdf}
}

@article{Tang2023,
  title = {A {{Comparative Review}} on {{Multi-modal Sensors Fusion}} Based on {{Deep Learning}}},
  author = {Tang, Qin and Liang, Jing and Zhu, Fangqi},
  date = {2023-07},
  journaltitle = {Signal Processing},
  shortjournal = {Signal Processing},
  pages = {109165},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2023.109165},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0165168423002396},
  urldate = {2023-07-07},
  abstract = {The wide deployment of multi-modal sensors in various areas generates vast amounts of data with characteristics of high volume, wide variety, and high integrity. However, traditional data fusion methods face immense challenges when dealing with multimodal data containing abundant intermodality and cross-modality information. Deep learning has the ability to automatically extract and understand the potential association of multi-modal information. Despite this, there is a lack of a comprehensive review of the inherent inference mechanisms of deep learning for multi-modal sensor fusion. This work investigates up-to-date developments in multi-modal sensor fusion via deep learning to provide a broad picture of data fusion needs and technologies. It compares the characteristics of multi-modal data for various sensors, summarizes background concepts about data fusion and deep learning, and carefully reviews a large number of investigations in four inference mechanisms: adaptive learning, deep generative, deep discriminative, and algorithms unrolling. The pros and cons of the above methodologies are presented, and several popular application domains are discussed, including medical imaging, autonomous driving, remote sensing, and robotics. A large collection of multi-modal datasets published in recent years is presented, and several tables that quantitatively compare and summarize the performance of fusion algorithms are provided. Finally, by acknowledging the limitations of current research, we establish potential open challenges and future directions as guidance for deep learning-based multi-sensor fusion.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\FSB56LYU\Tang 等 - 2023 - A Comparative Review on Multi-modal Sensors Fusion.pdf}
}

@inproceedings{Tao2022,
  title = {{{3D Lidar Reconstruction}} with {{Probabilistic Depth Completion}} for {{Robotic Navigation}}},
  author = {Tao, Y. and Popovic, M. and Wang, Y. and Digumarti, S.T. and Chebrolu, N. and Fallon, M.},
  date = {2022},
  volume = {2022-October},
  pages = {5339--5346},
  doi = {10.1109/IROS47612.2022.9981531},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146361153&doi=10.1109%2fIROS47612.2022.9981531&partnerID=40&md5=207c6b474c21d7ff6dc31544f7bac21f},
  abstract = {Safe motion planning in robotics requires planning into space which has been verified to be free of obstacles. However, obtaining such environment representations using lidars is challenging by virtue of the sparsity of their depth measurements. We present a learning-aided 3D lidar reconstruction framework that upsamples sparse lidar depth measurements with the aid of overlapping camera images so as to generate denser reconstructions with more definitively free space than can be achieved with the raw lidar measurements alone. We use a neural network with an encoder-decoder structure to predict dense depth images along with depth uncertainty estimates which are fused using a volumetric mapping system. We conduct experiments on real-world outdoor datasets captured using a handheld sensing device and a legged robot. Using input data from a 16-beam lidar mapping a building network, our experiments showed that the amount of estimated free space was increased by more than 40\% with our approach. We also show that our approach trained on a synthetic dataset generalises well to real-world outdoor scenes without additional fine-tuning. Finally, we demonstrate how motion planning tasks can benefit from these denser reconstructions. © 2022 IEEE.},
  eventtitle = {{{IEEE International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  annotation = {rate: 2}
}

@inproceedings{Teng2022,
  title = {An {{Error-State Model Predictive Control}} on {{Connected Matrix Lie Groups}} for {{Legged Robot Control}}},
  author = {Teng, S. and Chen, D. and Clark, W. and Ghaffari, M.},
  date = {2022},
  volume = {2022-October},
  pages = {8850--8857},
  doi = {10.1109/IROS47612.2022.9981282},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146310911&doi=10.1109%2fIROS47612.2022.9981282&partnerID=40&md5=799fac3c151503c3db8d68fdcbffe249},
  abstract = {This paper reports on a new error-state Model Predictive Control (MPC) approach to connected matrix Lie groups for robot control. The linearized tracking error dynamics and the linearized equations of motion are derived in the Lie algebra. Moreover, given an initial condition, the linearized tracking error dynamics and equations of motion are globally valid and evolve independently of the system trajectory. By exploiting the symmetry of the problem, the proposed approach shows faster convergence of rotation and position simultaneously than the state-of-the-art geometric variational MPC based on variational-based linearization. Numerical simulation on tracking control of a fully-actuated 3D rigid body dynamics confirms the benefits of the proposed approach compared to the baselines. Furthermore, the proposed MPC is also verified in pose control and locomotion experiments on a quadrupedal robot MIT Mini Cheetah. © 2022 IEEE.},
  eventtitle = {{{IEEE International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  annotation = {rate: 2}
}

@article{Tsao2023,
  title = {Analytic {{IMU Preintegration That Associates Uncertainty}} on {{Matrix Lie Groups}} for {{Consistent Visual}}–{{Inertial Navigation Systems}}},
  author = {Tsao, Shu-Hua and Jan, Shau-Shiun},
  date = {2023-06},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {8},
  number = {6},
  pages = {3819--3826},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2023.3273510},
  url = {https://ieeexplore.ieee.org/document/10120754/},
  urldate = {2023-05-18},
  abstract = {We present a novel right-invariant inertial measurement unit (RI IMU) preintegration model and apply it to an optimization-based visual–inertial navigation system (VINS). We find that the unobservable subspace of the proposed estimator was only affected by landmarks from the standpoint of an observability analysis. We highlight that the proposed VINS utilizing the RI IMU preintegration model is consistent with Jacobians evaluated using the most recent estimates of extended poses. Monte Carlo simulations and real-world experiments using the 4Seasons data sets validated our analysis and demonstrated the effectiveness of the proposed VINS by comparing its performance with VINS using different IMU preintegration models and other state-of-the-art VINS.},
  langid = {english},
  keywords = {IMU,不变卡尔曼,预积分},
  file = {C:\Users\15469\Zotero\storage\YYY8G4SM\Tsao 和 Jan - 2023 - Analytic IMU Preintegration That Associates Uncert.pdf}
}

@article{Tu2018,
  title = {State Estimation of a Robot Joint by a Novel Nonlinear Tracking Differentiator},
  author = {Tu, X. and Zhou, Y. and Zhao, P. and Cheng, X.},
  date = {2018},
  journaltitle = {Industrial Robot},
  volume = {45},
  number = {1},
  pages = {11--22},
  doi = {10.1108/IR-08-2017-0149},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038819288&doi=10.1108%2fIR-08-2017-0149&partnerID=40&md5=6600c8e9cf7928d692d14d742b49d96d},
  abstract = {Purpose – This paper aims to present a method for improving the state estimation of a robot in the presence of noise measurement, which can improve the performance of the robot controller. Design/methodology/approach – In this work, a novel nonlinear tracking differentiator (NTD) was formulated to solve the problems of phase lag, low stability and amplitude attenuation faced by traditional tracking differentiators, which can be used for the state estimation of a robot. Based on the user-defined function stu() with linear and nonlinear characteristics, the authors establish a new acceleration function of NTD and confirm its global asymptotic stability by using the Lyapunov method and the system equivalence method. Phase plane analysis shows that the origin is its stable nodal point or focus point and uncovers the basic constraint conditions for parameter regulation. In addition, the convergence property and robustness performance against noises are studied by describing function method. Findings – Comparative simulations, robot state estimation experiments and joint trajectory tracking experiments have indicated that NTD proposed integrates tracking rapidness, accuracy and transitional stability and has high approximation and filtering effects on generalized derivatives of the signal, which contribute to an excellent performance of robot controller in stability and response speed in practice. Originality/value – The main contribution of this paper lies in the design of a novel NTD, which successfully improves the state estimation of a robot joint in noisy surroundings, the tracking performance of robot controller and the stability of the system. © Emerald Publishing Limited},
  issue = {1},
  annotation = {rate: 2}
}

@online{Vaidis2020,
  title = {Improving the {{Iterative Closest Point Algorithm}} Using {{Lie Algebra}}},
  author = {Vaidis, Maxime and Laconte, Johann and Kubelka, Vladimír and Pomerleau, François},
  date = {2020-10-21},
  eprint = {2010.11160},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.11160},
  urldate = {2023-04-26},
  abstract = {Mapping algorithms that rely on registering point clouds inevitably suffer from local drift, both in localization and in the built map. Applications that require accurate maps, such as environmental monitoring, benefit from additional sensor modalities that reduce such drift. In our work, we target the family of mappers based on the Iterative Closest Point (ICP) algorithm which use additional orientation sources such as the Inertial Measurement Unit (IMU). We introduce a new angular penalty term derived from Lie algebra. Our formulation avoids the need for tuning arbitrary parameters. Orientation covariance is used instead, and the resulting error term fits into the ICP cost function minimization problem. Experiments performed on our own real-world data and on the KITTI dataset show consistent behavior while suppressing the effect of outlying IMU measurements. We further discuss promising experiments, which should lead to optimal combination of all error terms in the ICP cost function minimization problem, allowing us to smoothly combine the geometric and inertial information provided by robot sensors.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\RCPPQHF2\Vaidis 等 - 2020 - Improving the Iterative Closest Point Algorithm us.pdf}
}

@article{Valsecchi2020,
  title = {Quadrupedal {{Locomotion}} on {{Uneven Terrain}} with {{Sensorized Feet}}},
  author = {Valsecchi, G. and Grandia, R. and Hutter, M.},
  date = {2020},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {2},
  pages = {1548--1555},
  doi = {10.1109/LRA.2020.2969160},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079666803&doi=10.1109%2fLRA.2020.2969160&partnerID=40&md5=3d67de43a65087f062f3b27744fc10e5},
  abstract = {Sensing of the terrain shape is crucial for legged robots deployed in the real world since the knowledge of the local terrain inclination at the contact points allows for an optimized force distribution that minimizes the risk of slipping. In this letter, we present a reactive locomotion strategy for torque controllable quadruped robots based on sensorized feet. Since the present approach works without exteroceptive sensing, it is robust against degraded vision. Inertial and force/torque sensors implemented in specially designed feet with articulated passive ankle joints measure the local terrain inclination and interaction forces. The proposed controller exploits the contact null-space in order to minimize the tangential forces to prevent slippage even in case of extreme contact conditions. We experimentally tested the proposed method in laboratory experiments and validated the approach with the quadrupedal robot ANYmal. © 2016 IEEE.},
  annotation = {rate: 1}
}

@inproceedings{VanNam2021,
  title = {Deep {{Learning}} Based-{{State Estimation}} for {{Holonomic Mobile Robots Using Intrinsic Sensors}}},
  author = {Van Nam, D. and Gon-Woo, K.},
  date = {2021},
  volume = {2021-October},
  pages = {12--16},
  doi = {10.23919/ICCAS52745.2021.9650051},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124206316&doi=10.23919%2fICCAS52745.2021.9650051&partnerID=40&md5=1fed5ddf28982656ae7ad4d4debb472a},
  abstract = {State estimation is a fundamental component of the navigation system of autonomous mobile robots. Generally, the robot setup is equipped with intrinsic and extrinsic sensors. The state estimators have relied almost on intrinsic sensors such as wheel encoders and inertial measurement units in textureless and structureless environments. This paper will analyze and propose the learning state estimation frameworks for the dead-reckoning of autonomous holonomic vehicles based only on intrinsic sensors. First, we review and categories the intrinsic-only estimation problem. Second, we describe the problem formulation using learning-based techniques. Next, the learning inertial-only estimation is presented with several strategies using the deep learning technique. The initial experiment results are analyzed and deployed using a holonomic mobile robot in real-world environments.  © 2021 ICROS.},
  eventtitle = {International {{Conference}} on {{Control}}, {{Automation}} and {{Systems}}},
  annotation = {rate: 2}
}

@online{Vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-10-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\I9S4NVHA\Vaswani 等。 - 2017 - Attention Is All You Need.pdf}
}

@article{Vigne2020,
  title = {State {{Estimation}} for a {{Legged Robot}} with {{Multiple Flexibilities Using IMUs}}: {{A Kinematic Approach}}},
  author = {Vigne, M. and Khoury, A.E. and Meglio, F.D. and Petit, N.},
  date = {2020},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {1},
  pages = {195--202},
  doi = {10.1109/LRA.2019.2953006},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077753368&doi=10.1109%2fLRA.2019.2953006&partnerID=40&md5=e6eeb3378c39c10b1e1dc055e1730849},
  abstract = {This letter presents a general state estimation method for legged robots having internal flexibilities. In details, the method is applicable to any robot with an arbitrary number of punctual deformations, during standing and walking phases on flat ground. Focused on balance applications, this estimator reconstructs the position relative to the contact foot, the absolute orientation and the world velocity of each robot body. It reconciles the rigid kinematics, given by joint encoders, with attitude measurements obtained from several IMUs, through a kinematic model incorporating the flexibilities. Compared to previous works, no dynamic model of the flexibilities is employed. For illustration, the estimator is successfully tested on the exoskeleton Atalante, both in static and while walking. It is then used in closed-loop to control the position of the flying foot during a quasi-static step. © 2019 IEEE.},
  issue = {1},
  annotation = {rate: 2}
}

@inproceedings{Wagner2016,
  title = {Foot Contact Estimation for Legged Robots in Rough Terrain},
  author = {Wagner, L. and Fankhauser, P. and Bloesch, M. and Hutter, M.},
  date = {2016},
  pages = {395--403},
  doi = {10.1142/9789813149137_0047},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999700063&doi=10.1142%2f9789813149137_0047&partnerID=40&md5=90662b911d968e16522f394c80ab1d50},
  abstract = {Accurate sensing of the terrain shape is a key issue for robots moving in rough environments. For legged robots, an important aspect is the terrain inclination for each foot which is in contact with the ground. Knowledge about the terrain inclination is required to design controllers that mitigate the risk of slippage by minimizing the tangential forces applied on the ground. In this paper, we introduce a method to estimate the contact surface normal for each foot of a legged robot relying solely on measurements from the joint torques and from a force sensor located at the foot. The force sensors at the foot optically determine the deformation of the foot to estimate the force applied to the sensor (optical force sensors). We formulate a measurement model of the optical force sensor and combine it with the joint torque measurements in an Extended Kalman Filter (EKF). The resulting method is able to determine the contact force and surface normal through contact of the foot with the ground. The approach is implemented and evaluated on a torque-controllable quadrupedal robot and shown to reliably estimate the surface normal during dynamic motions for each leg individually. © 2016, World Scientific Publishing Co. Pte Ltd. All rights reserved.},
  eventtitle = {Advances in {{Cooperative Robotics}}: {{Proceedings}} of the 19th {{International Conference}} on {{Climbing}} and {{Walking Robots}} and the {{Support Technologies}} for {{Mobile Machines}}, {{CLAWAR}} 2016},
  annotation = {rate: 2}
}

@inproceedings{Wang2023,
  title = {{{FDIO}}: {{Extended Kalman Filter-Aided Deep Inertial Odometry}}},
  shorttitle = {{{FDIO}}},
  booktitle = {2023 {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  author = {Wang, Yingying and Cheng, Hu and Meng, Max Q.-H.},
  date = {2023-07-08},
  pages = {482--487},
  publisher = {{IEEE}},
  location = {{Sanya, China}},
  doi = {10.1109/ICARM58088.2023.10218871},
  url = {https://ieeexplore.ieee.org/document/10218871/},
  urldate = {2023-09-15},
  abstract = {Smartphone-based deep inertial odometry has recently gained great research interest, which utilizes the inertial measurement unit (IMU) and deep learning technique for relative states estimate. In this paper, we propose FDIO: a Filter-aided Deep Inertial Odometry, which utilizes an extended Kalman filter (EKF) for orientation tracking following by a learning module for position estimation. Our inertial odometry requires only the inertial signals without relying on external device orientation information. In the position learning module, this paper proposes a novel representation of the pedestrians’ velocity and a robust loss for regression. The proposed FDIO is validated by using the public RoNIN dataset. Experimental results show that our model outperforms state-of-the-art deep inertial odometry architectures.},
  eventtitle = {2023 {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  isbn = {9798350300178},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\M7HS3RGT\Wang 等 - 2023 - FDIO Extended Kalman Filter-Aided Deep Inertial O.pdf}
}

@article{Wang2023a,
  title = {{{SW-LIO}}: {{A Sliding Window Based Tightly Coupled LiDAR-Inertial Odometry}}},
  shorttitle = {{{SW-LIO}}},
  author = {Wang, Zelin and Liu, Xu and Yang, Limin and Gao, Feng},
  date = {2023-10},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {8},
  number = {10},
  pages = {6675--6682},
  issn = {2377-3766},
  doi = {10.1109/LRA.2023.3311371},
  abstract = {This letter presents SW-LIO, a tightly coupled LiDAR-inertial odometry based on the sliding window approach. The proposed methodology encompasses rapid ground segmentation and the design of an iterative error-state Kalman filter (ESKF) to effectively fuse LiDAR point clouds and IMU measurements. By establishing a coupling relationship between the current state and the previous frames through the sliding window, the point clouds from the previous frames serve as a constraint for the current pose, resulting in more accurate state estimation. Furthermore, ground residual, bias residual and gravity residual are proposed, enabling more precise estimation of state variables beyond pose. These enhancements enable the system to deliver superior initial values for the filter in fast-moving or unstable environments, thereby improving the system's robustness. To evaluate the proposed framework, comprehensive testing has been conducted on public datasets as well as challenging real-world scenarios. The experimental results demonstrate that SW-LIO outperforms other state-of-the-art methods in terms of robustness and precision while maintaining similar time consumption.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  file = {C\:\\Users\\15469\\Zotero\\storage\\HUFN444I\\Wang 等 - 2023 - SW-LIO A Sliding Window Based Tightly Coupled LiD.pdf;C\:\\Users\\15469\\Zotero\\storage\\IGEXRPBU\\10238772.html}
}

@article{Wellhausen2019,
  title = {Where Should i Walk({{Predicting}} Terrain Properties from Images via Self-Supervised Learning},
  author = {Wellhausen, L. and Dosovitskiy, A. and Ranftl, R. and Walas, K. and Cadena, C. and Hutter, M.},
  date = {2019},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {4},
  number = {2},
  pages = {1509--1516},
  doi = {10.1109/LRA.2019.2895390},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063310966&doi=10.1109%2fLRA.2019.2895390&partnerID=40&md5=b2d8b708cb3e5c37a48cc01ca9f0db01},
  abstract = {Legged robots have the potential to traverse diverse and rugged terrain. To find a safe and efficient navigation path and to carefully select individual footholds, it is useful to be able to predict properties of the terrain ahead of the robot. In this letter, we propose a method to collect data from robot-terrain interaction and associate it to images. Using sparse data acquired in teleoperation experiments with a quadrupedal robot, we train a neural network to generate a dense prediction of the terrain properties in front of the robot. To generate training data, we project the foothold positions from the robot trajectory into on-board camera images. We then attach labels to these footholds by identifying the dominant features of the force-torque signal measured with sensorized feet. We show that data collected in this fashion can be used to train a convolutional network for terrain property prediction as well as weakly supervised semantic segmentation. Finally, we show that the predicted terrain properties can be used for autonomous navigation of the ANYmal quadruped robot. © 2016 IEEE.},
  annotation = {rate: 3}
}

@article{Wisth2019,
  title = {Robust {{Legged Robot State Estimation Using Factor Graph Optimization}}},
  author = {Wisth, D. and Camurri, M. and Fallon, M.},
  date = {2019},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {4},
  number = {4},
  pages = {4507--4514},
  doi = {10.1109/LRA.2019.2933768},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077492783&doi=10.1109%2fLRA.2019.2933768&partnerID=40&md5=f90347e77410822cab791c44e356dcf3},
  abstract = {Legged robots, specifically quadrupeds, are becoming increasingly attractive for industrial applications such as inspection. However, to leave the laboratory and to become useful to an end user requires reliability in harsh conditions. From the perspective of state estimation, it is essential to be able to accurately estimate the robot's state despite challenges such as uneven or slippery terrain, textureless and reflective scenes, as well as dynamic camera occlusions. We are motivated to reduce the dependency on foot contact classifications, which fail when slipping, and to reduce position drift during dynamic motions such as trotting. To this end, we present a factor graph optimization method for state estimation which tightly fuses and smooths inertial navigation, leg odometry and visual odometry. The effectiveness of the approach is demonstrated using the ANYmal quadruped robot navigating in a realistic outdoor industrial environment. This experiment included trotting, walking, crossing obstacles and ascending a staircase. The proposed approach decreased the relative position error by up to 55\% and absolute position error by 76\% compared to kinematic-inertial odometry. © 2016 IEEE.},
  annotation = {rate: 5}
}

@inproceedings{Wisth2020,
  title = {Preintegrated {{Velocity Bias Estimation}} to {{Overcome Contact Nonlinearities}} in {{Legged Robot Odometry}}},
  author = {Wisth, D. and Camurri, M. and Fallon, M.},
  date = {2020},
  pages = {392--398},
  doi = {10.1109/ICRA40945.2020.9197214},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086790135&doi=10.1109%2fICRA40945.2020.9197214&partnerID=40&md5=da8356c52f14495f6644c4225f4ccd3b},
  abstract = {In this paper, we present a novel factor graph formulation to estimate the pose and velocity of a quadruped robot on slippery and deformable terrain. The factor graph introduces a preintegrated velocity factor that incorporates velocity inputs from leg odometry and also estimates related biases. From our experimentation we have seen that it is difficult to model uncertainties at the contact point such as slip or deforming terrain, as well as leg flexibility. To accommodate for these effects and to minimize leg odometry drift, we extend the robot's state vector with a bias term for this preintegrated velocity factor. The bias term can be accurately estimated thanks to the tight fusion of the preintegrated velocity factor with stereo vision and IMU factors, without which it would be unobservable. The system has been validated on several scenarios that involve dynamic motions of the ANYmal robot on loose rocks, slopes and muddy ground. We demonstrate a 26\% improvement of relative pose error compared to our previous work and 52\% compared to a state-of-the-art proprioceptive state estimator. © 2020 IEEE.},
  eventtitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  annotation = {rate: 5},
  file = {C:\Users\15469\Zotero\storage\WDLQLZP5\Wisth 等 - 2020 - Preintegrated Velocity Bias Estimation to Overcome.pdf}
}

@article{Wisth2021,
  title = {Unified {{Multi-Modal Landmark Tracking}} for {{Tightly Coupled Lidar-Visual-Inertial Odometry}}},
  author = {Wisth, David and Camurri, Marco and Das, Sandipan and Fallon, Maurice},
  date = {2021-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {2},
  pages = {1004--1011},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3056380},
  url = {https://ieeexplore.ieee.org/document/9345356/},
  urldate = {2023-04-26},
  abstract = {We present an efficient multi-sensor odometry system for mobile platforms that jointly optimizes visual, lidar, and inertial information within a single integrated factor graph. This runs in real-time at full framerate using fixed lag smoothing. To perform such tight integration, a new method to extract 3D line and planar primitives from lidar point clouds is presented. This approach overcomes the suboptimality of typical frame-to-frame tracking methods by treating the primitives as landmarks and tracking them over multiple scans. True integration of lidar features with standard visual features and IMU is made possible using a subtle passive synchronization of lidar and camera frames. The lightweight formulation of the 3D features allows for real-time execution on a single CPU. Our proposed system has been tested on a variety of platforms and scenarios, including underground exploration with a legged robot and outdoor scanning with a dynamically moving handheld device, for a total duration of 96 min and 2.4 km traveled distance. In these test sequences, using only one exteroceptive sensor leads to failure due to either underconstrained geometry (affecting lidar) or textureless areas caused by aggressive lighting changes (affecting vision). In these conditions, our factor graph naturally uses the best information available from each sensor modality without any hard switches.},
  langid = {english},
  annotation = {rate: 0},
  file = {C:\Users\15469\Zotero\storage\T33EK63J\Wisth 等 - 2021 - Unified Multi-Modal Landmark Tracking for Tightly .pdf}
}

@article{Wisth2023,
  title = {{{VILENS}}: {{Visual}}, {{Inertial}}, {{Lidar}}, and {{Leg Odometry}} for {{All-Terrain Legged Robots}}},
  author = {Wisth, D. and Camurri, M. and Fallon, M.},
  date = {2023},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {39},
  number = {1},
  pages = {309--326},
  doi = {10.1109/TRO.2022.3193788},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136130682&doi=10.1109%2fTRO.2022.3193788&partnerID=40&md5=d85c02f3e034fc40f34883fef265713b},
  abstract = {We present visual inertial lidar legged navigation system (VILENS), an odometry system for legged robots based on factor graphs. The key novelty is the tight fusion of four different sensor modalities to achieve reliable operation when the individual sensors would otherwise produce degenerate estimation. To minimize leg odometry drift, we extend the robot's state with a linear velocity bias term, which is estimated online. This bias is observable because of the tight fusion of this preintegrated velocity factor with vision, lidar, and inertial measurement unit (IMU) factors. Extensive experimental validation on different ANYmal quadruped robots is presented, for a total duration of 2 h and 1.8 km traveled. The experiments involved dynamic locomotion over loose rocks, slopes, and mud, which caused challenges such as slippage and terrain deformation. Perceptual challenges included dark and dusty underground caverns, and open and feature-deprived areas. We show an average improvement of 62\% translational and 51\% rotational errors compared to a state-of-the-art loosely coupled approach. To demonstrate its robustness, VILENS was also integrated with a perceptive controller and a local path planner. © 2004-2012 IEEE.},
  keywords = {camera,factor graph,IMU,leg odometry,激光雷达,状态估计},
  annotation = {rate: 4},
  file = {C:\Users\15469\Zotero\storage\RENBWCS3\Wisth 等 - 2023 - VILENS Visual, Inertial, Lidar, and Leg Odometry .pdf}
}

@inproceedings{Xinjilefu2014,
  title = {Dynamic State Estimation Using {{Quadratic Programming}}},
  booktitle = {2014 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Xinjilefu, X and Feng, Siyuan and Atkeson, Christopher G.},
  date = {2014-09},
  pages = {989--994},
  publisher = {{IEEE}},
  location = {{Chicago, IL, USA}},
  doi = {10.1109/IROS.2014.6942679},
  url = {http://ieeexplore.ieee.org/document/6942679/},
  urldate = {2023-01-12},
  abstract = {We propose a framework for using full-body dynamics for humanoid state estimation. It is formulated as an optimization problem and solved with Quadratic Programming (QP). This formulation provides two main advantages over a nonlinear Kalman filter for dynamic state estimation. QP does not require the dynamic system to be written in the state space form, and it handles equality and inequality constraints naturally. The QP state estimator considers modeling error as part of the optimization vector and includes it in the cost function. The proposed QP state estimator is tested on a Boston Dynamics Atlas humanoid robot.},
  eventtitle = {2014 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2014)},
  isbn = {978-1-4799-6934-0 978-1-4799-6931-9},
  langid = {english},
  annotation = {rate: 2},
  file = {C:\Users\15469\Zotero\storage\97DBZZI3\Xinjilefu 等 - 2014 - Dynamic state estimation using Quadratic Programmi.pdf}
}

@thesis{Xinjilefu2015,
  title = {State Estimation for Humanoid Robotics},
  author = {Xinjilefu, X},
  date = {2015},
  annotation = {rate: 3},
  file = {C:\Users\15469\Zotero\storage\XL6L8T37\Xinjilefu - 2015 - state estimation for humanoid robotics.pdf}
}

@inproceedings{Xinjilefu2015a,
  title = {Center of Mass Estimator for Humanoids and Its Application in Modelling Error Compensation, Fall Detection and Prevention},
  booktitle = {2015 {{IEEE-RAS}} 15th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Xinjilefu, X. and Feng, Siyuan and Atkeson, Christopher G.},
  date = {2015-11},
  pages = {67--73},
  publisher = {{IEEE}},
  location = {{Seoul, South Korea}},
  doi = {10.1109/HUMANOIDS.2015.7363533},
  url = {http://ieeexplore.ieee.org/document/7363533/},
  urldate = {2023-01-12},
  eventtitle = {2015 {{IEEE-RAS}} 15th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  isbn = {978-1-4799-6885-5},
  langid = {english},
  annotation = {rate: 3},
  file = {C:\Users\15469\Zotero\storage\WTLR4BF2\Xinjilefu 等 - 2015 - Center of mass estimator for humanoids and its app.pdf}
}

@inproceedings{Xiong2019,
  title = {A {{Double Stage EKF-based Stereo Visual Inertial Odometry}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Real-time Computing}} and {{Robotics}} ({{RCAR}})},
  author = {Xiong, Xiaogang and Chen, Wenqing and Liu, Zhichao and Shen, Qiang},
  date = {2019-08},
  pages = {947--952},
  publisher = {{IEEE}},
  location = {{Irkutsk, Russia}},
  doi = {10.1109/RCAR47638.2019.9043969},
  url = {https://ieeexplore.ieee.org/document/9043969/},
  urldate = {2023-01-10},
  eventtitle = {2019 {{IEEE International Conference}} on {{Real-time Computing}} and {{Robotics}} ({{RCAR}})},
  isbn = {978-1-72813-726-1},
  file = {C:\Users\15469\Zotero\storage\SXBDG2RC\Xiong 等 - 2019 - A Double Stage EKF-based Stereo Visual Inertial Od.pdf}
}

@online{Xu2021,
  title = {{{FAST-LIO}}: {{A Fast}}, {{Robust LiDAR-inertial Odometry Package}} by {{Tightly-Coupled Iterated Kalman Filter}}},
  shorttitle = {{{FAST-LIO}}},
  author = {Xu, Wei and Zhang, Fu},
  date = {2021-04-14},
  eprint = {2010.08196},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.08196},
  urldate = {2023-03-27},
  abstract = {This paper presents a computationally efficient and robust LiDAR-inertial odometry framework. We fuse LiDAR feature points with IMU data using a tightly-coupled iterated extended Kalman filter to allow robust navigation in fastmotion, noisy or cluttered environments where degeneration occurs. To lower the computation load in the presence of a large number of measurements, we present a new formula to compute the Kalman gain. The new formula has computation load depending on the state dimension instead of the measurement dimension. The proposed method and its implementation are tested in various indoor and outdoor environments. In all tests, our method produces reliable navigation results in realtime: running on a quadrotor onboard computer, it fuses more than 1,200 effective feature points in a scan and completes all iterations of an iEKF step within 25 ms. Our codes are opensourced on Github2.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\XCQ32AV9\Xu 和 Zhang - 2021 - FAST-LIO A Fast, Robust LiDAR-inertial Odometry P.pdf}
}

@article{Xu2022,
  title = {Robots' {{State Estimation}} and {{Observability Analysis Based}} on {{Statistical Motion Models}}},
  author = {Xu, W. and He, D. and Cai, Y. and Zhang, F.},
  date = {2022},
  journaltitle = {IEEE Transactions on Control Systems Technology},
  volume = {30},
  number = {5},
  pages = {2030--2045},
  doi = {10.1109/TCST.2021.3133080},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123312851&doi=10.1109%2fTCST.2021.3133080&partnerID=40&md5=5722808ae01eb2359ffc557c47a69e45},
  abstract = {This article presents a generic motion model to capture mobile robots' dynamic behaviors (translation and rotation). The model is based on statistical models driven by white random processes and is formulated into a full state estimation algorithm based on the error-state extended Kalman filtering framework (ESEKF). The major benefits of this method are its versatility, being applicable to different robotic systems without accurately modeling the robots' specific dynamics, and the ability to estimate the robot's (angular) acceleration, jerk, or higher order dynamic states with low delay. Mathematical analyses with numerical simulations are presented to show the properties of the statistical model-based estimation framework and reveal its connection to existing low-pass filters. Furthermore, a new paradigm is developed for robotic observability analysis by developing Lie derivatives and associated partial differentiation directly on manifolds. It is shown that this new paradigm is much simpler and more natural than existing methods based on quaternion parameterizations. It is also scalable to high-dimensional systems. A novel thin set concept is introduced to characterize the unobservable subset of the system states, providing the theoretical foundation to observability analysis of robotic systems operating on manifolds and in high dimension. Finally, extensive experiments, including full state estimation and extrinsic calibration (both POS-IMU and IMU-IMU) on a quadrotor unmanned aerial vehicle (UAV), a handheld platform, and a ground vehicle, are conducted. Comparisons with existing methods show that the proposed method can effectively estimate all extrinsic parameters, the robot's translation/angular acceleration, and other state variables (e.g., position, velocity, and attitude) with high accuracy and low delay.  © 2021 IEEE.},
  issue = {5},
  annotation = {rate: 0}
}

@article{Xu2022a,
  title = {{{FAST-LIO2}}: {{Fast Direct LiDAR-Inertial Odometry}}},
  shorttitle = {{{FAST-LIO2}}},
  author = {Xu, Wei and Cai, Yixi and He, Dongjiao and Lin, Jiarong and Zhang, Fu},
  date = {2022-08},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {38},
  number = {4},
  pages = {2053--2073},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2022.3141876},
  url = {https://ieeexplore.ieee.org/document/9697912/},
  urldate = {2023-03-31},
  abstract = {This article presents FAST-LIO2: a fast, robust, and versatile LiDAR-inertial odometry framework. Building on a highly efficient tightly coupled iterated Kalman filter, FAST-LIO2 has two key novelties that allow fast, robust, and accurate LiDAR navigation (and mapping). The first one is directly registering raw points to the map (and subsequently update the map, i.e., mapping) without extracting features. This enables the exploitation of subtle features in the environment and, hence, increases the accuracy. The elimination of a hand-engineered feature extraction module also makes it naturally adaptable to emerging LiDARs of different scanning patterns; the second main novelty is maintaining a map by an incremental k-dimensional (k-d) tree data structure, incremental k-d tree (ikd-Tree), that enables incremental updates (i.e., point insertion and delete) and dynamic rebalancing. Compared with existing dynamic data structures (octree, R∗-tree, and nanoflann k-d tree), ikd-Tree achieves superior overall performance while naturally supports downsampling on the tree. We conduct an exhaustive benchmark comparison in 19 sequences from a variety of open LiDAR datasets. FAST-LIO2 achieves consistently higher accuracy at a much lower computation load than other state-of-the-art LiDAR-inertial navigation systems. Various real-world experiments on solid-state LiDARs with small field of view are also conducted. Overall, FAST-LIO2 is computationally efficient (e.g., up to 100 Hz odometry and mapping in large outdoor environments), robust (e.g., reliable pose estimation in cluttered indoor environments with rotation up to 1000 deg/s), versatile (i.e., applicable to both multiline spinning and solid-state LiDARs, unmanned aerial vehicle (UAV) and handheld platforms, and Inteland ARM-based processors), while still achieving a higher accuracy than existing methods. Our implementation of the system FASTLIO2 and the data structure ikd-Tree are both open-sourced on Github.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\FPMTYF88\Xu 等 - 2022 - FAST-LIO2 Fast Direct LiDAR-Inertial Odometry.pdf}
}

@article{Xu2023,
  title = {Distributed {{Invariant Extended Kalman Filter Using Lie Groups}}: {{Algorithm}} and {{Experiments}}},
  shorttitle = {Distributed {{Invariant Extended Kalman Filter Using Lie Groups}}},
  author = {Xu, Jie and Zhu, Pengxiang and Zhou, Yizhi and Ren, Wei},
  date = {2023},
  journaltitle = {IEEE Transactions on Control Systems Technology},
  shortjournal = {IEEE Trans. Contr. Syst. Technol.},
  pages = {1--13},
  issn = {1063-6536, 1558-0865, 2374-0159},
  doi = {10.1109/TCST.2023.3290299},
  url = {https://ieeexplore.ieee.org/document/10232373/},
  urldate = {2023-09-15},
  abstract = {Distributed Kalman filters have been widely studied in vector space and have been applied to 2-D target state estimation using sensor networks. In this article, we introduce a novel distributed invariant extended Kalman filer (DIEKF) that exploits matrix Lie groups and is suitable to track the target’s 6-DOF motion in a 3-D environment. The DIEKF is based on the proposed extended covariance intersection (CI) algorithm that guarantees consistency in matrix Lie groups. The DIEKF is fully distributed as each agent only uses the information from itself and the one-hop communication neighbors, and it is robust to a time-varying communication topology and changing blind agents. In addition to assuming a known target model, we study the case where the target’s true motion is unknown. To evaluate the performance, first, we apply the algorithm in a camera network to track a target pose. Extensive Monte-Carlo simulations have been performed to analyze the performance. More importantly, the performance is further verified with real data collected by using a quadrotor with multiple ultra-wideband (UWB) anchor receivers. Overall, the proposed algorithm is more accurate and more consistent in comparison with our recent work on the quaternion-based distributed extended Kalman filter (QDEKF).},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\EM48T5JS\Xu 等 - 2023 - Distributed Invariant Extended Kalman Filter Using.pdf}
}

@article{Yang2022,
  title = {Online {{Kinematic Calibration}} for {{Legged Robots}}},
  author = {Yang, S. and Choset, H. and Manchester, Z.},
  date = {2022},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  pages = {8178--8185},
  doi = {10.1109/LRA.2022.3186501},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133771567&doi=10.1109%2fLRA.2022.3186501&partnerID=40&md5=f995982b691e01e65d0d6b4fdb6f5181},
  abstract = {This paper describes an online method to calibrate certain kinematic parameters of legged robots, including leg lengths, that can be difficult to measure offline due to dynamic deformation effects and rolling contacts. A kinematic model of the robot's legs that depends on these parameters is used, along with measurements from joint encoders, foot contact sensors, and aninertial measurement unit (IMU) to predict the robot's body velocity. This predicted velocity is then compared to another velocity measurement from, for example, a camera or motion capture system, and the difference between them is used to compute anupdate on the kinematic parameters. The method can be incorporated into both Kalman filter or sliding-window optimization-based state estimator. We provide a theoretical observability analysis of our method, as well as validation both in simulation and on hardware. Hardware experiments demonstrate that online kinematic calibration can significantly reduce position drift when relying on odometry.  © 2016 IEEE.},
  annotation = {rate: 4}
}

@online{Yang2022a,
  title = {Learning {{Vision-Guided Quadrupedal Locomotion End-to-End}} with {{Cross-Modal Transformers}}},
  author = {Yang, Ruihan and Zhang, Minghao and Hansen, Nicklas and Xu, Huazhe and Wang, Xiaolong},
  date = {2022-05-26},
  eprint = {2107.03996},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2107.03996},
  urldate = {2022-11-01},
  abstract = {We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method that leverages both proprioceptive states and visual observations for locomotion control. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We transfer our learned policy from simulation to a real robot by running it indoors and in the wild with unseen obstacles and terrain. Our method not only significantly improves over baselines, but also achieves far better generalization performance, especially when transferred to the real robot. Our project page with videos is at https://rchalyang.github.io/LocoTransformer/.},
  langid = {english},
  pubstate = {preprint},
  file = {C:\Users\15469\Zotero\storage\QD75ZTYM\Yang 等 - 2022 - Learning Vision-Guided Quadrupedal Locomotion End-.pdf}
}

@article{Yang2023,
  title = {State Estimation of Hydraulic Quadruped Robots Using Invariant-{{EKF}} and Kinematics with Neural Networks},
  author = {Yang, Shangru and Yang, Qingjun and Zhu, Rui and Zhang, Zhenyang and Li, Congfei and Liu, Hu},
  date = {2023-06-26},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-023-08755-y},
  url = {https://link.springer.com/10.1007/s00521-023-08755-y},
  urldate = {2023-07-18},
  abstract = {The research on state estimation for quadruped robots is critical. Its result passed to motion controller makes the robot navigate autonomously and adjust the gait to a more stable motion. The current research depends on a multi-sensor fusion of cameras, lidars or other proprioceptive sensors, such as Inertial Measurement Unit (IMU) and encoders. The highfrequency data are generally derived from body sensors, which is to be fused with data from external sensors directly, or preprocessed with EKF first. Due to its unguaranteed convergence and robustness of tracking state mutations, EKF is insufficient. Therefore, we study state estimation for hydraulic quadruped robot based on the fusion of IMU measurement and leg odometry in this paper, and Invariant Extended Kalman Filter (IEKF) is successfully applied to quadruped robots by using this method. Besides, neural networks are utilized to train the weight functions of foot force and the state of leg odometry, and our trained functions improve the accuracy of observation compared with common weight average methods. Finally, our experiments of accuracy show that the root mean square error of our method is significantly reduced and the absolute trajectory error is reduced by 30\% compared to traditional IEKF. The algorithm achieves the drift per distance travelled below 4 cm/m. Moreover, it has good robustness.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\8HQBIIEW\Yang 等 - 2023 - State estimation of hydraulic quadruped robots usi.pdf}
}

@article{Yang2023a,
  title = {Multi-{{IMU Proprioceptive Odometry}} for {{Legged Robots}}},
  author = {Yang, Shuo and Zhang, Zixin and Bokser, Benjamin and Manchester, Zachary},
  date = {2023},
  journaltitle = {IROS},
  abstract = {This paper presents a novel, low-cost proprioceptive sensing solution for legged robots with point feet to achieve accurate low-drift long-term position and velocity estimation. In addition to conventional sensors, including one body Inertial Measurement Unit (IMU) and joint encoders, we attach an additional IMU to each calf link of the robot just above the foot. An extended Kalman filter is used to fuse data from all sensors to estimate the robot’s body and foot positions in the world frame. Using the additional IMUs, the filter is able to reliably determine foot contact modes and detect foot slips without tactile or pressure-based foot contact sensors. This sensing solution is validated in various hardware experiments, which confirm that it can reduce position drift by nearly an order of magnitude compared to conventional approaches with only a very modest increase in hardware and computational costs.},
  langid = {english},
  keywords = {IMU,leg odometry,卡尔曼,扩展卡尔曼,状态估计},
  file = {C:\Users\15469\Zotero\storage\Y8XFQSCL\Yang 等 - Multi-IMU Proprioceptive Odometry for Legged Robot.pdf}
}

@inproceedings{Yang2023b,
  title = {Cerberus: {{Low-Drift Visual-Inertial-Leg Odometry For Agile Locomotion}}},
  shorttitle = {Cerberus},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Yang, Shuo and Zhang, Zixin and Fu, Zhengyu and Manchester, Zachary},
  date = {2023-05-29},
  pages = {4193--4199},
  publisher = {{IEEE}},
  location = {{London, United Kingdom}},
  doi = {10.1109/ICRA48891.2023.10160486},
  url = {https://ieeexplore.ieee.org/document/10160486/},
  urldate = {2023-08-22},
  abstract = {We present an open-source Visual-Inertial-Leg Odometry (VILO) state estimation solution for legged robots, called Cerberus, which precisely estimates position on various terrains in real-time using a set of standard sensors, including stereo cameras, IMU, joint encoders, and contact sensors. In addition to estimating robot states, we perform online kinematic parameter calibration and outlier rejection to substantially reduce position drift. Hardware experiments in various indoor and outdoor environments validate that online calibration of kinematic parameters can reduce estimation drift to less than 1\% during long-distance, high-speed locomotion. Our drift results are better than those of any other state estimation method using the same set of sensors reported in the literature. Moreover, our state estimator performs well even when the robot experiences large impacts and camera occlusion. The implementation of the state estimator, along with the datasets used to compute our results, is available at https://github. com/ShuoYangRobotics/Cerberus.},
  eventtitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {9798350323658},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\VF86VQWM\Yang 等 - 2023 - Cerberus Low-Drift Visual-Inertial-Leg Odometry F.pdf}
}

@incollection{Zhang2017,
  title = {Torque {{Control}} in {{Legged Locomotion}}},
  booktitle = {Bioinspired {{Legged Locomotion}}: {{Models}}, {{Concepts}}, {{Control}} and {{Applications}}},
  author = {Zhang, J. and Cheah, C.C. and Collins, S.H.},
  date = {2017},
  pages = {347--400},
  doi = {10.1016/B978-0-12-803766-9.00007-5},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043433721&doi=10.1016%2fB978-0-12-803766-9.00007-5&partnerID=40&md5=31dc0b2b688c5f7c0507fabeeb06d9d4},
  abstract = {Many torque control approaches have been proposed for robotic devices used in legged locomotion, but few comparisons have been performed across controllers in the same system. In this study, we compared the torque-tracking performance of nine control strategies, including variations on classical feedback control, model-based control, adaptive control, and iterative learning. To account for interactions between patterns in desired torque and tracking performance, we tested each in combination with four high-level controllers that determined desired torque based on time, joint angle, a neuromuscular model, or electromyographic measurements. Controllers were implemented on an ankle exoskeleton with series elastic actuation driven by an off-board motor through a uni-directional Bowden cable. The exoskeleton was worn by one human subject walking on a treadmill at 1.25 m˙s-1 for one hundred steady-state steps under each condition. We found that the combination of proportional control, damping injection and iterative learning resulted in substantially lower root-mean-squared error than other torque control approaches for all high-level controllers. With this low level torque controller, RMS errors can be as low as 1.3\% of peak torque for real-time tracking, and 0.2\% for the average stride. Model-free, integration-free feedback control seems to be well suited to the uncertain, changing dynamics of the human-robot system, while iterative learning is advantageous in the cyclic task of walking. © 2017 Elsevier Inc. All rights reserved.},
  annotation = {rate: 2}
}

@inproceedings{Zhang2018,
  title = {Dense {{RGB-D SLAM}} for {{Humanoid Robots}} in the {{Dynamic Humans Environment}}},
  booktitle = {2018 {{IEEE-RAS}} 18th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Zhang, Tianwei and Uchiyama, Emiko and Nakamura, Yoshihiko},
  date = {2018-11},
  pages = {270--276},
  publisher = {{IEEE}},
  location = {{Beijing, China}},
  doi = {10.1109/HUMANOIDS.2018.8625019},
  url = {https://ieeexplore.ieee.org/document/8625019/},
  urldate = {2023-04-27},
  abstract = {These two problems block the SLAM method applications for humanoids. On the one hand, humans are often considered as moving obstacles or moving targets in the humanoids working spaces, which result the dynamic environment problem. On the other hand, the disturbances caused by the executions of biped locomotion and the environment structure discontinuity caused by the falling down case make big challenge for SLAM approaches. In this paper, we propose a robust dense RGB-D environment reconstruction method for humanoids working in dynamic humans space. The proposed approach efficiently detects humans and fast reconstructs the static environments through deep learning-based human body detection, and then implement a graph-based segmentation on the RGB-D point clouds, which separates detected moving humans from the static environment. Finally, the separated static environments are aligned with using state-of-the-art frame-to-model scheme. Experimental results on both public benchmark and a newly developed HRP-4 humanoids SLAM dataset indicate that the proposed approach achieves outstanding performance in full dynamic environments.},
  eventtitle = {2018 {{IEEE-RAS}} 18th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  isbn = {978-1-5386-7283-9},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\BW499SDT\Zhang 等 - 2018 - Dense RGB-D SLAM for Humanoid Robots in the Dynami.pdf}
}

@inproceedings{Zhang2019,
  title = {{{HRPSlam}}: {{A Benchmark}} for {{RGB-D Dynamic SLAM}} and {{Humanoid Vision}}},
  shorttitle = {{{HRPSlam}}},
  booktitle = {2019 {{Third IEEE International Conference}} on {{Robotic Computing}} ({{IRC}})},
  author = {Zhang, Tianwei and Nakamura, Yoshihiko},
  date = {2019-02},
  pages = {110--116},
  publisher = {{IEEE}},
  location = {{Naples, Italy}},
  doi = {10.1109/IRC.2019.00024},
  url = {https://ieeexplore.ieee.org/document/8675614/},
  urldate = {2023-04-27},
  abstract = {The dynamic environment is one of the challenging leftover problems for SLAM methods. Recently, researchers made a lot of efforts and achieved some breakthroughs in the dynamic SLAM research field. However, the existed RGB-D SLAM datasets are lacking the dynamic environment. In this paper, we present a novel SLAM benchmark which is dedicated to the dynamic environment SLAM solutions. Moreover, these datasets are acquired from an on-board camera on the HRP-4 humanoid robot, the humanoid dynamic motions, such as the shake from biped walking, the falling down case are included. The data of other on-board sensors, such as IMU, force sensor and joint angle encoders, are provided as well.},
  eventtitle = {2019 {{Third IEEE International Conference}} on {{Robotic Computing}} ({{IRC}})},
  isbn = {978-1-5386-9245-5},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\2HGC4RM6\Zhang 和 Nakamura - 2019 - HRPSlam A Benchmark for RGB-D Dynamic SLAM and Hu.pdf}
}

@article{Zhang2020,
  title = {Humanoid {{Robot RGB-D SLAM}} in the {{Dynamic Human Environment}}},
  author = {Zhang, Tianwei and Nakamura, Yoshihiko},
  date = {2020-04},
  journaltitle = {International Journal of Humanoid Robotics},
  shortjournal = {Int. J. Human. Robot.},
  volume = {17},
  number = {02},
  pages = {2050009},
  issn = {0219-8436, 1793-6942},
  doi = {10.1142/S0219843620500097},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0219843620500097},
  urldate = {2023-04-27},
  abstract = {Unsteady locomotion and the dynamic environment are two problems that block humanoid robots to apply visual Simultaneous Localization and Mapping (SLAM) approaches. Humans are often considered as moving obstacles and targets in humanoid robots working space. Thus, in this paper, we propose a robust dense RGB-D SLAM approach for the humanoid robots working in the dynamic human environments. To deal with the dynamic human objects, a deep learning-based human detector is combined in the proposed method. After the removal of the dynamic object, we fast reconstruct the static environments through a dense RGB-D point clouds fusion framework. In addition to the humanoid robot falling problem, which usually results in visual sensing discontinuities, we propose a novel point clouds registration-based method to relocate the robot pose. Therefore, our robot can continue the self localization and mapping after the falling. Experimental results on both the public benchmarks and the real humanoid robot SLAM experiments indicated that the proposed approach outperformed state-of-the-art SLAM solutions in dynamic human environments.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\HYQKFD3Y\Zhang 和 Nakamura - 2020 - Humanoid Robot RGB-D SLAM in the Dynamic Human Env.pdf}
}

@online{Zhang2023,
  title = {{{InstaLoc}}: {{One-shot Global Lidar Localisation}} in {{Indoor Environments}} through {{Instance Learning}}},
  shorttitle = {{{InstaLoc}}},
  author = {Zhang, Lintong and Digumarti, Tejaswi and Tinchev, Georgi and Fallon, Maurice},
  date = {2023-05-16},
  eprint = {2305.09552},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.09552},
  urldate = {2023-05-24},
  abstract = {Localization for autonomous robots in prior maps is crucial for their functionality. This paper offers a solution to this problem for indoor environments called InstaLoc, which operates on an individual lidar scan to localize it within a prior map. We draw on inspiration from how humans navigate and position themselves by recognizing the layout of distinctive objects and structures. Mimicking the human approach, InstaLoc identifies and matches object instances in the scene with those from a prior map. As far as we know, this is the first method to use panoptic segmentation directly inferring on 3D lidar scans for indoor localization. InstaLoc operates through two networks based on spatially sparse tensors to directly infer dense 3D lidar point clouds. The first network is a panoptic segmentation network that produces object instances and their semantic classes. The second smaller network produces a descriptor for each object instance. A consensus based matching algorithm then matches the instances to the prior map and estimates a six degrees of freedom (DoF) pose for the input cloud in the prior map. The significance of InstaLoc is that it has two efficient networks. It requires only one to two hours of training on a mobile GPU and runs in real-time at 1 Hz. Our method achieves between two and four times more detections when localizing, as compared to baseline methods, and achieves higher precision on these detections.},
  langid = {english},
  pubstate = {preprint},
  keywords = {RSS,SLAM,机器学习,激光雷达},
  file = {C:\Users\15469\Zotero\storage\7WE6R2Q2\Zhang 等 - 2023 - InstaLoc One-shot Global Lidar Localisation in In.pdf}
}

@article{Zhang2023a,
  title = {{{SLoMo}}: {{A General System}} for {{Legged Robot Motion Imitation}} from {{Casual Videos}}},
  author = {Zhang, John Z and Yang, Shuo and Yang, Gengshan and Bishop, Arun L and Gurumurthy, Swaminathan and Ramanan, Deva and Manchester, Zachary},
  date = {2023},
  abstract = {We present SLoMo: a first-of-its-kind framework for transferring skilled motions from casually captured “in-thewild” video footage of humans and animals to legged robots. SLoMo works in three stages: 1) synthesize a physically plausible reconstructed key-point trajectory from monocular videos; 2) optimize a dynamically feasible reference trajectory for the robot offline that includes body and foot motion, as well as a contact sequence that closely tracks the key points; 3) track the reference trajectory online using a general-purpose model-predictive controller on robot hardware. Traditional motion imitation for legged motor skills often requires expert animators, collaborative demonstrations, and/or expensive motion-capture equipment, all of which limit scalability. Instead, SLoMo only relies on easyto-obtain videos, readily available in online repositories like YouTube. It converts videos into motion primitives that can be executed reliably by real-world robots. We demonstrate our approach by transferring the motions of cats, dogs, and humans to example robots including a quadruped (on hardware) and a humanoid (in simulation). Videos are available at https://slomowww.github.io/website.},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\KD5EZL3T\Zhang 等 - 2023 - SLoMo A General System for Legged Robot Motion Im.pdf}
}

@book{Zhou,
  title = {以利为利},
  author = {周, 飞舟},
  annotation = {titleTranslation: titleTranslation:},
  file = {C:\Users\15469\Zotero\storage\ZEJFIFWU\以利为利.pdf}
}

@inproceedings{Zhou2023,
  title = {Design and {{Implementation}} of an {{Underactuated Biped Robot Prototype}} on {{Compliant Ground}}},
  booktitle = {2023 {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  author = {Zhou, Qian and Yin, Xiong and Guo, Zhilong and Pei, Yu and Feng, Zhao and Yao, Daojin},
  date = {2023-07-08},
  pages = {947--952},
  publisher = {{IEEE}},
  location = {{Sanya, China}},
  doi = {10.1109/ICARM58088.2023.10218850},
  url = {https://ieeexplore.ieee.org/document/10218850/},
  urldate = {2023-09-15},
  abstract = {Considering the influence of the actual compliant grounds on the stable walking of the underactuated robot, a prototype of the planar underactuated biped walking robot is designed in this paper, and the simulation and experiment are carried out. Firstly, a biped robot model is established, and the compliant ground is equivalent to a spring-damping system. An adaptive feedforward control strategy is adopted to realize stable underactuated walking by controlling the robot's centroid velocity. Then, according to the control strategy, a 600mm high, 5.62kg weight planar underactuated robot prototype was produced, including the mechanical structure and software system. Finally, the underactuated walking with the step size of 0.1839m is realized through simulation and robot prototype experiment. The experimental results show that the robot prototype designed in this paper can carry out underactuated walking on different compliant grounds.},
  eventtitle = {2023 {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  isbn = {9798350300178},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\6G5S3WQK\Zhou 等 - 2023 - Design and Implementation of an Underactuated Bipe.pdf}
}

@book{zotero-11840,
  title = {银河系漫游指南},
  file = {C:\Users\15469\Zotero\storage\GWXBU4AQ\银河系漫游指南.pdf}
}

@book{zotero-12218,
  title = {Science Research Writing for Native and Non-Native Speakers of {{English}}},
  file = {C:\Users\15469\Zotero\storage\ZG3ICPJ5\Hilary Glasman-Deal - Science research writing for non-native speakers of English (2020)_2.pdf}
}

@software{zotero-5747,
  title = {{{ZoteroStyle}}},
  url = {https://github.com/MuiseDestiny/ZoteroStyle},
  abstract = {这是Zotero Style插件生成的条目，用于记录阅读数据}
}

@software{zotero-7163,
  title = {Addon {{Item}}}
}

@inproceedings{Zuo2020,
  title = {{{LIC-Fusion}} 2.0: {{LiDAR-Inertial-Camera Odometry}} with {{Sliding-Window Plane-Feature Tracking}}},
  shorttitle = {{{LIC-Fusion}} 2.0},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Zuo, Xingxing and Yang, Yulin and Geneva, Patrick and Lv, Jiajun and Liu, Yong and Huang, Guoquan and Pollefeys, Marc},
  date = {2020-10-24},
  pages = {5112--5119},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/IROS45743.2020.9340704},
  url = {https://ieeexplore.ieee.org/document/9340704/},
  urldate = {2023-06-25},
  eventtitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  isbn = {978-1-72816-212-6},
  langid = {english},
  file = {C:\Users\15469\Zotero\storage\FAR8NJPM\Zuo 等 - 2020 - LIC-Fusion 2.0 LiDAR-Inertial-Camera Odometry wit.pdf}
}
